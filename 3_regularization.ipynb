{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell author : Hoang NT\n",
    "# Cell : Global variables, imports and helper functions\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "def scale_dev(dsize) :\n",
    "    return 1.0 / math.sqrt(float(dsize))\n",
    "\n",
    "def randomize(dataset, labels) :\n",
    "    perm = np.random.permutation(labels.shape[0])\n",
    "    return dataset[perm], labels[perm]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 2.314857\n",
      "Minibatch accuracy: 10.9%\n",
      "Minibatch loss at step 500: 0.860063\n",
      "Minibatch accuracy: 78.9%\n",
      "Minibatch loss at step 1000: 0.829281\n",
      "Minibatch accuracy: 76.6%\n",
      "Minibatch loss at step 1500: 0.762297\n",
      "Minibatch accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 0.822789\n",
      "Minibatch accuracy: 78.1%\n",
      "Minibatch loss at step 2500: 0.865590\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 3000: 0.789147\n",
      "Minibatch accuracy: 79.7%\n",
      "Minibatch loss at step 3500: 0.684425\n",
      "Minibatch accuracy: 83.6%\n",
      "Minibatch loss at step 4000: 0.655271\n",
      "Minibatch accuracy: 84.4%\n",
      "Minibatch loss at step 4500: 0.722902\n",
      "Minibatch accuracy: 79.7%\n",
      "Minibatch loss at step 5000: 0.781310\n",
      "Minibatch accuracy: 79.7%\n",
      "Minibatch loss at step 5500: 0.825230\n",
      "Minibatch accuracy: 78.1%\n",
      "Minibatch loss at step 6000: 0.699435\n",
      "Minibatch accuracy: 84.4%\n",
      "Minibatch loss at step 6500: 0.694292\n",
      "Minibatch accuracy: 83.6%\n",
      "Minibatch loss at step 7000: 0.763707\n",
      "Minibatch accuracy: 84.4%\n",
      "Minibatch loss at step 7500: 0.780489\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation data accuracy: 83.2%\n"
     ]
    }
   ],
   "source": [
    "# Cell author : Hoang NT\n",
    "# Cell : Logistic model with regularization\n",
    "\n",
    "_alpha = 0.1 # Learning rate\n",
    "_beta = 0.005 # Regularization factor\n",
    "num_labels = 10\n",
    "num_step = 8000\n",
    "batch_size = 128\n",
    "logis_scope = 'logistic'\n",
    "\n",
    "# Quick flag\n",
    "logis_train = True\n",
    "\n",
    "# Build the computational graph\n",
    "# Input:\n",
    "### data_dataset - tf.placeholder - Input data for calculation\n",
    "### num_labels - int - length of onehot label vector\n",
    "# Output:\n",
    "### logits - tf.Tensor - Result of computation\n",
    "def logistic_inference(train_data) :\n",
    "    # Weight matrix initialized as randomized matrix\n",
    "    with tf.variable_scope(logis_scope) :\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal(\n",
    "                                                [IMAGE_PIXELS, num_labels],\n",
    "                                                stddev=scale_dev(IMAGE_PIXELS)))\n",
    "        biases = tf.get_variable('biases', initializer=tf.zeros([num_labels]))\n",
    "    logits = tf.matmul(train_data, weights) + biases\n",
    "    return logits\n",
    "\n",
    "# Add loss function with regularization term to the graph\n",
    "# Input:\n",
    "### logits - tf.Tensor - output node of inference\n",
    "### labels - tf.placeholder - training labels to computute loss\n",
    "### beta - float - regularization factor\n",
    "# Output:\n",
    "### loss - tf.Tensor - result node of loss computation\n",
    "def logistic_loss(logits, labels, beta) :\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "    # Get weights node\n",
    "    with tf.variable_scope(logis_scope) :\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        weights = tf.get_variable('weights', [IMAGE_PIXELS, num_labels])\n",
    "    loss_p = tf.reduce_mean(xentropy, name='mean_xentropy') \n",
    "    regu = beta * tf.nn.l2_loss(weights)\n",
    "    loss = loss_p + regu\n",
    "    return loss\n",
    "\n",
    "# Create training operations with SGD optimizer\n",
    "# Input:\n",
    "### loss - tf.Tensor - loss function result node\n",
    "### learning_rate - float - learning rate for SGD\n",
    "# Output:\n",
    "### train_op - tf.Tensor - operation node to run optimizer\n",
    "def logistic_training(loss, learning_rate) :\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    return train_op\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() :\n",
    "    # Create placeholder for training data and labels\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32, shape=[None, IMAGE_PIXELS])\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32, shape=[None, num_labels])\n",
    "    # Add computational nodes to graph\n",
    "    logits = logistic_inference(tf_train_dataset)\n",
    "    loss = logistic_loss(logits, tf_train_labels, _beta)\n",
    "    train_op = logistic_training(loss, _alpha)\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Prediction output for each batch\n",
    "    predictions_softmax = tf.nn.softmax(logits)\n",
    "    with tf.Session() as session :\n",
    "        session.run(init)\n",
    "        for step in range(num_step) :\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Training data for each batch\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([train_op, loss, predictions_softmax],\n",
    "                                           feed_dict = feed_dict)\n",
    "            if (step % 500 == 0) :\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions,\n",
    "                                                              batch_labels))\n",
    "            if (step % 50 == 0) :\n",
    "                randomize(train_dataset, train_labels)\n",
    "        \n",
    "        # Get weights for visualization\n",
    "        with tf.variable_scope(logis_scope):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            weights = tf.get_variable('weights', [IMAGE_PIXELS, num_labels])\n",
    "        \n",
    "        # Validation step\n",
    "        valid_dict = {tf_train_dataset : valid_dataset}\n",
    "        predictions, np_weights = session.run([predictions_softmax, weights],\n",
    "                                               feed_dict=valid_dict)\n",
    "        print(\"Validation data accuracy: %.1f%%\"\n",
    "              % accuracy(predictions, valid_labels))\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f74386738d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAEqCAYAAAAVsZj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFUIIIYQQHcv/AIp1kaNbpKkiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f748181b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = np_weights.reshape((10,28,28))\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(2,5,1)\n",
    "plt.imshow(weights[0], cmap=plt.cm.Blues, interpolation='none') \n",
    "plt.subplot(2,5,2)\n",
    "plt.imshow(weights[1], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,3)\n",
    "plt.imshow(weights[2], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,4)\n",
    "plt.imshow(weights[3], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,5)\n",
    "plt.imshow(weights[4], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,6)\n",
    "plt.imshow(weights[5], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,7)\n",
    "plt.imshow(weights[6], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,8)\n",
    "plt.imshow(weights[7], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,9)\n",
    "plt.imshow(weights[8], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,10)\n",
    "plt.imshow(weights[9], cmap=plt.cm.Blues, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 0: 4.335188\n",
      "Minibatch accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.094598\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 100: 1.090454\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 150: 1.088063\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 200: 1.086368\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 250: 1.085165\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 300: 1.084310\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 350: 1.083700\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 400: 1.083262\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 450: 1.082947\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 500: 1.082719\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 550: 1.082553\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 600: 1.082430\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 650: 1.082340\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 700: 1.082273\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 750: 1.082222\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 800: 1.082184\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 850: 1.082155\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 900: 1.082134\n",
      "Minibatch accuracy: 96.0%\n",
      "Minibatch loss at step 950: 1.082117\n",
      "Minibatch accuracy: 96.0%\n",
      "Validation data accuracy: 68.0%\n",
      "Sanitized test dataset accuracy: 66.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_step = 1000\n",
    "_beta = 0.5 # Disable regularization\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() :\n",
    "    tf_train_dataset = tf.placeholder(dtype=tf.float32, shape=[None,IMAGE_PIXELS])\n",
    "    tf_train_labels = tf.placeholder(dtype=tf.float32, shape=[None,num_labels])\n",
    "    logits = logistic_inference(tf_train_dataset)\n",
    "    loss = logistic_loss(logits, tf_train_labels, _beta)\n",
    "    train_op = logistic_training(loss, _alpha)\n",
    "    init = tf.initialize_all_variables()\n",
    "    predictions_softmax = tf.nn.softmax(logits)\n",
    "    with tf.Session() as sess :\n",
    "        sess.run(init)\n",
    "        for step in range(num_step) :\n",
    "            offset = 0\n",
    "            # Training data for each batch\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, \n",
    "                         tf_train_labels : batch_labels}\n",
    "            _, l, predictions = sess.run([train_op, loss, predictions_softmax],\n",
    "                                           feed_dict = feed_dict)\n",
    "            if (step % 50 == 0) :\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions,\n",
    "                                                              batch_labels))\n",
    "        # Validation step\n",
    "        valid_dict = {tf_train_dataset : test_dataset}\n",
    "        predictions = sess.run(predictions_softmax, feed_dict=valid_dict)\n",
    "        print(\"Validation data accuracy: %.1f%%\"\n",
    "              % accuracy(predictions, test_labels))\n",
    "        \n",
    "        # Get weights for visualization\n",
    "        with tf.variable_scope(logis_scope):\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            weights = tf.get_variable('weights', [IMAGE_PIXELS, num_labels])\n",
    "        \n",
    "        # Sanitized test dataset\n",
    "        f = open('notMNIST_sanitized_test.pickle', 'rb')\n",
    "        testsets = pickle.load(f)\n",
    "        san_test_dataset, san_test_labels = reformat(testsets.get('test_dataset'), \n",
    "                                                     testsets.get('test_labels'))\n",
    "        san_test_dict = {tf_train_dataset : san_test_dataset}\n",
    "        predictions, np_weights = sess.run([predictions_softmax, weights]\n",
    "                                           , feed_dict = san_test_dict)\n",
    "        print(\"Sanitized test dataset accuracy: %.1f%%\" \n",
    "             % accuracy(predictions, san_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7481222e10>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAEqCAYAAAAVsZj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nya6iKIqiKIoSt+hkV1EURVEURYlb/g8uzzz69l05YAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7430d63f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell author : Hoang NT\n",
    "# Cell : Visualize each colums of the weights as a 28x28 image\n",
    "\n",
    "###### DONOT RE-RUN THIS CELL #######\n",
    "\n",
    "# Case: extreme repetitive training dataset, 100% test accuracy, regularized.\n",
    "weights = np_weights.reshape((10,28,28))\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(2,5,1)\n",
    "plt.imshow(weights[0], cmap=plt.cm.Blues, interpolation='none') \n",
    "plt.subplot(2,5,2)\n",
    "plt.imshow(weights[1], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,3)\n",
    "plt.imshow(weights[2], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,4)\n",
    "plt.imshow(weights[3], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,5)\n",
    "plt.imshow(weights[4], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,6)\n",
    "plt.imshow(weights[5], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,7)\n",
    "plt.imshow(weights[6], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,8)\n",
    "plt.imshow(weights[7], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,9)\n",
    "plt.imshow(weights[8], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,10)\n",
    "plt.imshow(weights[9], cmap=plt.cm.Blues, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3ec73a6a50>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAEqCAYAAAAVsZj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8Vv0YVdRFEVRFEXxW/4fAXIwpRFnjEIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ece87fc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell author : Hoang NT\n",
    "# Cell : Visualize each colums of the weights as a 28x28 image\n",
    "\n",
    "# Case: extreme repetitive training dataset, 100% test accuracy, no regularization.\n",
    "weights = np_weights.reshape((10,28,28))\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(2,5,1)\n",
    "plt.imshow(weights[0], cmap=plt.cm.Blues, interpolation='none') \n",
    "plt.subplot(2,5,2)\n",
    "plt.imshow(weights[1], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,3)\n",
    "plt.imshow(weights[2], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,4)\n",
    "plt.imshow(weights[3], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,5)\n",
    "plt.imshow(weights[4], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,6)\n",
    "plt.imshow(weights[5], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,7)\n",
    "plt.imshow(weights[6], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,8)\n",
    "plt.imshow(weights[7], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,9)\n",
    "plt.imshow(weights[8], cmap=plt.cm.Blues, interpolation='none')\n",
    "plt.subplot(2,5,10)\n",
    "plt.imshow(weights[9], cmap=plt.cm.Blues, interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.270972\n",
      "Step 0 accuracy: 19.5%\n",
      "Step 500 loss: 0.746613\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.671384\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.582257\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.642093\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.633752\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.43\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.357931\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.720517\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.703237\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.676944\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.667923\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.655125\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.56\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.408971\n",
      "Step 0 accuracy: 5.5%\n",
      "Step 500 loss: 0.723029\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.786231\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.693249\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.722577\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.763880\n",
      "Step 2500 accuracy: 78.9%\n",
      "Accuracy in test dataset: 91.08\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.444036\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.903674\n",
      "Step 500 accuracy: 73.4%\n",
      "Step 1000 loss: 0.945640\n",
      "Step 1000 accuracy: 72.7%\n",
      "Step 1500 loss: 0.760174\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.786778\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.870510\n",
      "Step 2500 accuracy: 77.3%\n",
      "Accuracy in test dataset: 90.7\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.587294\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 1.135208\n",
      "Step 500 accuracy: 66.4%\n",
      "Step 1000 loss: 1.013592\n",
      "Step 1000 accuracy: 67.2%\n",
      "Step 1500 loss: 0.950349\n",
      "Step 1500 accuracy: 77.3%\n",
      "Step 2000 loss: 0.951170\n",
      "Step 2000 accuracy: 68.8%\n",
      "Step 2500 loss: 0.941782\n",
      "Step 2500 accuracy: 72.7%\n",
      "Accuracy in test dataset: 89.84\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.355804\n",
      "Step 0 accuracy: 4.7%\n",
      "Step 500 loss: 0.706208\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.658014\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.586656\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.620201\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.637907\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 91.87\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.288537\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.707171\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.670210\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.599071\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.669740\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.658968\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.71\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.365484\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.747115\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.778764\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.665872\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.717647\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.728324\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 91.53\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.461285\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.809631\n",
      "Step 500 accuracy: 75.8%\n",
      "Step 1000 loss: 0.791700\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.667840\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.747443\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.750950\n",
      "Step 2500 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.01\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.603227\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 1.072131\n",
      "Step 500 accuracy: 69.5%\n",
      "Step 1000 loss: 1.022290\n",
      "Step 1000 accuracy: 73.4%\n",
      "Step 1500 loss: 0.803242\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.885340\n",
      "Step 2000 accuracy: 74.2%\n",
      "Step 2500 loss: 0.891140\n",
      "Step 2500 accuracy: 75.8%\n",
      "Accuracy in test dataset: 90.58\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.264791\n",
      "Step 0 accuracy: 21.9%\n",
      "Step 500 loss: 0.699211\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.657080\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.605822\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.634372\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.617643\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 91.99\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.404602\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.698260\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.672136\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.607807\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.635574\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.627854\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.81\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.342578\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.774435\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.718959\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.637899\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.660499\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.666220\n",
      "Step 2500 accuracy: 78.9%\n",
      "Accuracy in test dataset: 91.65\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.402381\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.851431\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.756273\n",
      "Step 1000 accuracy: 76.6%\n",
      "Step 1500 loss: 0.704959\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.747483\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.794082\n",
      "Step 2500 accuracy: 78.1%\n",
      "Accuracy in test dataset: 91.16\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.531018\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.977476\n",
      "Step 500 accuracy: 74.2%\n",
      "Step 1000 loss: 0.837223\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.724291\n",
      "Step 1500 accuracy: 77.3%\n",
      "Step 2000 loss: 0.809309\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.814396\n",
      "Step 2500 accuracy: 77.3%\n",
      "Accuracy in test dataset: 90.82\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.311896\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.697787\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.663647\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.578507\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.619884\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.621713\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 92.17\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.346557\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.727394\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.663433\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.592963\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.622636\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.616876\n",
      "Step 2500 accuracy: 79.7%\n",
      "Accuracy in test dataset: 92.24\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.391652\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.726566\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.711216\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.601402\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.638838\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.652467\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.9\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.404017\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.840096\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.722476\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.672364\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.680340\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.761832\n",
      "Step 2500 accuracy: 78.1%\n",
      "Accuracy in test dataset: 91.75\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.611577\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.891698\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.783823\n",
      "Step 1000 accuracy: 76.6%\n",
      "Step 1500 loss: 0.786479\n",
      "Step 1500 accuracy: 78.1%\n",
      "Step 2000 loss: 0.820618\n",
      "Step 2000 accuracy: 76.6%\n",
      "Step 2500 loss: 0.789355\n",
      "Step 2500 accuracy: 75.8%\n",
      "Accuracy in test dataset: 91.02\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.375924\n",
      "Step 0 accuracy: 3.9%\n",
      "Step 500 loss: 0.703621\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.674933\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.596004\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.641697\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.634644\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.542428\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.523288\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.402320\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.505976\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.534513\n",
      "Step 5000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 93.18\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.396909\n",
      "Step 0 accuracy: 3.1%\n",
      "Step 500 loss: 0.731810\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.733774\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.596781\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.661717\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.681479\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.581988\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.531570\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.390280\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.561124\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.551964\n",
      "Step 5000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 92.49\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.416919\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.838221\n",
      "Step 500 accuracy: 74.2%\n",
      "Step 1000 loss: 0.764174\n",
      "Step 1000 accuracy: 76.6%\n",
      "Step 1500 loss: 0.680189\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.722736\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.787021\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.620326\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.676767\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.484830\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.601876\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.605181\n",
      "Step 5000 accuracy: 80.5%\n",
      "Accuracy in test dataset: 91.93\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.461381\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.863864\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.853404\n",
      "Step 1000 accuracy: 71.9%\n",
      "Step 1500 loss: 0.851460\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.875149\n",
      "Step 2000 accuracy: 74.2%\n",
      "Step 2500 loss: 0.824763\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.761599\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.686074\n",
      "Step 3500 accuracy: 82.8%\n",
      "Step 4000 loss: 0.655801\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.705806\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.622997\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.01\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.589888\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 1.032675\n",
      "Step 500 accuracy: 70.3%\n",
      "Step 1000 loss: 1.005500\n",
      "Step 1000 accuracy: 68.8%\n",
      "Step 1500 loss: 0.974848\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.817931\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.996599\n",
      "Step 2500 accuracy: 74.2%\n",
      "Step 3000 loss: 0.989253\n",
      "Step 3000 accuracy: 75.8%\n",
      "Step 3500 loss: 0.907422\n",
      "Step 3500 accuracy: 76.6%\n",
      "Step 4000 loss: 0.772331\n",
      "Step 4000 accuracy: 78.9%\n",
      "Step 4500 loss: 0.751760\n",
      "Step 4500 accuracy: 75.8%\n",
      "Step 5000 loss: 0.746724\n",
      "Step 5000 accuracy: 78.1%\n",
      "Accuracy in test dataset: 90.29\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.320831\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.708766\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.667141\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.588526\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.628665\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.640534\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.532186\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.485287\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.372297\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.510320\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.507978\n",
      "Step 5000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 93.21\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.288046\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.714154\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.686477\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.619193\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.662514\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.649778\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.547245\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.506079\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.365531\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.505350\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.571111\n",
      "Step 5000 accuracy: 81.2%\n",
      "Accuracy in test dataset: 92.97\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.358426\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.807486\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.737072\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.677336\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.727476\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.678612\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.650365\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.607243\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.445784\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.536920\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.623767\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.49\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.416763\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.897595\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.855087\n",
      "Step 1000 accuracy: 71.1%\n",
      "Step 1500 loss: 0.818291\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.811162\n",
      "Step 2000 accuracy: 74.2%\n",
      "Step 2500 loss: 0.769177\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.739833\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.675166\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.569418\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.610832\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.671789\n",
      "Step 5000 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.71\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.602170\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.973064\n",
      "Step 500 accuracy: 68.0%\n",
      "Step 1000 loss: 0.898276\n",
      "Step 1000 accuracy: 71.9%\n",
      "Step 1500 loss: 0.933866\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.849807\n",
      "Step 2000 accuracy: 74.2%\n",
      "Step 2500 loss: 0.892911\n",
      "Step 2500 accuracy: 75.8%\n",
      "Step 3000 loss: 0.844210\n",
      "Step 3000 accuracy: 79.7%\n",
      "Step 3500 loss: 0.818607\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.665019\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.677547\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.752577\n",
      "Step 5000 accuracy: 78.1%\n",
      "Accuracy in test dataset: 90.8\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.301195\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.717844\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.677214\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.593871\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.610456\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.627890\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.541720\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.471168\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.375869\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.489326\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.490283\n",
      "Step 5000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 93.47\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.332598\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.720900\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.672346\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.626979\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.627260\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.664444\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.538237\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.467250\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.400070\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.538906\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.542807\n",
      "Step 5000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 93.32\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.360787\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.764467\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.732600\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.658428\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.673562\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.673774\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.617415\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.544797\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.415231\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.542597\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.615057\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 92.7\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.418791\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.827556\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.772438\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.670997\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.770720\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.771036\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.672208\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.621819\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.534850\n",
      "Step 4000 accuracy: 85.2%\n",
      "Step 4500 loss: 0.625941\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.645221\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 92.05\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.463714\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.960063\n",
      "Step 500 accuracy: 72.7%\n",
      "Step 1000 loss: 0.939097\n",
      "Step 1000 accuracy: 72.7%\n",
      "Step 1500 loss: 0.829932\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.800186\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.886386\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.716385\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.709363\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.507096\n",
      "Step 4000 accuracy: 83.6%\n",
      "Step 4500 loss: 0.676882\n",
      "Step 4500 accuracy: 78.1%\n",
      "Step 5000 loss: 0.657235\n",
      "Step 5000 accuracy: 81.2%\n",
      "Accuracy in test dataset: 91.58\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.319057\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.703181\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.660167\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.594817\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.616702\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.597385\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.521218\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.456818\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.364815\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.488197\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.496540\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.58\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.347254\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.735498\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.671756\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.578646\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.622230\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.589416\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.552687\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.505174\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.373960\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.494048\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.496966\n",
      "Step 5000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.57\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.362843\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.707248\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.714944\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.628849\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.694020\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.684400\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.559286\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.498556\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.443653\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.504309\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.540124\n",
      "Step 5000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 93.2\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.475049\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.741236\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.759468\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.701489\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.707770\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.701232\n",
      "Step 2500 accuracy: 78.1%\n",
      "Step 3000 loss: 0.684129\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.581671\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.462661\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.552316\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.628911\n",
      "Step 5000 accuracy: 80.5%\n",
      "Accuracy in test dataset: 92.67\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.621124\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.863340\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.785175\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.687660\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.777727\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.921694\n",
      "Step 2500 accuracy: 75.8%\n",
      "Step 3000 loss: 0.795369\n",
      "Step 3000 accuracy: 78.9%\n",
      "Step 3500 loss: 0.624207\n",
      "Step 3500 accuracy: 84.4%\n",
      "Step 4000 loss: 0.459750\n",
      "Step 4000 accuracy: 85.2%\n",
      "Step 4500 loss: 0.613203\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.623998\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.86\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.358518\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.721594\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.685238\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.595206\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.649552\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.631042\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.544783\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.481570\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.385767\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.510075\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.522955\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.449542\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.427147\n",
      "Step 6000 accuracy: 89.8%\n",
      "Step 6500 loss: 0.382040\n",
      "Step 6500 accuracy: 89.1%\n",
      "Step 7000 loss: 0.473649\n",
      "Step 7000 accuracy: 87.5%\n",
      "Step 7500 loss: 0.473552\n",
      "Step 7500 accuracy: 85.9%\n",
      "Step 8000 loss: 0.247675\n",
      "Step 8000 accuracy: 93.0%\n",
      "Step 8500 loss: 0.323697\n",
      "Step 8500 accuracy: 91.4%\n",
      "Step 9000 loss: 0.332932\n",
      "Step 9000 accuracy: 91.4%\n",
      "Step 9500 loss: 0.367961\n",
      "Step 9500 accuracy: 89.1%\n",
      "Step 10000 loss: 0.591882\n",
      "Step 10000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 94.1\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.316499\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.744344\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.684694\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.637467\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.658251\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.687792\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.565699\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.499499\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.428284\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.516654\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.550144\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.531929\n",
      "Step 5500 accuracy: 85.2%\n",
      "Step 6000 loss: 0.492508\n",
      "Step 6000 accuracy: 90.6%\n",
      "Step 6500 loss: 0.380962\n",
      "Step 6500 accuracy: 89.8%\n",
      "Step 7000 loss: 0.484570\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.524930\n",
      "Step 7500 accuracy: 85.2%\n",
      "Step 8000 loss: 0.284987\n",
      "Step 8000 accuracy: 92.2%\n",
      "Step 8500 loss: 0.384307\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.403556\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.434023\n",
      "Step 9500 accuracy: 83.6%\n",
      "Step 10000 loss: 0.631879\n",
      "Step 10000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.9\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.437628\n",
      "Step 0 accuracy: 5.5%\n",
      "Step 500 loss: 0.775638\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.827146\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.678286\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.772778\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.743174\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.659244\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.614913\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.490216\n",
      "Step 4000 accuracy: 85.2%\n",
      "Step 4500 loss: 0.616617\n",
      "Step 4500 accuracy: 81.2%\n",
      "Step 5000 loss: 0.605477\n",
      "Step 5000 accuracy: 82.0%\n",
      "Step 5500 loss: 0.691056\n",
      "Step 5500 accuracy: 81.2%\n",
      "Step 6000 loss: 0.606634\n",
      "Step 6000 accuracy: 85.9%\n",
      "Step 6500 loss: 0.476478\n",
      "Step 6500 accuracy: 85.9%\n",
      "Step 7000 loss: 0.678461\n",
      "Step 7000 accuracy: 82.0%\n",
      "Step 7500 loss: 0.582668\n",
      "Step 7500 accuracy: 84.4%\n",
      "Step 8000 loss: 0.390641\n",
      "Step 8000 accuracy: 88.3%\n",
      "Step 8500 loss: 0.441554\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.535940\n",
      "Step 9000 accuracy: 83.6%\n",
      "Step 9500 loss: 0.490947\n",
      "Step 9500 accuracy: 83.6%\n",
      "Step 10000 loss: 0.603899\n",
      "Step 10000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 92.92\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.334706\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 1.022268\n",
      "Step 500 accuracy: 68.8%\n",
      "Step 1000 loss: 0.769826\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.727296\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.775186\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.829338\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.862757\n",
      "Step 3000 accuracy: 78.9%\n",
      "Step 3500 loss: 0.622040\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.600435\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.683116\n",
      "Step 4500 accuracy: 78.1%\n",
      "Step 5000 loss: 0.730052\n",
      "Step 5000 accuracy: 78.9%\n",
      "Step 5500 loss: 0.699997\n",
      "Step 5500 accuracy: 81.2%\n",
      "Step 6000 loss: 0.659426\n",
      "Step 6000 accuracy: 82.0%\n",
      "Step 6500 loss: 0.632258\n",
      "Step 6500 accuracy: 83.6%\n",
      "Step 7000 loss: 0.728163\n",
      "Step 7000 accuracy: 82.8%\n",
      "Step 7500 loss: 0.687683\n",
      "Step 7500 accuracy: 78.9%\n",
      "Step 8000 loss: 0.456380\n",
      "Step 8000 accuracy: 83.6%\n",
      "Step 8500 loss: 0.500140\n",
      "Step 8500 accuracy: 85.9%\n",
      "Step 9000 loss: 0.601971\n",
      "Step 9000 accuracy: 83.6%\n",
      "Step 9500 loss: 0.657696\n",
      "Step 9500 accuracy: 77.3%\n",
      "Step 10000 loss: 0.659913\n",
      "Step 10000 accuracy: 78.9%\n",
      "Accuracy in test dataset: 91.9\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.525300\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 1.131468\n",
      "Step 500 accuracy: 68.8%\n",
      "Step 1000 loss: 1.078894\n",
      "Step 1000 accuracy: 64.1%\n",
      "Step 1500 loss: 0.952614\n",
      "Step 1500 accuracy: 72.7%\n",
      "Step 2000 loss: 0.936252\n",
      "Step 2000 accuracy: 72.7%\n",
      "Step 2500 loss: 1.155925\n",
      "Step 2500 accuracy: 69.5%\n",
      "Step 3000 loss: 0.778832\n",
      "Step 3000 accuracy: 75.8%\n",
      "Step 3500 loss: 0.921041\n",
      "Step 3500 accuracy: 78.1%\n",
      "Step 4000 loss: 0.759908\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.777218\n",
      "Step 4500 accuracy: 75.8%\n",
      "Step 5000 loss: 0.871475\n",
      "Step 5000 accuracy: 76.6%\n",
      "Step 5500 loss: 0.981942\n",
      "Step 5500 accuracy: 71.1%\n",
      "Step 6000 loss: 0.788721\n",
      "Step 6000 accuracy: 78.9%\n",
      "Step 6500 loss: 0.733904\n",
      "Step 6500 accuracy: 78.9%\n",
      "Step 7000 loss: 0.856452\n",
      "Step 7000 accuracy: 75.0%\n",
      "Step 7500 loss: 0.870615\n",
      "Step 7500 accuracy: 76.6%\n",
      "Step 8000 loss: 0.716165\n",
      "Step 8000 accuracy: 78.1%\n",
      "Step 8500 loss: 0.670322\n",
      "Step 8500 accuracy: 82.0%\n",
      "Step 9000 loss: 0.891793\n",
      "Step 9000 accuracy: 78.1%\n",
      "Step 9500 loss: 0.784581\n",
      "Step 9500 accuracy: 73.4%\n",
      "Step 10000 loss: 0.791023\n",
      "Step 10000 accuracy: 78.9%\n",
      "Accuracy in test dataset: 90.69\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.303484\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.702814\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.665378\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.593639\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.617217\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.625954\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.536201\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.486016\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.393304\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.502735\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.544596\n",
      "Step 5000 accuracy: 83.6%\n",
      "Step 5500 loss: 0.457278\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.408215\n",
      "Step 6000 accuracy: 88.3%\n",
      "Step 6500 loss: 0.376428\n",
      "Step 6500 accuracy: 89.8%\n",
      "Step 7000 loss: 0.441967\n",
      "Step 7000 accuracy: 87.5%\n",
      "Step 7500 loss: 0.459834\n",
      "Step 7500 accuracy: 89.8%\n",
      "Step 8000 loss: 0.232599\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.343633\n",
      "Step 8500 accuracy: 89.1%\n",
      "Step 9000 loss: 0.329350\n",
      "Step 9000 accuracy: 91.4%\n",
      "Step 9500 loss: 0.366065\n",
      "Step 9500 accuracy: 89.1%\n",
      "Step 10000 loss: 0.569253\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 94.51\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.401671\n",
      "Step 0 accuracy: 3.9%\n",
      "Step 500 loss: 0.716554\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.689405\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.614673\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.675625\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.669720\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.554317\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.495628\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.389298\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.545862\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.518091\n",
      "Step 5000 accuracy: 84.4%\n",
      "Step 5500 loss: 0.469636\n",
      "Step 5500 accuracy: 86.7%\n",
      "Step 6000 loss: 0.439427\n",
      "Step 6000 accuracy: 89.8%\n",
      "Step 6500 loss: 0.388598\n",
      "Step 6500 accuracy: 87.5%\n",
      "Step 7000 loss: 0.502133\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.508917\n",
      "Step 7500 accuracy: 83.6%\n",
      "Step 8000 loss: 0.261142\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.331075\n",
      "Step 8500 accuracy: 89.1%\n",
      "Step 9000 loss: 0.374786\n",
      "Step 9000 accuracy: 89.8%\n",
      "Step 9500 loss: 0.387978\n",
      "Step 9500 accuracy: 88.3%\n",
      "Step 10000 loss: 0.579610\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 94.21\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.359582\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.763547\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.744292\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.647835\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.684329\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.731250\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.619408\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.585852\n",
      "Step 3500 accuracy: 84.4%\n",
      "Step 4000 loss: 0.477916\n",
      "Step 4000 accuracy: 86.7%\n",
      "Step 4500 loss: 0.585051\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.581554\n",
      "Step 5000 accuracy: 83.6%\n",
      "Step 5500 loss: 0.545374\n",
      "Step 5500 accuracy: 85.9%\n",
      "Step 6000 loss: 0.496413\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.450206\n",
      "Step 6500 accuracy: 85.2%\n",
      "Step 7000 loss: 0.613200\n",
      "Step 7000 accuracy: 82.8%\n",
      "Step 7500 loss: 0.594766\n",
      "Step 7500 accuracy: 83.6%\n",
      "Step 8000 loss: 0.314174\n",
      "Step 8000 accuracy: 89.8%\n",
      "Step 8500 loss: 0.423442\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.493785\n",
      "Step 9000 accuracy: 86.7%\n",
      "Step 9500 loss: 0.456540\n",
      "Step 9500 accuracy: 84.4%\n",
      "Step 10000 loss: 0.606881\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 93.52\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.450671\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.799316\n",
      "Step 500 accuracy: 73.4%\n",
      "Step 1000 loss: 0.869967\n",
      "Step 1000 accuracy: 75.0%\n",
      "Step 1500 loss: 0.735769\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.724373\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.829183\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.639446\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.582950\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.543164\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.639247\n",
      "Step 4500 accuracy: 80.5%\n",
      "Step 5000 loss: 0.648002\n",
      "Step 5000 accuracy: 83.6%\n",
      "Step 5500 loss: 0.628830\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.686716\n",
      "Step 6000 accuracy: 85.9%\n",
      "Step 6500 loss: 0.556356\n",
      "Step 6500 accuracy: 82.8%\n",
      "Step 7000 loss: 0.657354\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.589837\n",
      "Step 7500 accuracy: 80.5%\n",
      "Step 8000 loss: 0.403007\n",
      "Step 8000 accuracy: 88.3%\n",
      "Step 8500 loss: 0.463975\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.588466\n",
      "Step 9000 accuracy: 85.2%\n",
      "Step 9500 loss: 0.504449\n",
      "Step 9500 accuracy: 82.0%\n",
      "Step 10000 loss: 0.683888\n",
      "Step 10000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.55\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.620330\n",
      "Step 0 accuracy: 4.7%\n",
      "Step 500 loss: 0.983315\n",
      "Step 500 accuracy: 75.0%\n",
      "Step 1000 loss: 0.962760\n",
      "Step 1000 accuracy: 66.4%\n",
      "Step 1500 loss: 0.842856\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.883658\n",
      "Step 2000 accuracy: 71.1%\n",
      "Step 2500 loss: 0.865074\n",
      "Step 2500 accuracy: 74.2%\n",
      "Step 3000 loss: 0.760033\n",
      "Step 3000 accuracy: 79.7%\n",
      "Step 3500 loss: 0.803730\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.620202\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.744099\n",
      "Step 4500 accuracy: 80.5%\n",
      "Step 5000 loss: 0.744048\n",
      "Step 5000 accuracy: 78.1%\n",
      "Step 5500 loss: 0.775515\n",
      "Step 5500 accuracy: 79.7%\n",
      "Step 6000 loss: 0.620495\n",
      "Step 6000 accuracy: 85.2%\n",
      "Step 6500 loss: 0.621389\n",
      "Step 6500 accuracy: 83.6%\n",
      "Step 7000 loss: 0.707364\n",
      "Step 7000 accuracy: 82.8%\n",
      "Step 7500 loss: 0.748359\n",
      "Step 7500 accuracy: 78.1%\n",
      "Step 8000 loss: 0.509023\n",
      "Step 8000 accuracy: 82.0%\n",
      "Step 8500 loss: 0.553585\n",
      "Step 8500 accuracy: 82.8%\n",
      "Step 9000 loss: 0.661605\n",
      "Step 9000 accuracy: 83.6%\n",
      "Step 9500 loss: 0.734190\n",
      "Step 9500 accuracy: 78.9%\n",
      "Step 10000 loss: 0.629761\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 91.58\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.336812\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.707334\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.671478\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.592148\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.630833\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.627241\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.545538\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.479563\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.385823\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.487085\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.493291\n",
      "Step 5000 accuracy: 85.2%\n",
      "Step 5500 loss: 0.424815\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.407673\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.326446\n",
      "Step 6500 accuracy: 92.2%\n",
      "Step 7000 loss: 0.460207\n",
      "Step 7000 accuracy: 86.7%\n",
      "Step 7500 loss: 0.430590\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.231924\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.319085\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.321027\n",
      "Step 9000 accuracy: 92.2%\n",
      "Step 9500 loss: 0.382883\n",
      "Step 9500 accuracy: 89.1%\n",
      "Step 10000 loss: 0.560048\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 94.62\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.309156\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.735853\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.667398\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.608364\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.637648\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.629254\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.512387\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.508803\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.400674\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.509958\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.511429\n",
      "Step 5000 accuracy: 85.2%\n",
      "Step 5500 loss: 0.450467\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.439898\n",
      "Step 6000 accuracy: 89.1%\n",
      "Step 6500 loss: 0.392321\n",
      "Step 6500 accuracy: 90.6%\n",
      "Step 7000 loss: 0.481478\n",
      "Step 7000 accuracy: 85.9%\n",
      "Step 7500 loss: 0.472656\n",
      "Step 7500 accuracy: 86.7%\n",
      "Step 8000 loss: 0.261051\n",
      "Step 8000 accuracy: 92.2%\n",
      "Step 8500 loss: 0.292422\n",
      "Step 8500 accuracy: 91.4%\n",
      "Step 9000 loss: 0.357203\n",
      "Step 9000 accuracy: 90.6%\n",
      "Step 9500 loss: 0.404065\n",
      "Step 9500 accuracy: 86.7%\n",
      "Step 10000 loss: 0.555151\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 94.45\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.383831\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.796716\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.692902\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.616006\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.714301\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.684494\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.598583\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.536624\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.437337\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.525213\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.591396\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.497585\n",
      "Step 5500 accuracy: 85.9%\n",
      "Step 6000 loss: 0.487258\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.433467\n",
      "Step 6500 accuracy: 89.1%\n",
      "Step 7000 loss: 0.542174\n",
      "Step 7000 accuracy: 85.9%\n",
      "Step 7500 loss: 0.561036\n",
      "Step 7500 accuracy: 82.8%\n",
      "Step 8000 loss: 0.280781\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.380634\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.427475\n",
      "Step 9000 accuracy: 86.7%\n",
      "Step 9500 loss: 0.423275\n",
      "Step 9500 accuracy: 86.7%\n",
      "Step 10000 loss: 0.615180\n",
      "Step 10000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.97\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.370239\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.797290\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.802808\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.718607\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.715591\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.766825\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.672565\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.572116\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.460909\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.621103\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.660434\n",
      "Step 5000 accuracy: 79.7%\n",
      "Step 5500 loss: 0.567442\n",
      "Step 5500 accuracy: 85.2%\n",
      "Step 6000 loss: 0.560663\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.499501\n",
      "Step 6500 accuracy: 84.4%\n",
      "Step 7000 loss: 0.611732\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.607541\n",
      "Step 7500 accuracy: 79.7%\n",
      "Step 8000 loss: 0.371563\n",
      "Step 8000 accuracy: 86.7%\n",
      "Step 8500 loss: 0.437977\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.536495\n",
      "Step 9000 accuracy: 82.8%\n",
      "Step 9500 loss: 0.481205\n",
      "Step 9500 accuracy: 82.8%\n",
      "Step 10000 loss: 0.638353\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 93.22\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.563573\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.885150\n",
      "Step 500 accuracy: 74.2%\n",
      "Step 1000 loss: 0.937950\n",
      "Step 1000 accuracy: 73.4%\n",
      "Step 1500 loss: 0.828851\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.817673\n",
      "Step 2000 accuracy: 75.0%\n",
      "Step 2500 loss: 0.908411\n",
      "Step 2500 accuracy: 76.6%\n",
      "Step 3000 loss: 0.793680\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.673863\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.668882\n",
      "Step 4000 accuracy: 83.6%\n",
      "Step 4500 loss: 0.671654\n",
      "Step 4500 accuracy: 80.5%\n",
      "Step 5000 loss: 0.695601\n",
      "Step 5000 accuracy: 78.9%\n",
      "Step 5500 loss: 0.684289\n",
      "Step 5500 accuracy: 82.8%\n",
      "Step 6000 loss: 0.542593\n",
      "Step 6000 accuracy: 88.3%\n",
      "Step 6500 loss: 0.493823\n",
      "Step 6500 accuracy: 85.2%\n",
      "Step 7000 loss: 0.703604\n",
      "Step 7000 accuracy: 82.0%\n",
      "Step 7500 loss: 0.671576\n",
      "Step 7500 accuracy: 78.9%\n",
      "Step 8000 loss: 0.465782\n",
      "Step 8000 accuracy: 86.7%\n",
      "Step 8500 loss: 0.481493\n",
      "Step 8500 accuracy: 84.4%\n",
      "Step 9000 loss: 0.643931\n",
      "Step 9000 accuracy: 86.7%\n",
      "Step 9500 loss: 0.551250\n",
      "Step 9500 accuracy: 82.8%\n",
      "Step 10000 loss: 0.701441\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.26\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.306121\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.697236\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.665256\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.573020\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.624274\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.607560\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.520808\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.449080\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.371278\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.493451\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.494298\n",
      "Step 5000 accuracy: 85.9%\n",
      "Step 5500 loss: 0.423269\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.410150\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.358757\n",
      "Step 6500 accuracy: 89.8%\n",
      "Step 7000 loss: 0.425593\n",
      "Step 7000 accuracy: 86.7%\n",
      "Step 7500 loss: 0.430570\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.216100\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.301391\n",
      "Step 8500 accuracy: 90.6%\n",
      "Step 9000 loss: 0.326737\n",
      "Step 9000 accuracy: 91.4%\n",
      "Step 9500 loss: 0.337302\n",
      "Step 9500 accuracy: 89.8%\n",
      "Step 10000 loss: 0.550054\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 94.77\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.368783\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.700819\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.699835\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.592421\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.612530\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.607257\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.550124\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.461308\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.402848\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.495780\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.499816\n",
      "Step 5000 accuracy: 84.4%\n",
      "Step 5500 loss: 0.439237\n",
      "Step 5500 accuracy: 86.7%\n",
      "Step 6000 loss: 0.404660\n",
      "Step 6000 accuracy: 88.3%\n",
      "Step 6500 loss: 0.363393\n",
      "Step 6500 accuracy: 89.8%\n",
      "Step 7000 loss: 0.445161\n",
      "Step 7000 accuracy: 86.7%\n",
      "Step 7500 loss: 0.434490\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.231988\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.300930\n",
      "Step 8500 accuracy: 92.2%\n",
      "Step 9000 loss: 0.349389\n",
      "Step 9000 accuracy: 91.4%\n",
      "Step 9500 loss: 0.374209\n",
      "Step 9500 accuracy: 89.1%\n",
      "Step 10000 loss: 0.605522\n",
      "Step 10000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 94.64\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.357124\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.711979\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.740978\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.651685\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.692613\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.641064\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.608917\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.538632\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.472445\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.553086\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.541577\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.478456\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.471757\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.401916\n",
      "Step 6500 accuracy: 88.3%\n",
      "Step 7000 loss: 0.550678\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.518849\n",
      "Step 7500 accuracy: 83.6%\n",
      "Step 8000 loss: 0.273971\n",
      "Step 8000 accuracy: 93.0%\n",
      "Step 8500 loss: 0.350493\n",
      "Step 8500 accuracy: 90.6%\n",
      "Step 9000 loss: 0.450586\n",
      "Step 9000 accuracy: 85.2%\n",
      "Step 9500 loss: 0.444500\n",
      "Step 9500 accuracy: 88.3%\n",
      "Step 10000 loss: 0.602886\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 94.32\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.453520\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.812001\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.757694\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.685239\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.735747\n",
      "Step 2000 accuracy: 75.8%\n",
      "Step 2500 loss: 0.784857\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.675200\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.635098\n",
      "Step 3500 accuracy: 81.2%\n",
      "Step 4000 loss: 0.422610\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.567504\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.651584\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.572991\n",
      "Step 5500 accuracy: 87.5%\n",
      "Step 6000 loss: 0.522091\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.459447\n",
      "Step 6500 accuracy: 84.4%\n",
      "Step 7000 loss: 0.536587\n",
      "Step 7000 accuracy: 85.9%\n",
      "Step 7500 loss: 0.609828\n",
      "Step 7500 accuracy: 81.2%\n",
      "Step 8000 loss: 0.327103\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.442331\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.429407\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.494757\n",
      "Step 9500 accuracy: 84.4%\n",
      "Step 10000 loss: 0.623859\n",
      "Step 10000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 93.54\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.579311\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.870928\n",
      "Step 500 accuracy: 71.9%\n",
      "Step 1000 loss: 0.828842\n",
      "Step 1000 accuracy: 75.8%\n",
      "Step 1500 loss: 0.680387\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.784190\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.776202\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.730558\n",
      "Step 3000 accuracy: 79.7%\n",
      "Step 3500 loss: 0.650095\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.535296\n",
      "Step 4000 accuracy: 86.7%\n",
      "Step 4500 loss: 0.595026\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.627987\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.714638\n",
      "Step 5500 accuracy: 80.5%\n",
      "Step 6000 loss: 0.588311\n",
      "Step 6000 accuracy: 83.6%\n",
      "Step 6500 loss: 0.530469\n",
      "Step 6500 accuracy: 82.8%\n",
      "Step 7000 loss: 0.681037\n",
      "Step 7000 accuracy: 83.6%\n",
      "Step 7500 loss: 0.752160\n",
      "Step 7500 accuracy: 78.1%\n",
      "Step 8000 loss: 0.417791\n",
      "Step 8000 accuracy: 87.5%\n",
      "Step 8500 loss: 0.423368\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.590130\n",
      "Step 9000 accuracy: 85.9%\n",
      "Step 9500 loss: 0.569223\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.670384\n",
      "Step 10000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 92.75\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.315286\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.635729\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.593897\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.616583\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.508427\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.597123\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 91.78\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.368602\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.664708\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.632971\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.629507\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.540123\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.651112\n",
      "Step 2500 accuracy: 84.0%\n",
      "Accuracy in test dataset: 91.53\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.360622\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.726095\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.696042\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.682341\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.585671\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.697221\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 91.09\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.503629\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.814252\n",
      "Step 500 accuracy: 74.6%\n",
      "Step 1000 loss: 0.758776\n",
      "Step 1000 accuracy: 79.3%\n",
      "Step 1500 loss: 0.815919\n",
      "Step 1500 accuracy: 77.7%\n",
      "Step 2000 loss: 0.667764\n",
      "Step 2000 accuracy: 78.5%\n",
      "Step 2500 loss: 0.822638\n",
      "Step 2500 accuracy: 77.0%\n",
      "Accuracy in test dataset: 90.74\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.532069\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 1.022457\n",
      "Step 500 accuracy: 69.5%\n",
      "Step 1000 loss: 0.855139\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.922005\n",
      "Step 1500 accuracy: 75.4%\n",
      "Step 2000 loss: 0.795738\n",
      "Step 2000 accuracy: 75.8%\n",
      "Step 2500 loss: 0.919513\n",
      "Step 2500 accuracy: 72.3%\n",
      "Accuracy in test dataset: 90.0\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.311876\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.646819\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.601294\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.615306\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.507187\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.587029\n",
      "Step 2500 accuracy: 83.2%\n",
      "Accuracy in test dataset: 91.94\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.386184\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.662917\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.614128\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.602368\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.518751\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.613881\n",
      "Step 2500 accuracy: 83.2%\n",
      "Accuracy in test dataset: 91.83\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.372059\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.704359\n",
      "Step 500 accuracy: 76.2%\n",
      "Step 1000 loss: 0.653676\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.647923\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.577368\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.673403\n",
      "Step 2500 accuracy: 81.2%\n",
      "Accuracy in test dataset: 91.54\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.441727\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.785666\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.722093\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.774892\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.696945\n",
      "Step 2000 accuracy: 79.3%\n",
      "Step 2500 loss: 0.760017\n",
      "Step 2500 accuracy: 79.3%\n",
      "Accuracy in test dataset: 91.01\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.625339\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.806182\n",
      "Step 500 accuracy: 74.6%\n",
      "Step 1000 loss: 0.904344\n",
      "Step 1000 accuracy: 77.7%\n",
      "Step 1500 loss: 0.815091\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.697826\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.885332\n",
      "Step 2500 accuracy: 77.3%\n",
      "Accuracy in test dataset: 90.53\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.335698\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.632837\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.593006\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.605499\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.514222\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.576020\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 92.11\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.384252\n",
      "Step 0 accuracy: 5.9%\n",
      "Step 500 loss: 0.667665\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.615217\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.615686\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.521964\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.593276\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.08\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.341420\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.668155\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.658577\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.660127\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.553155\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.680347\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.74\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.464770\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.705873\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.698194\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.734219\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.605905\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.750201\n",
      "Step 2500 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.38\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.520937\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.805385\n",
      "Step 500 accuracy: 76.2%\n",
      "Step 1000 loss: 0.745567\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.782755\n",
      "Step 1500 accuracy: 77.3%\n",
      "Step 2000 loss: 0.647423\n",
      "Step 2000 accuracy: 80.9%\n",
      "Step 2500 loss: 0.713821\n",
      "Step 2500 accuracy: 78.9%\n",
      "Accuracy in test dataset: 90.82\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.326164\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.632753\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.596727\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.588125\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.494310\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.559157\n",
      "Step 2500 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.34\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.314455\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.650059\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.606733\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.619797\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.502671\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.583867\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.11\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.391745\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.663391\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.625538\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.677699\n",
      "Step 1500 accuracy: 80.9%\n",
      "Step 2000 loss: 0.543680\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.619678\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.01\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.432439\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.729545\n",
      "Step 500 accuracy: 75.0%\n",
      "Step 1000 loss: 0.663889\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.700432\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.560220\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.651834\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.56\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.628509\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.729102\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.730295\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.706131\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.647265\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.757681\n",
      "Step 2500 accuracy: 79.3%\n",
      "Accuracy in test dataset: 91.26\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.323146\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.636191\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.608435\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.611111\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.515796\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.601629\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.483936\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.386218\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.616225\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.417389\n",
      "Step 4500 accuracy: 88.7%\n",
      "Step 5000 loss: 0.491888\n",
      "Step 5000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 93.2\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.282606\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.667738\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.651428\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.656011\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.530064\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.637009\n",
      "Step 2500 accuracy: 83.2%\n",
      "Step 3000 loss: 0.535746\n",
      "Step 3000 accuracy: 85.5%\n",
      "Step 3500 loss: 0.414993\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.657347\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.466385\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.546354\n",
      "Step 5000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 92.73\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.329185\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.752897\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.709008\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.657875\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.581257\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.704527\n",
      "Step 2500 accuracy: 80.9%\n",
      "Step 3000 loss: 0.595990\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.437652\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.712633\n",
      "Step 4000 accuracy: 78.1%\n",
      "Step 4500 loss: 0.547371\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.641873\n",
      "Step 5000 accuracy: 80.9%\n",
      "Accuracy in test dataset: 92.22\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.498994\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.834870\n",
      "Step 500 accuracy: 73.8%\n",
      "Step 1000 loss: 0.747456\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.794250\n",
      "Step 1500 accuracy: 78.1%\n",
      "Step 2000 loss: 0.663017\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.780593\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.666779\n",
      "Step 3000 accuracy: 82.4%\n",
      "Step 3500 loss: 0.550030\n",
      "Step 3500 accuracy: 82.8%\n",
      "Step 4000 loss: 0.754012\n",
      "Step 4000 accuracy: 75.4%\n",
      "Step 4500 loss: 0.635791\n",
      "Step 4500 accuracy: 80.9%\n",
      "Step 5000 loss: 0.707459\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 91.35\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.514821\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 1.012206\n",
      "Step 500 accuracy: 71.1%\n",
      "Step 1000 loss: 0.880744\n",
      "Step 1000 accuracy: 76.6%\n",
      "Step 1500 loss: 0.877472\n",
      "Step 1500 accuracy: 75.0%\n",
      "Step 2000 loss: 0.800518\n",
      "Step 2000 accuracy: 75.0%\n",
      "Step 2500 loss: 0.969566\n",
      "Step 2500 accuracy: 75.4%\n",
      "Step 3000 loss: 0.842612\n",
      "Step 3000 accuracy: 74.6%\n",
      "Step 3500 loss: 0.648833\n",
      "Step 3500 accuracy: 80.9%\n",
      "Step 4000 loss: 0.882571\n",
      "Step 4000 accuracy: 72.3%\n",
      "Step 4500 loss: 0.754725\n",
      "Step 4500 accuracy: 78.9%\n",
      "Step 5000 loss: 0.985579\n",
      "Step 5000 accuracy: 74.6%\n",
      "Accuracy in test dataset: 90.55\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.304906\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.644394\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.609042\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.605136\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.514922\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.584187\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.470691\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.372208\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.609076\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.429031\n",
      "Step 4500 accuracy: 87.1%\n",
      "Step 5000 loss: 0.497895\n",
      "Step 5000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 92.97\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.315631\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.658486\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.611918\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.620780\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.530325\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.606006\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.490882\n",
      "Step 3000 accuracy: 88.3%\n",
      "Step 3500 loss: 0.389420\n",
      "Step 3500 accuracy: 89.5%\n",
      "Step 4000 loss: 0.640865\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.447051\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.496631\n",
      "Step 5000 accuracy: 85.5%\n",
      "Accuracy in test dataset: 93.23\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.310016\n",
      "Step 0 accuracy: 15.2%\n",
      "Step 500 loss: 0.730920\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.646519\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.677100\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.569628\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.691760\n",
      "Step 2500 accuracy: 79.3%\n",
      "Step 3000 loss: 0.528763\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.437497\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.663144\n",
      "Step 4000 accuracy: 78.9%\n",
      "Step 4500 loss: 0.517160\n",
      "Step 4500 accuracy: 84.8%\n",
      "Step 5000 loss: 0.609866\n",
      "Step 5000 accuracy: 82.4%\n",
      "Accuracy in test dataset: 92.5\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.432186\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.785582\n",
      "Step 500 accuracy: 77.0%\n",
      "Step 1000 loss: 0.698105\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.742108\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.571170\n",
      "Step 2000 accuracy: 80.9%\n",
      "Step 2500 loss: 0.720013\n",
      "Step 2500 accuracy: 79.3%\n",
      "Step 3000 loss: 0.629116\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.480863\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.770114\n",
      "Step 4000 accuracy: 74.2%\n",
      "Step 4500 loss: 0.583380\n",
      "Step 4500 accuracy: 81.2%\n",
      "Step 5000 loss: 0.619316\n",
      "Step 5000 accuracy: 84.0%\n",
      "Accuracy in test dataset: 91.83\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.636915\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.877192\n",
      "Step 500 accuracy: 73.8%\n",
      "Step 1000 loss: 0.788617\n",
      "Step 1000 accuracy: 79.3%\n",
      "Step 1500 loss: 0.827035\n",
      "Step 1500 accuracy: 75.4%\n",
      "Step 2000 loss: 0.735519\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.895634\n",
      "Step 2500 accuracy: 76.6%\n",
      "Step 3000 loss: 0.726136\n",
      "Step 3000 accuracy: 80.5%\n",
      "Step 3500 loss: 0.566702\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.793880\n",
      "Step 4000 accuracy: 76.2%\n",
      "Step 4500 loss: 0.671302\n",
      "Step 4500 accuracy: 83.2%\n",
      "Step 5000 loss: 0.784172\n",
      "Step 5000 accuracy: 80.9%\n",
      "Accuracy in test dataset: 91.2\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.316425\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.632596\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.597667\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.610067\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.503934\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.574374\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.465905\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.368529\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.590177\n",
      "Step 4000 accuracy: 81.6%\n",
      "Step 4500 loss: 0.416447\n",
      "Step 4500 accuracy: 88.7%\n",
      "Step 5000 loss: 0.469827\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.43\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.314221\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.657512\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.614166\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.602561\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.517643\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.583740\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.491070\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.378480\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.599760\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.437231\n",
      "Step 4500 accuracy: 87.5%\n",
      "Step 5000 loss: 0.481103\n",
      "Step 5000 accuracy: 84.8%\n",
      "Accuracy in test dataset: 93.34\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.394000\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.691627\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.641105\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.647195\n",
      "Step 1500 accuracy: 81.6%\n",
      "Step 2000 loss: 0.574456\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.646847\n",
      "Step 2500 accuracy: 80.1%\n",
      "Step 3000 loss: 0.537408\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.438668\n",
      "Step 3500 accuracy: 87.1%\n",
      "Step 4000 loss: 0.667683\n",
      "Step 4000 accuracy: 79.3%\n",
      "Step 4500 loss: 0.486020\n",
      "Step 4500 accuracy: 85.5%\n",
      "Step 5000 loss: 0.572222\n",
      "Step 5000 accuracy: 83.2%\n",
      "Accuracy in test dataset: 92.73\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.552519\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.776758\n",
      "Step 500 accuracy: 76.6%\n",
      "Step 1000 loss: 0.675990\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.718158\n",
      "Step 1500 accuracy: 80.1%\n",
      "Step 2000 loss: 0.588233\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.748723\n",
      "Step 2500 accuracy: 79.3%\n",
      "Step 3000 loss: 0.570716\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.460946\n",
      "Step 3500 accuracy: 85.5%\n",
      "Step 4000 loss: 0.669161\n",
      "Step 4000 accuracy: 78.9%\n",
      "Step 4500 loss: 0.567683\n",
      "Step 4500 accuracy: 82.4%\n",
      "Step 5000 loss: 0.634518\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.28\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.537395\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.856701\n",
      "Step 500 accuracy: 75.0%\n",
      "Step 1000 loss: 0.719490\n",
      "Step 1000 accuracy: 80.1%\n",
      "Step 1500 loss: 0.773709\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.689550\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.815225\n",
      "Step 2500 accuracy: 78.1%\n",
      "Step 3000 loss: 0.631877\n",
      "Step 3000 accuracy: 84.0%\n",
      "Step 3500 loss: 0.486068\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.778014\n",
      "Step 4000 accuracy: 78.9%\n",
      "Step 4500 loss: 0.607262\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.738371\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 91.52\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.306361\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.623089\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.582793\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.604751\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.499638\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.575504\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.467337\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.356930\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.576687\n",
      "Step 4000 accuracy: 80.9%\n",
      "Step 4500 loss: 0.408644\n",
      "Step 4500 accuracy: 89.8%\n",
      "Step 5000 loss: 0.455099\n",
      "Step 5000 accuracy: 86.3%\n",
      "Accuracy in test dataset: 93.53\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.390872\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.648892\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.600891\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.605443\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.506649\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.578295\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.470039\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.385614\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.593134\n",
      "Step 4000 accuracy: 80.9%\n",
      "Step 4500 loss: 0.427156\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.495893\n",
      "Step 5000 accuracy: 84.0%\n",
      "Accuracy in test dataset: 93.39\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.427598\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.659514\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.635060\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.647451\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.539471\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.642877\n",
      "Step 2500 accuracy: 83.2%\n",
      "Step 3000 loss: 0.515008\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.403666\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.652368\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.453846\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.525673\n",
      "Step 5000 accuracy: 84.8%\n",
      "Accuracy in test dataset: 93.0\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.445090\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.664312\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.680315\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.665339\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.590697\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.677620\n",
      "Step 2500 accuracy: 81.6%\n",
      "Step 3000 loss: 0.568133\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.470660\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.692243\n",
      "Step 4000 accuracy: 80.1%\n",
      "Step 4500 loss: 0.511643\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.583582\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 92.52\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.530571\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.809699\n",
      "Step 500 accuracy: 75.4%\n",
      "Step 1000 loss: 0.653677\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.737325\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.664730\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.771386\n",
      "Step 2500 accuracy: 78.1%\n",
      "Step 3000 loss: 0.620813\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.522493\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.794349\n",
      "Step 4000 accuracy: 77.0%\n",
      "Step 4500 loss: 0.563013\n",
      "Step 4500 accuracy: 83.2%\n",
      "Step 5000 loss: 0.717758\n",
      "Step 5000 accuracy: 81.2%\n",
      "Accuracy in test dataset: 92.01\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.295038\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.647722\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.609744\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.617657\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.520252\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.598854\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.485529\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.392870\n",
      "Step 3500 accuracy: 87.9%\n",
      "Step 4000 loss: 0.625059\n",
      "Step 4000 accuracy: 80.1%\n",
      "Step 4500 loss: 0.427324\n",
      "Step 4500 accuracy: 87.5%\n",
      "Step 5000 loss: 0.497690\n",
      "Step 5000 accuracy: 85.5%\n",
      "Step 5500 loss: 0.518034\n",
      "Step 5500 accuracy: 85.5%\n",
      "Step 6000 loss: 0.476956\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.453846\n",
      "Step 6500 accuracy: 86.7%\n",
      "Step 7000 loss: 0.316544\n",
      "Step 7000 accuracy: 89.8%\n",
      "Step 7500 loss: 0.385048\n",
      "Step 7500 accuracy: 87.9%\n",
      "Step 8000 loss: 0.337607\n",
      "Step 8000 accuracy: 90.2%\n",
      "Step 8500 loss: 0.403013\n",
      "Step 8500 accuracy: 87.9%\n",
      "Step 9000 loss: 0.447157\n",
      "Step 9000 accuracy: 87.1%\n",
      "Step 9500 loss: 0.357647\n",
      "Step 9500 accuracy: 88.7%\n",
      "Step 10000 loss: 0.322970\n",
      "Step 10000 accuracy: 91.4%\n",
      "Accuracy in test dataset: 94.2\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.325201\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.671346\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.641219\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.649845\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.541863\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.615962\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.524779\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.415146\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.643187\n",
      "Step 4000 accuracy: 80.1%\n",
      "Step 4500 loss: 0.457149\n",
      "Step 4500 accuracy: 87.5%\n",
      "Step 5000 loss: 0.529144\n",
      "Step 5000 accuracy: 84.8%\n",
      "Step 5500 loss: 0.569162\n",
      "Step 5500 accuracy: 82.4%\n",
      "Step 6000 loss: 0.498148\n",
      "Step 6000 accuracy: 85.2%\n",
      "Step 6500 loss: 0.536326\n",
      "Step 6500 accuracy: 84.4%\n",
      "Step 7000 loss: 0.348282\n",
      "Step 7000 accuracy: 90.2%\n",
      "Step 7500 loss: 0.410254\n",
      "Step 7500 accuracy: 88.7%\n",
      "Step 8000 loss: 0.350406\n",
      "Step 8000 accuracy: 89.8%\n",
      "Step 8500 loss: 0.452111\n",
      "Step 8500 accuracy: 86.3%\n",
      "Step 9000 loss: 0.452818\n",
      "Step 9000 accuracy: 87.1%\n",
      "Step 9500 loss: 0.435865\n",
      "Step 9500 accuracy: 86.7%\n",
      "Step 10000 loss: 0.354029\n",
      "Step 10000 accuracy: 90.6%\n",
      "Accuracy in test dataset: 93.95\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.357935\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.724272\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.686575\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.703109\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.598227\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.756820\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.555397\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.483574\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.718615\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.531720\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.645503\n",
      "Step 5000 accuracy: 83.2%\n",
      "Step 5500 loss: 0.661851\n",
      "Step 5500 accuracy: 80.1%\n",
      "Step 6000 loss: 0.577151\n",
      "Step 6000 accuracy: 82.0%\n",
      "Step 6500 loss: 0.573136\n",
      "Step 6500 accuracy: 82.8%\n",
      "Step 7000 loss: 0.397447\n",
      "Step 7000 accuracy: 89.5%\n",
      "Step 7500 loss: 0.503884\n",
      "Step 7500 accuracy: 86.3%\n",
      "Step 8000 loss: 0.474129\n",
      "Step 8000 accuracy: 84.0%\n",
      "Step 8500 loss: 0.546629\n",
      "Step 8500 accuracy: 81.2%\n",
      "Step 9000 loss: 0.580673\n",
      "Step 9000 accuracy: 84.0%\n",
      "Step 9500 loss: 0.503771\n",
      "Step 9500 accuracy: 86.7%\n",
      "Step 10000 loss: 0.428510\n",
      "Step 10000 accuracy: 89.8%\n",
      "Accuracy in test dataset: 92.82\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.458457\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.839926\n",
      "Step 500 accuracy: 72.7%\n",
      "Step 1000 loss: 0.782582\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.802550\n",
      "Step 1500 accuracy: 80.9%\n",
      "Step 2000 loss: 0.655702\n",
      "Step 2000 accuracy: 79.3%\n",
      "Step 2500 loss: 0.845378\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.673038\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.536961\n",
      "Step 3500 accuracy: 84.8%\n",
      "Step 4000 loss: 0.820151\n",
      "Step 4000 accuracy: 75.8%\n",
      "Step 4500 loss: 0.631705\n",
      "Step 4500 accuracy: 81.6%\n",
      "Step 5000 loss: 0.738868\n",
      "Step 5000 accuracy: 80.5%\n",
      "Step 5500 loss: 0.712231\n",
      "Step 5500 accuracy: 79.7%\n",
      "Step 6000 loss: 0.686248\n",
      "Step 6000 accuracy: 82.8%\n",
      "Step 6500 loss: 0.726371\n",
      "Step 6500 accuracy: 79.7%\n",
      "Step 7000 loss: 0.511506\n",
      "Step 7000 accuracy: 87.5%\n",
      "Step 7500 loss: 0.603028\n",
      "Step 7500 accuracy: 83.2%\n",
      "Step 8000 loss: 0.575636\n",
      "Step 8000 accuracy: 84.4%\n",
      "Step 8500 loss: 0.611959\n",
      "Step 8500 accuracy: 81.2%\n",
      "Step 9000 loss: 0.695259\n",
      "Step 9000 accuracy: 80.1%\n",
      "Step 9500 loss: 0.579810\n",
      "Step 9500 accuracy: 82.8%\n",
      "Step 10000 loss: 0.509753\n",
      "Step 10000 accuracy: 84.8%\n",
      "Accuracy in test dataset: 91.9\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.540977\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.906708\n",
      "Step 500 accuracy: 70.3%\n",
      "Step 1000 loss: 0.841801\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.933372\n",
      "Step 1500 accuracy: 75.4%\n",
      "Step 2000 loss: 0.771712\n",
      "Step 2000 accuracy: 76.2%\n",
      "Step 2500 loss: 1.069074\n",
      "Step 2500 accuracy: 70.7%\n",
      "Step 3000 loss: 0.854943\n",
      "Step 3000 accuracy: 77.7%\n",
      "Step 3500 loss: 0.681547\n",
      "Step 3500 accuracy: 79.7%\n",
      "Step 4000 loss: 0.855849\n",
      "Step 4000 accuracy: 74.2%\n",
      "Step 4500 loss: 0.768776\n",
      "Step 4500 accuracy: 77.3%\n",
      "Step 5000 loss: 0.892267\n",
      "Step 5000 accuracy: 75.8%\n",
      "Step 5500 loss: 0.817083\n",
      "Step 5500 accuracy: 77.0%\n",
      "Step 6000 loss: 0.816201\n",
      "Step 6000 accuracy: 74.2%\n",
      "Step 6500 loss: 0.890137\n",
      "Step 6500 accuracy: 74.6%\n",
      "Step 7000 loss: 0.678521\n",
      "Step 7000 accuracy: 81.2%\n",
      "Step 7500 loss: 0.746494\n",
      "Step 7500 accuracy: 82.8%\n",
      "Step 8000 loss: 0.694873\n",
      "Step 8000 accuracy: 80.1%\n",
      "Step 8500 loss: 0.736818\n",
      "Step 8500 accuracy: 77.7%\n",
      "Step 9000 loss: 0.801059\n",
      "Step 9000 accuracy: 76.6%\n",
      "Step 9500 loss: 0.684365\n",
      "Step 9500 accuracy: 83.2%\n",
      "Step 10000 loss: 0.675156\n",
      "Step 10000 accuracy: 79.3%\n",
      "Accuracy in test dataset: 90.81\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.345419\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.651652\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.607777\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.605120\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.506326\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.585701\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.474894\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.376382\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.616886\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.422379\n",
      "Step 4500 accuracy: 89.5%\n",
      "Step 5000 loss: 0.492771\n",
      "Step 5000 accuracy: 85.5%\n",
      "Step 5500 loss: 0.529889\n",
      "Step 5500 accuracy: 84.4%\n",
      "Step 6000 loss: 0.457593\n",
      "Step 6000 accuracy: 87.1%\n",
      "Step 6500 loss: 0.459276\n",
      "Step 6500 accuracy: 87.1%\n",
      "Step 7000 loss: 0.295980\n",
      "Step 7000 accuracy: 90.6%\n",
      "Step 7500 loss: 0.368158\n",
      "Step 7500 accuracy: 87.5%\n",
      "Step 8000 loss: 0.332377\n",
      "Step 8000 accuracy: 91.0%\n",
      "Step 8500 loss: 0.398746\n",
      "Step 8500 accuracy: 87.9%\n",
      "Step 9000 loss: 0.408958\n",
      "Step 9000 accuracy: 87.9%\n",
      "Step 9500 loss: 0.356702\n",
      "Step 9500 accuracy: 89.5%\n",
      "Step 10000 loss: 0.313439\n",
      "Step 10000 accuracy: 91.8%\n",
      "Accuracy in test dataset: 94.42\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.347497\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.657728\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.605628\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.613115\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.509641\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.617799\n",
      "Step 2500 accuracy: 83.6%\n",
      "Step 3000 loss: 0.507648\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.390643\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.617536\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.444459\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.535563\n",
      "Step 5000 accuracy: 83.6%\n",
      "Step 5500 loss: 0.547149\n",
      "Step 5500 accuracy: 82.8%\n",
      "Step 6000 loss: 0.490073\n",
      "Step 6000 accuracy: 85.9%\n",
      "Step 6500 loss: 0.492115\n",
      "Step 6500 accuracy: 86.3%\n",
      "Step 7000 loss: 0.331448\n",
      "Step 7000 accuracy: 90.6%\n",
      "Step 7500 loss: 0.395621\n",
      "Step 7500 accuracy: 87.5%\n",
      "Step 8000 loss: 0.355414\n",
      "Step 8000 accuracy: 90.6%\n",
      "Step 8500 loss: 0.422343\n",
      "Step 8500 accuracy: 88.3%\n",
      "Step 9000 loss: 0.462756\n",
      "Step 9000 accuracy: 88.3%\n",
      "Step 9500 loss: 0.403960\n",
      "Step 9500 accuracy: 87.5%\n",
      "Step 10000 loss: 0.336553\n",
      "Step 10000 accuracy: 90.6%\n",
      "Accuracy in test dataset: 94.28\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.381995\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.706436\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.660403\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.678796\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.572432\n",
      "Step 2000 accuracy: 81.6%\n",
      "Step 2500 loss: 0.654479\n",
      "Step 2500 accuracy: 80.9%\n",
      "Step 3000 loss: 0.559983\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.453417\n",
      "Step 3500 accuracy: 85.5%\n",
      "Step 4000 loss: 0.668044\n",
      "Step 4000 accuracy: 79.3%\n",
      "Step 4500 loss: 0.515253\n",
      "Step 4500 accuracy: 85.5%\n",
      "Step 5000 loss: 0.581979\n",
      "Step 5000 accuracy: 84.0%\n",
      "Step 5500 loss: 0.610823\n",
      "Step 5500 accuracy: 81.6%\n",
      "Step 6000 loss: 0.560118\n",
      "Step 6000 accuracy: 85.5%\n",
      "Step 6500 loss: 0.550858\n",
      "Step 6500 accuracy: 84.0%\n",
      "Step 7000 loss: 0.420301\n",
      "Step 7000 accuracy: 88.3%\n",
      "Step 7500 loss: 0.459306\n",
      "Step 7500 accuracy: 86.3%\n",
      "Step 8000 loss: 0.404896\n",
      "Step 8000 accuracy: 87.5%\n",
      "Step 8500 loss: 0.495772\n",
      "Step 8500 accuracy: 84.4%\n",
      "Step 9000 loss: 0.533921\n",
      "Step 9000 accuracy: 84.4%\n",
      "Step 9500 loss: 0.498908\n",
      "Step 9500 accuracy: 84.8%\n",
      "Step 10000 loss: 0.403477\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.59\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.438851\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.793810\n",
      "Step 500 accuracy: 74.6%\n",
      "Step 1000 loss: 0.727074\n",
      "Step 1000 accuracy: 80.1%\n",
      "Step 1500 loss: 0.745905\n",
      "Step 1500 accuracy: 77.7%\n",
      "Step 2000 loss: 0.606795\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.730491\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.614176\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.477025\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.739899\n",
      "Step 4000 accuracy: 75.8%\n",
      "Step 4500 loss: 0.600595\n",
      "Step 4500 accuracy: 81.2%\n",
      "Step 5000 loss: 0.692867\n",
      "Step 5000 accuracy: 82.0%\n",
      "Step 5500 loss: 0.643348\n",
      "Step 5500 accuracy: 82.0%\n",
      "Step 6000 loss: 0.642556\n",
      "Step 6000 accuracy: 80.5%\n",
      "Step 6500 loss: 0.631730\n",
      "Step 6500 accuracy: 79.3%\n",
      "Step 7000 loss: 0.448038\n",
      "Step 7000 accuracy: 87.1%\n",
      "Step 7500 loss: 0.548029\n",
      "Step 7500 accuracy: 85.2%\n",
      "Step 8000 loss: 0.517053\n",
      "Step 8000 accuracy: 85.5%\n",
      "Step 8500 loss: 0.614197\n",
      "Step 8500 accuracy: 82.0%\n",
      "Step 9000 loss: 0.654291\n",
      "Step 9000 accuracy: 81.6%\n",
      "Step 9500 loss: 0.542611\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.474527\n",
      "Step 10000 accuracy: 87.1%\n",
      "Accuracy in test dataset: 92.6\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.543223\n",
      "Step 0 accuracy: 11.3%\n",
      "Step 500 loss: 0.818757\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.792796\n",
      "Step 1000 accuracy: 78.5%\n",
      "Step 1500 loss: 0.782457\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.756260\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.836316\n",
      "Step 2500 accuracy: 75.4%\n",
      "Step 3000 loss: 0.732275\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.588755\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.779909\n",
      "Step 4000 accuracy: 77.7%\n",
      "Step 4500 loss: 0.645204\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.661804\n",
      "Step 5000 accuracy: 81.6%\n",
      "Step 5500 loss: 0.789140\n",
      "Step 5500 accuracy: 74.6%\n",
      "Step 6000 loss: 0.693746\n",
      "Step 6000 accuracy: 80.5%\n",
      "Step 6500 loss: 0.764672\n",
      "Step 6500 accuracy: 78.1%\n",
      "Step 7000 loss: 0.595823\n",
      "Step 7000 accuracy: 83.2%\n",
      "Step 7500 loss: 0.534035\n",
      "Step 7500 accuracy: 84.4%\n",
      "Step 8000 loss: 0.585827\n",
      "Step 8000 accuracy: 82.0%\n",
      "Step 8500 loss: 0.655916\n",
      "Step 8500 accuracy: 79.7%\n",
      "Step 9000 loss: 0.700978\n",
      "Step 9000 accuracy: 80.5%\n",
      "Step 9500 loss: 0.654473\n",
      "Step 9500 accuracy: 79.3%\n",
      "Step 10000 loss: 0.557621\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 91.62\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.376040\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.641686\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.600411\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.593612\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.513826\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.571158\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.468933\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.367754\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.591408\n",
      "Step 4000 accuracy: 80.1%\n",
      "Step 4500 loss: 0.417273\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.477180\n",
      "Step 5000 accuracy: 85.9%\n",
      "Step 5500 loss: 0.505339\n",
      "Step 5500 accuracy: 85.2%\n",
      "Step 6000 loss: 0.439753\n",
      "Step 6000 accuracy: 87.9%\n",
      "Step 6500 loss: 0.442663\n",
      "Step 6500 accuracy: 87.5%\n",
      "Step 7000 loss: 0.292574\n",
      "Step 7000 accuracy: 91.4%\n",
      "Step 7500 loss: 0.350458\n",
      "Step 7500 accuracy: 90.2%\n",
      "Step 8000 loss: 0.316538\n",
      "Step 8000 accuracy: 91.8%\n",
      "Step 8500 loss: 0.391005\n",
      "Step 8500 accuracy: 88.3%\n",
      "Step 9000 loss: 0.406503\n",
      "Step 9000 accuracy: 88.3%\n",
      "Step 9500 loss: 0.342167\n",
      "Step 9500 accuracy: 90.2%\n",
      "Step 10000 loss: 0.301049\n",
      "Step 10000 accuracy: 91.8%\n",
      "Accuracy in test dataset: 94.59\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.316694\n",
      "Step 0 accuracy: 14.5%\n",
      "Step 500 loss: 0.630393\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.600240\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.616543\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.513426\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.594461\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.480730\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.383310\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.595264\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.419037\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.514639\n",
      "Step 5000 accuracy: 83.6%\n",
      "Step 5500 loss: 0.548142\n",
      "Step 5500 accuracy: 84.8%\n",
      "Step 6000 loss: 0.456222\n",
      "Step 6000 accuracy: 88.3%\n",
      "Step 6500 loss: 0.482436\n",
      "Step 6500 accuracy: 86.3%\n",
      "Step 7000 loss: 0.299213\n",
      "Step 7000 accuracy: 90.2%\n",
      "Step 7500 loss: 0.383026\n",
      "Step 7500 accuracy: 87.5%\n",
      "Step 8000 loss: 0.371230\n",
      "Step 8000 accuracy: 89.5%\n",
      "Step 8500 loss: 0.412630\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.447909\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.388444\n",
      "Step 9500 accuracy: 87.9%\n",
      "Step 10000 loss: 0.305694\n",
      "Step 10000 accuracy: 92.2%\n",
      "Accuracy in test dataset: 94.37\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.292717\n",
      "Step 0 accuracy: 15.2%\n",
      "Step 500 loss: 0.681977\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.642991\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.662415\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.587683\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.631730\n",
      "Step 2500 accuracy: 82.4%\n",
      "Step 3000 loss: 0.522529\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.435459\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.686315\n",
      "Step 4000 accuracy: 79.3%\n",
      "Step 4500 loss: 0.489969\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.532359\n",
      "Step 5000 accuracy: 87.9%\n",
      "Step 5500 loss: 0.586073\n",
      "Step 5500 accuracy: 82.4%\n",
      "Step 6000 loss: 0.550702\n",
      "Step 6000 accuracy: 85.5%\n",
      "Step 6500 loss: 0.515635\n",
      "Step 6500 accuracy: 82.8%\n",
      "Step 7000 loss: 0.386068\n",
      "Step 7000 accuracy: 89.8%\n",
      "Step 7500 loss: 0.440246\n",
      "Step 7500 accuracy: 87.1%\n",
      "Step 8000 loss: 0.388526\n",
      "Step 8000 accuracy: 89.8%\n",
      "Step 8500 loss: 0.474226\n",
      "Step 8500 accuracy: 85.5%\n",
      "Step 9000 loss: 0.511998\n",
      "Step 9000 accuracy: 84.0%\n",
      "Step 9500 loss: 0.430613\n",
      "Step 9500 accuracy: 87.5%\n",
      "Step 10000 loss: 0.372552\n",
      "Step 10000 accuracy: 89.5%\n",
      "Accuracy in test dataset: 93.91\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.427909\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.779929\n",
      "Step 500 accuracy: 75.4%\n",
      "Step 1000 loss: 0.682504\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.704940\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.609116\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.690599\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.562664\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.453216\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.724660\n",
      "Step 4000 accuracy: 78.9%\n",
      "Step 4500 loss: 0.511480\n",
      "Step 4500 accuracy: 84.0%\n",
      "Step 5000 loss: 0.678280\n",
      "Step 5000 accuracy: 81.6%\n",
      "Step 5500 loss: 0.622151\n",
      "Step 5500 accuracy: 82.0%\n",
      "Step 6000 loss: 0.560602\n",
      "Step 6000 accuracy: 85.2%\n",
      "Step 6500 loss: 0.573008\n",
      "Step 6500 accuracy: 85.2%\n",
      "Step 7000 loss: 0.427307\n",
      "Step 7000 accuracy: 86.7%\n",
      "Step 7500 loss: 0.504737\n",
      "Step 7500 accuracy: 84.8%\n",
      "Step 8000 loss: 0.466406\n",
      "Step 8000 accuracy: 85.9%\n",
      "Step 8500 loss: 0.483305\n",
      "Step 8500 accuracy: 86.3%\n",
      "Step 9000 loss: 0.590983\n",
      "Step 9000 accuracy: 83.2%\n",
      "Step 9500 loss: 0.546405\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.409774\n",
      "Step 10000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 93.13\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.657278\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.882259\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.755060\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.790149\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.612639\n",
      "Step 2000 accuracy: 80.1%\n",
      "Step 2500 loss: 0.767968\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.652028\n",
      "Step 3000 accuracy: 84.0%\n",
      "Step 3500 loss: 0.508583\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.794643\n",
      "Step 4000 accuracy: 75.0%\n",
      "Step 4500 loss: 0.606383\n",
      "Step 4500 accuracy: 81.2%\n",
      "Step 5000 loss: 0.734903\n",
      "Step 5000 accuracy: 81.2%\n",
      "Step 5500 loss: 0.666033\n",
      "Step 5500 accuracy: 77.7%\n",
      "Step 6000 loss: 0.690129\n",
      "Step 6000 accuracy: 82.0%\n",
      "Step 6500 loss: 0.738142\n",
      "Step 6500 accuracy: 79.7%\n",
      "Step 7000 loss: 0.505595\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.620702\n",
      "Step 7500 accuracy: 82.8%\n",
      "Step 8000 loss: 0.558317\n",
      "Step 8000 accuracy: 82.8%\n",
      "Step 8500 loss: 0.570048\n",
      "Step 8500 accuracy: 83.2%\n",
      "Step 9000 loss: 0.681802\n",
      "Step 9000 accuracy: 82.4%\n",
      "Step 9500 loss: 0.568551\n",
      "Step 9500 accuracy: 83.6%\n",
      "Step 10000 loss: 0.503796\n",
      "Step 10000 accuracy: 86.3%\n",
      "Accuracy in test dataset: 92.13\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.303103\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.627093\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.590176\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.600053\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.496566\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.565209\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.464435\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.363761\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.583692\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.401536\n",
      "Step 4500 accuracy: 90.2%\n",
      "Step 5000 loss: 0.460796\n",
      "Step 5000 accuracy: 86.7%\n",
      "Step 5500 loss: 0.504505\n",
      "Step 5500 accuracy: 85.5%\n",
      "Step 6000 loss: 0.421954\n",
      "Step 6000 accuracy: 87.1%\n",
      "Step 6500 loss: 0.428876\n",
      "Step 6500 accuracy: 86.7%\n",
      "Step 7000 loss: 0.284609\n",
      "Step 7000 accuracy: 90.6%\n",
      "Step 7500 loss: 0.336851\n",
      "Step 7500 accuracy: 91.8%\n",
      "Step 8000 loss: 0.318751\n",
      "Step 8000 accuracy: 90.6%\n",
      "Step 8500 loss: 0.371835\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.385695\n",
      "Step 9000 accuracy: 88.3%\n",
      "Step 9500 loss: 0.344398\n",
      "Step 9500 accuracy: 89.8%\n",
      "Step 10000 loss: 0.298338\n",
      "Step 10000 accuracy: 92.2%\n",
      "Accuracy in test dataset: 94.73\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.330923\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.641636\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.599547\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.610574\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.510560\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.583016\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.474269\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.377577\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.606002\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.421608\n",
      "Step 4500 accuracy: 89.1%\n",
      "Step 5000 loss: 0.485139\n",
      "Step 5000 accuracy: 86.3%\n",
      "Step 5500 loss: 0.519699\n",
      "Step 5500 accuracy: 84.8%\n",
      "Step 6000 loss: 0.443085\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.450464\n",
      "Step 6500 accuracy: 86.3%\n",
      "Step 7000 loss: 0.307214\n",
      "Step 7000 accuracy: 91.4%\n",
      "Step 7500 loss: 0.373272\n",
      "Step 7500 accuracy: 88.7%\n",
      "Step 8000 loss: 0.323761\n",
      "Step 8000 accuracy: 90.2%\n",
      "Step 8500 loss: 0.408654\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.410748\n",
      "Step 9000 accuracy: 88.3%\n",
      "Step 9500 loss: 0.350868\n",
      "Step 9500 accuracy: 89.8%\n",
      "Step 10000 loss: 0.344886\n",
      "Step 10000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 94.57\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.351416\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.660085\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.623589\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.623908\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.530498\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.623047\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.470914\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.422361\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.638311\n",
      "Step 4000 accuracy: 80.9%\n",
      "Step 4500 loss: 0.495355\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.526289\n",
      "Step 5000 accuracy: 85.2%\n",
      "Step 5500 loss: 0.547284\n",
      "Step 5500 accuracy: 85.2%\n",
      "Step 6000 loss: 0.496600\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.479883\n",
      "Step 6500 accuracy: 86.3%\n",
      "Step 7000 loss: 0.329570\n",
      "Step 7000 accuracy: 90.2%\n",
      "Step 7500 loss: 0.417835\n",
      "Step 7500 accuracy: 87.9%\n",
      "Step 8000 loss: 0.388862\n",
      "Step 8000 accuracy: 89.1%\n",
      "Step 8500 loss: 0.442419\n",
      "Step 8500 accuracy: 87.1%\n",
      "Step 9000 loss: 0.475260\n",
      "Step 9000 accuracy: 86.3%\n",
      "Step 9500 loss: 0.421866\n",
      "Step 9500 accuracy: 87.1%\n",
      "Step 10000 loss: 0.336252\n",
      "Step 10000 accuracy: 90.6%\n",
      "Accuracy in test dataset: 94.28\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.457891\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.731973\n",
      "Step 500 accuracy: 77.0%\n",
      "Step 1000 loss: 0.686697\n",
      "Step 1000 accuracy: 81.6%\n",
      "Step 1500 loss: 0.697471\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.598695\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.691101\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.542206\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.434170\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.703621\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.531284\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.589475\n",
      "Step 5000 accuracy: 84.0%\n",
      "Step 5500 loss: 0.621181\n",
      "Step 5500 accuracy: 82.0%\n",
      "Step 6000 loss: 0.526582\n",
      "Step 6000 accuracy: 84.0%\n",
      "Step 6500 loss: 0.557585\n",
      "Step 6500 accuracy: 84.0%\n",
      "Step 7000 loss: 0.410374\n",
      "Step 7000 accuracy: 88.3%\n",
      "Step 7500 loss: 0.451867\n",
      "Step 7500 accuracy: 87.1%\n",
      "Step 8000 loss: 0.440095\n",
      "Step 8000 accuracy: 87.5%\n",
      "Step 8500 loss: 0.492235\n",
      "Step 8500 accuracy: 85.5%\n",
      "Step 9000 loss: 0.515433\n",
      "Step 9000 accuracy: 84.4%\n",
      "Step 9500 loss: 0.473625\n",
      "Step 9500 accuracy: 86.3%\n",
      "Step 10000 loss: 0.401111\n",
      "Step 10000 accuracy: 88.7%\n",
      "Accuracy in test dataset: 93.56\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.611192\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.780153\n",
      "Step 500 accuracy: 77.0%\n",
      "Step 1000 loss: 0.754399\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.735985\n",
      "Step 1500 accuracy: 78.1%\n",
      "Step 2000 loss: 0.635104\n",
      "Step 2000 accuracy: 80.9%\n",
      "Step 2500 loss: 0.784625\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.626095\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.501926\n",
      "Step 3500 accuracy: 85.5%\n",
      "Step 4000 loss: 0.768797\n",
      "Step 4000 accuracy: 77.0%\n",
      "Step 4500 loss: 0.552940\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.658530\n",
      "Step 5000 accuracy: 80.9%\n",
      "Step 5500 loss: 0.651201\n",
      "Step 5500 accuracy: 80.1%\n",
      "Step 6000 loss: 0.609412\n",
      "Step 6000 accuracy: 82.0%\n",
      "Step 6500 loss: 0.583893\n",
      "Step 6500 accuracy: 82.0%\n",
      "Step 7000 loss: 0.473281\n",
      "Step 7000 accuracy: 87.1%\n",
      "Step 7500 loss: 0.576820\n",
      "Step 7500 accuracy: 85.2%\n",
      "Step 8000 loss: 0.551764\n",
      "Step 8000 accuracy: 83.2%\n",
      "Step 8500 loss: 0.537083\n",
      "Step 8500 accuracy: 84.4%\n",
      "Step 9000 loss: 0.664448\n",
      "Step 9000 accuracy: 82.0%\n",
      "Step 9500 loss: 0.566940\n",
      "Step 9500 accuracy: 85.9%\n",
      "Step 10000 loss: 0.454764\n",
      "Step 10000 accuracy: 87.9%\n",
      "Accuracy in test dataset: 92.73\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.307558\n",
      "Step 0 accuracy: 11.9%\n",
      "Step 500 loss: 0.584272\n",
      "Step 500 accuracy: 83.2%\n",
      "Step 1000 loss: 0.583200\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.488962\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.552874\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.475214\n",
      "Step 2500 accuracy: 85.5%\n",
      "Accuracy in test dataset: 91.84\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.352964\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.598634\n",
      "Step 500 accuracy: 83.2%\n",
      "Step 1000 loss: 0.618003\n",
      "Step 1000 accuracy: 82.2%\n",
      "Step 1500 loss: 0.503345\n",
      "Step 1500 accuracy: 86.5%\n",
      "Step 2000 loss: 0.586767\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.484634\n",
      "Step 2500 accuracy: 85.9%\n",
      "Accuracy in test dataset: 91.68\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.332762\n",
      "Step 0 accuracy: 11.9%\n",
      "Step 500 loss: 0.665109\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.643961\n",
      "Step 1000 accuracy: 80.3%\n",
      "Step 1500 loss: 0.564362\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.660783\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.534339\n",
      "Step 2500 accuracy: 84.8%\n",
      "Accuracy in test dataset: 91.2\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.494613\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.770335\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.743511\n",
      "Step 1000 accuracy: 78.5%\n",
      "Step 1500 loss: 0.642630\n",
      "Step 1500 accuracy: 81.4%\n",
      "Step 2000 loss: 0.752326\n",
      "Step 2000 accuracy: 80.1%\n",
      "Step 2500 loss: 0.635508\n",
      "Step 2500 accuracy: 82.2%\n",
      "Accuracy in test dataset: 90.76\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.594124\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.895057\n",
      "Step 500 accuracy: 72.9%\n",
      "Step 1000 loss: 0.854491\n",
      "Step 1000 accuracy: 73.8%\n",
      "Step 1500 loss: 0.811412\n",
      "Step 1500 accuracy: 77.5%\n",
      "Step 2000 loss: 0.886490\n",
      "Step 2000 accuracy: 74.0%\n",
      "Step 2500 loss: 0.771729\n",
      "Step 2500 accuracy: 79.1%\n",
      "Accuracy in test dataset: 90.18\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.263021\n",
      "Step 0 accuracy: 17.0%\n",
      "Step 500 loss: 0.573856\n",
      "Step 500 accuracy: 84.4%\n",
      "Step 1000 loss: 0.573219\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.482079\n",
      "Step 1500 accuracy: 87.1%\n",
      "Step 2000 loss: 0.547957\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.464389\n",
      "Step 2500 accuracy: 86.5%\n",
      "Accuracy in test dataset: 92.01\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.361247\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.599748\n",
      "Step 500 accuracy: 83.8%\n",
      "Step 1000 loss: 0.582018\n",
      "Step 1000 accuracy: 82.6%\n",
      "Step 1500 loss: 0.502095\n",
      "Step 1500 accuracy: 85.0%\n",
      "Step 2000 loss: 0.562452\n",
      "Step 2000 accuracy: 83.8%\n",
      "Step 2500 loss: 0.482310\n",
      "Step 2500 accuracy: 86.3%\n",
      "Accuracy in test dataset: 91.87\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.445997\n",
      "Step 0 accuracy: 6.6%\n",
      "Step 500 loss: 0.641172\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.633826\n",
      "Step 1000 accuracy: 81.8%\n",
      "Step 1500 loss: 0.542629\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.609561\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.532963\n",
      "Step 2500 accuracy: 84.2%\n",
      "Accuracy in test dataset: 91.51\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.490666\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.698878\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.686254\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.630873\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.681113\n",
      "Step 2000 accuracy: 78.7%\n",
      "Step 2500 loss: 0.573125\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 91.03\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.603765\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.792987\n",
      "Step 500 accuracy: 78.3%\n",
      "Step 1000 loss: 0.759630\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.656829\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.739802\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.692003\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 90.61\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.308571\n",
      "Step 0 accuracy: 13.1%\n",
      "Step 500 loss: 0.566147\n",
      "Step 500 accuracy: 83.8%\n",
      "Step 1000 loss: 0.569338\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.467156\n",
      "Step 1500 accuracy: 87.7%\n",
      "Step 2000 loss: 0.544708\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.460096\n",
      "Step 2500 accuracy: 85.9%\n",
      "Accuracy in test dataset: 92.08\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.332251\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.575019\n",
      "Step 500 accuracy: 83.4%\n",
      "Step 1000 loss: 0.567638\n",
      "Step 1000 accuracy: 83.4%\n",
      "Step 1500 loss: 0.487812\n",
      "Step 1500 accuracy: 86.9%\n",
      "Step 2000 loss: 0.556747\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.475372\n",
      "Step 2500 accuracy: 85.7%\n",
      "Accuracy in test dataset: 92.01\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.399634\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.606979\n",
      "Step 500 accuracy: 83.2%\n",
      "Step 1000 loss: 0.600665\n",
      "Step 1000 accuracy: 81.6%\n",
      "Step 1500 loss: 0.534572\n",
      "Step 1500 accuracy: 84.2%\n",
      "Step 2000 loss: 0.602286\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.524141\n",
      "Step 2500 accuracy: 85.5%\n",
      "Accuracy in test dataset: 91.83\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.467761\n",
      "Step 0 accuracy: 6.4%\n",
      "Step 500 loss: 0.664789\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.657212\n",
      "Step 1000 accuracy: 80.7%\n",
      "Step 1500 loss: 0.600745\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.625522\n",
      "Step 2000 accuracy: 81.6%\n",
      "Step 2500 loss: 0.558570\n",
      "Step 2500 accuracy: 83.4%\n",
      "Accuracy in test dataset: 91.34\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.592371\n",
      "Step 0 accuracy: 8.0%\n",
      "Step 500 loss: 0.791033\n",
      "Step 500 accuracy: 76.8%\n",
      "Step 1000 loss: 0.740344\n",
      "Step 1000 accuracy: 77.7%\n",
      "Step 1500 loss: 0.644883\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.711608\n",
      "Step 2000 accuracy: 80.3%\n",
      "Step 2500 loss: 0.643418\n",
      "Step 2500 accuracy: 83.0%\n",
      "Accuracy in test dataset: 91.0\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.361393\n",
      "Step 0 accuracy: 9.2%\n",
      "Step 500 loss: 0.556823\n",
      "Step 500 accuracy: 85.2%\n",
      "Step 1000 loss: 0.564348\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.468635\n",
      "Step 1500 accuracy: 88.1%\n",
      "Step 2000 loss: 0.534638\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.446706\n",
      "Step 2500 accuracy: 86.1%\n",
      "Accuracy in test dataset: 92.48\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.325716\n",
      "Step 0 accuracy: 12.3%\n",
      "Step 500 loss: 0.568330\n",
      "Step 500 accuracy: 84.8%\n",
      "Step 1000 loss: 0.558629\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.467951\n",
      "Step 1500 accuracy: 87.1%\n",
      "Step 2000 loss: 0.532152\n",
      "Step 2000 accuracy: 84.2%\n",
      "Step 2500 loss: 0.451736\n",
      "Step 2500 accuracy: 86.9%\n",
      "Accuracy in test dataset: 92.11\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.394944\n",
      "Step 0 accuracy: 10.7%\n",
      "Step 500 loss: 0.604950\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.584542\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.516803\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.568578\n",
      "Step 2000 accuracy: 83.4%\n",
      "Step 2500 loss: 0.487660\n",
      "Step 2500 accuracy: 85.2%\n",
      "Accuracy in test dataset: 92.07\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.478111\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.623277\n",
      "Step 500 accuracy: 83.2%\n",
      "Step 1000 loss: 0.611454\n",
      "Step 1000 accuracy: 82.6%\n",
      "Step 1500 loss: 0.564756\n",
      "Step 1500 accuracy: 84.6%\n",
      "Step 2000 loss: 0.622870\n",
      "Step 2000 accuracy: 81.8%\n",
      "Step 2500 loss: 0.537090\n",
      "Step 2500 accuracy: 85.2%\n",
      "Accuracy in test dataset: 91.72\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.589118\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.692882\n",
      "Step 500 accuracy: 79.5%\n",
      "Step 1000 loss: 0.660392\n",
      "Step 1000 accuracy: 79.9%\n",
      "Step 1500 loss: 0.642227\n",
      "Step 1500 accuracy: 81.8%\n",
      "Step 2000 loss: 0.697392\n",
      "Step 2000 accuracy: 79.3%\n",
      "Step 2500 loss: 0.583865\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 91.3\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.334949\n",
      "Step 0 accuracy: 8.4%\n",
      "Step 500 loss: 0.578333\n",
      "Step 500 accuracy: 84.0%\n",
      "Step 1000 loss: 0.571829\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.498037\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.553775\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.470357\n",
      "Step 2500 accuracy: 86.3%\n",
      "Step 3000 loss: 0.473299\n",
      "Step 3000 accuracy: 85.4%\n",
      "Step 3500 loss: 0.521960\n",
      "Step 3500 accuracy: 86.5%\n",
      "Step 4000 loss: 0.406049\n",
      "Step 4000 accuracy: 89.5%\n",
      "Step 4500 loss: 0.398849\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.441415\n",
      "Step 5000 accuracy: 88.5%\n",
      "Accuracy in test dataset: 93.02\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.329035\n",
      "Step 0 accuracy: 11.1%\n",
      "Step 500 loss: 0.611763\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.589718\n",
      "Step 1000 accuracy: 83.4%\n",
      "Step 1500 loss: 0.519635\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.570120\n",
      "Step 2000 accuracy: 84.2%\n",
      "Step 2500 loss: 0.489314\n",
      "Step 2500 accuracy: 85.4%\n",
      "Step 3000 loss: 0.478446\n",
      "Step 3000 accuracy: 86.1%\n",
      "Step 3500 loss: 0.561142\n",
      "Step 3500 accuracy: 85.5%\n",
      "Step 4000 loss: 0.431576\n",
      "Step 4000 accuracy: 88.7%\n",
      "Step 4500 loss: 0.441229\n",
      "Step 4500 accuracy: 87.5%\n",
      "Step 5000 loss: 0.461992\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 92.84\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.381333\n",
      "Step 0 accuracy: 8.8%\n",
      "Step 500 loss: 0.678870\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.649313\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.585440\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.627002\n",
      "Step 2000 accuracy: 83.4%\n",
      "Step 2500 loss: 0.554517\n",
      "Step 2500 accuracy: 83.2%\n",
      "Step 3000 loss: 0.553231\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.601517\n",
      "Step 3500 accuracy: 84.0%\n",
      "Step 4000 loss: 0.526347\n",
      "Step 4000 accuracy: 85.4%\n",
      "Step 4500 loss: 0.527783\n",
      "Step 4500 accuracy: 84.6%\n",
      "Step 5000 loss: 0.543176\n",
      "Step 5000 accuracy: 84.8%\n",
      "Accuracy in test dataset: 92.1\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.430368\n",
      "Step 0 accuracy: 11.9%\n",
      "Step 500 loss: 0.745567\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.722455\n",
      "Step 1000 accuracy: 78.7%\n",
      "Step 1500 loss: 0.685573\n",
      "Step 1500 accuracy: 80.7%\n",
      "Step 2000 loss: 0.672562\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.617296\n",
      "Step 2500 accuracy: 82.4%\n",
      "Step 3000 loss: 0.623045\n",
      "Step 3000 accuracy: 82.2%\n",
      "Step 3500 loss: 0.722228\n",
      "Step 3500 accuracy: 81.2%\n",
      "Step 4000 loss: 0.577668\n",
      "Step 4000 accuracy: 83.8%\n",
      "Step 4500 loss: 0.602707\n",
      "Step 4500 accuracy: 83.4%\n",
      "Step 5000 loss: 0.665091\n",
      "Step 5000 accuracy: 82.6%\n",
      "Accuracy in test dataset: 91.34\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.517427\n",
      "Step 0 accuracy: 10.0%\n",
      "Step 500 loss: 0.895766\n",
      "Step 500 accuracy: 73.2%\n",
      "Step 1000 loss: 0.847490\n",
      "Step 1000 accuracy: 76.0%\n",
      "Step 1500 loss: 0.800490\n",
      "Step 1500 accuracy: 76.4%\n",
      "Step 2000 loss: 0.855996\n",
      "Step 2000 accuracy: 76.4%\n",
      "Step 2500 loss: 0.790813\n",
      "Step 2500 accuracy: 76.8%\n",
      "Step 3000 loss: 0.769972\n",
      "Step 3000 accuracy: 79.9%\n",
      "Step 3500 loss: 0.885028\n",
      "Step 3500 accuracy: 75.8%\n",
      "Step 4000 loss: 0.686990\n",
      "Step 4000 accuracy: 80.3%\n",
      "Step 4500 loss: 0.758938\n",
      "Step 4500 accuracy: 78.9%\n",
      "Step 5000 loss: 0.813585\n",
      "Step 5000 accuracy: 80.7%\n",
      "Accuracy in test dataset: 90.49\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.334468\n",
      "Step 0 accuracy: 15.0%\n",
      "Step 500 loss: 0.571630\n",
      "Step 500 accuracy: 84.4%\n",
      "Step 1000 loss: 0.572897\n",
      "Step 1000 accuracy: 82.6%\n",
      "Step 1500 loss: 0.481016\n",
      "Step 1500 accuracy: 87.3%\n",
      "Step 2000 loss: 0.554400\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.463626\n",
      "Step 2500 accuracy: 86.9%\n",
      "Step 3000 loss: 0.449451\n",
      "Step 3000 accuracy: 85.7%\n",
      "Step 3500 loss: 0.513864\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.406175\n",
      "Step 4000 accuracy: 88.7%\n",
      "Step 4500 loss: 0.386967\n",
      "Step 4500 accuracy: 89.5%\n",
      "Step 5000 loss: 0.427723\n",
      "Step 5000 accuracy: 88.1%\n",
      "Accuracy in test dataset: 93.17\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.344918\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.583752\n",
      "Step 500 accuracy: 82.6%\n",
      "Step 1000 loss: 0.586118\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.490238\n",
      "Step 1500 accuracy: 86.3%\n",
      "Step 2000 loss: 0.578983\n",
      "Step 2000 accuracy: 83.0%\n",
      "Step 2500 loss: 0.470720\n",
      "Step 2500 accuracy: 85.9%\n",
      "Step 3000 loss: 0.467710\n",
      "Step 3000 accuracy: 85.5%\n",
      "Step 3500 loss: 0.527889\n",
      "Step 3500 accuracy: 85.4%\n",
      "Step 4000 loss: 0.431662\n",
      "Step 4000 accuracy: 87.7%\n",
      "Step 4500 loss: 0.428146\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.434333\n",
      "Step 5000 accuracy: 88.5%\n",
      "Accuracy in test dataset: 92.9\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.389127\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.617653\n",
      "Step 500 accuracy: 83.0%\n",
      "Step 1000 loss: 0.612791\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.549080\n",
      "Step 1500 accuracy: 84.2%\n",
      "Step 2000 loss: 0.605780\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.518343\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.531940\n",
      "Step 3000 accuracy: 83.2%\n",
      "Step 3500 loss: 0.603722\n",
      "Step 3500 accuracy: 83.8%\n",
      "Step 4000 loss: 0.490951\n",
      "Step 4000 accuracy: 87.3%\n",
      "Step 4500 loss: 0.480153\n",
      "Step 4500 accuracy: 85.4%\n",
      "Step 5000 loss: 0.500348\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 92.46\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.435127\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.672837\n",
      "Step 500 accuracy: 81.1%\n",
      "Step 1000 loss: 0.693741\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.640329\n",
      "Step 1500 accuracy: 82.6%\n",
      "Step 2000 loss: 0.667686\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.580638\n",
      "Step 2500 accuracy: 84.2%\n",
      "Step 3000 loss: 0.568832\n",
      "Step 3000 accuracy: 83.0%\n",
      "Step 3500 loss: 0.656144\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.552799\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.573602\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.592583\n",
      "Step 5000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.03\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.579884\n",
      "Step 0 accuracy: 6.6%\n",
      "Step 500 loss: 0.781398\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.766815\n",
      "Step 1000 accuracy: 77.9%\n",
      "Step 1500 loss: 0.731488\n",
      "Step 1500 accuracy: 80.9%\n",
      "Step 2000 loss: 0.763813\n",
      "Step 2000 accuracy: 78.7%\n",
      "Step 2500 loss: 0.683848\n",
      "Step 2500 accuracy: 81.6%\n",
      "Step 3000 loss: 0.653097\n",
      "Step 3000 accuracy: 81.8%\n",
      "Step 3500 loss: 0.714372\n",
      "Step 3500 accuracy: 80.9%\n",
      "Step 4000 loss: 0.628456\n",
      "Step 4000 accuracy: 82.2%\n",
      "Step 4500 loss: 0.633832\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.640630\n",
      "Step 5000 accuracy: 84.6%\n",
      "Accuracy in test dataset: 91.25\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.337900\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.566048\n",
      "Step 500 accuracy: 84.2%\n",
      "Step 1000 loss: 0.567573\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.480881\n",
      "Step 1500 accuracy: 87.7%\n",
      "Step 2000 loss: 0.545207\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.459651\n",
      "Step 2500 accuracy: 85.7%\n",
      "Step 3000 loss: 0.448331\n",
      "Step 3000 accuracy: 86.5%\n",
      "Step 3500 loss: 0.494053\n",
      "Step 3500 accuracy: 86.5%\n",
      "Step 4000 loss: 0.399971\n",
      "Step 4000 accuracy: 89.5%\n",
      "Step 4500 loss: 0.374916\n",
      "Step 4500 accuracy: 88.9%\n",
      "Step 5000 loss: 0.412526\n",
      "Step 5000 accuracy: 89.8%\n",
      "Accuracy in test dataset: 93.45\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.348019\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.574726\n",
      "Step 500 accuracy: 83.4%\n",
      "Step 1000 loss: 0.577568\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.494024\n",
      "Step 1500 accuracy: 87.3%\n",
      "Step 2000 loss: 0.554163\n",
      "Step 2000 accuracy: 84.6%\n",
      "Step 2500 loss: 0.476119\n",
      "Step 2500 accuracy: 85.9%\n",
      "Step 3000 loss: 0.460482\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.511546\n",
      "Step 3500 accuracy: 86.5%\n",
      "Step 4000 loss: 0.419678\n",
      "Step 4000 accuracy: 89.5%\n",
      "Step 4500 loss: 0.393389\n",
      "Step 4500 accuracy: 88.7%\n",
      "Step 5000 loss: 0.445421\n",
      "Step 5000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 93.15\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.359694\n",
      "Step 0 accuracy: 10.4%\n",
      "Step 500 loss: 0.621869\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.623731\n",
      "Step 1000 accuracy: 81.8%\n",
      "Step 1500 loss: 0.528778\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.574435\n",
      "Step 2000 accuracy: 81.6%\n",
      "Step 2500 loss: 0.513747\n",
      "Step 2500 accuracy: 85.0%\n",
      "Step 3000 loss: 0.510092\n",
      "Step 3000 accuracy: 85.7%\n",
      "Step 3500 loss: 0.544322\n",
      "Step 3500 accuracy: 86.1%\n",
      "Step 4000 loss: 0.440347\n",
      "Step 4000 accuracy: 88.1%\n",
      "Step 4500 loss: 0.455490\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.488763\n",
      "Step 5000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 92.81\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.457637\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.675869\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.700270\n",
      "Step 1000 accuracy: 80.3%\n",
      "Step 1500 loss: 0.582680\n",
      "Step 1500 accuracy: 85.0%\n",
      "Step 2000 loss: 0.648094\n",
      "Step 2000 accuracy: 81.1%\n",
      "Step 2500 loss: 0.549186\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.523513\n",
      "Step 3000 accuracy: 84.0%\n",
      "Step 3500 loss: 0.598252\n",
      "Step 3500 accuracy: 83.0%\n",
      "Step 4000 loss: 0.482271\n",
      "Step 4000 accuracy: 87.3%\n",
      "Step 4500 loss: 0.510463\n",
      "Step 4500 accuracy: 85.7%\n",
      "Step 5000 loss: 0.568216\n",
      "Step 5000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 92.28\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.585447\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.748979\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.708500\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.679460\n",
      "Step 1500 accuracy: 80.7%\n",
      "Step 2000 loss: 0.725594\n",
      "Step 2000 accuracy: 79.1%\n",
      "Step 2500 loss: 0.598363\n",
      "Step 2500 accuracy: 81.8%\n",
      "Step 3000 loss: 0.610532\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.646190\n",
      "Step 3500 accuracy: 83.0%\n",
      "Step 4000 loss: 0.586990\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.573825\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.600537\n",
      "Step 5000 accuracy: 84.8%\n",
      "Accuracy in test dataset: 91.67\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.296788\n",
      "Step 0 accuracy: 11.5%\n",
      "Step 500 loss: 0.558502\n",
      "Step 500 accuracy: 85.4%\n",
      "Step 1000 loss: 0.563605\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.465813\n",
      "Step 1500 accuracy: 87.1%\n",
      "Step 2000 loss: 0.530300\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.451822\n",
      "Step 2500 accuracy: 87.1%\n",
      "Step 3000 loss: 0.447702\n",
      "Step 3000 accuracy: 86.5%\n",
      "Step 3500 loss: 0.487326\n",
      "Step 3500 accuracy: 87.7%\n",
      "Step 4000 loss: 0.370836\n",
      "Step 4000 accuracy: 90.2%\n",
      "Step 4500 loss: 0.367041\n",
      "Step 4500 accuracy: 89.6%\n",
      "Step 5000 loss: 0.402207\n",
      "Step 5000 accuracy: 88.9%\n",
      "Accuracy in test dataset: 93.65\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.327222\n",
      "Step 0 accuracy: 14.5%\n",
      "Step 500 loss: 0.556607\n",
      "Step 500 accuracy: 85.4%\n",
      "Step 1000 loss: 0.577362\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.487569\n",
      "Step 1500 accuracy: 87.5%\n",
      "Step 2000 loss: 0.545583\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.461096\n",
      "Step 2500 accuracy: 86.7%\n",
      "Step 3000 loss: 0.458092\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.499610\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.395185\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.383234\n",
      "Step 4500 accuracy: 88.9%\n",
      "Step 5000 loss: 0.438016\n",
      "Step 5000 accuracy: 88.1%\n",
      "Accuracy in test dataset: 93.45\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.380798\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.593251\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.601377\n",
      "Step 1000 accuracy: 81.6%\n",
      "Step 1500 loss: 0.527094\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.582300\n",
      "Step 2000 accuracy: 83.0%\n",
      "Step 2500 loss: 0.497749\n",
      "Step 2500 accuracy: 85.9%\n",
      "Step 3000 loss: 0.486796\n",
      "Step 3000 accuracy: 85.7%\n",
      "Step 3500 loss: 0.542578\n",
      "Step 3500 accuracy: 85.4%\n",
      "Step 4000 loss: 0.454782\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.430443\n",
      "Step 4500 accuracy: 87.7%\n",
      "Step 5000 loss: 0.451875\n",
      "Step 5000 accuracy: 87.3%\n",
      "Accuracy in test dataset: 93.04\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.448077\n",
      "Step 0 accuracy: 11.3%\n",
      "Step 500 loss: 0.641118\n",
      "Step 500 accuracy: 82.6%\n",
      "Step 1000 loss: 0.626173\n",
      "Step 1000 accuracy: 80.7%\n",
      "Step 1500 loss: 0.544780\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.614307\n",
      "Step 2000 accuracy: 83.0%\n",
      "Step 2500 loss: 0.524794\n",
      "Step 2500 accuracy: 83.6%\n",
      "Step 3000 loss: 0.559900\n",
      "Step 3000 accuracy: 83.8%\n",
      "Step 3500 loss: 0.619812\n",
      "Step 3500 accuracy: 84.2%\n",
      "Step 4000 loss: 0.476887\n",
      "Step 4000 accuracy: 87.9%\n",
      "Step 4500 loss: 0.478704\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.509105\n",
      "Step 5000 accuracy: 86.3%\n",
      "Accuracy in test dataset: 92.66\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.634304\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.705503\n",
      "Step 500 accuracy: 79.9%\n",
      "Step 1000 loss: 0.683038\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.590052\n",
      "Step 1500 accuracy: 83.4%\n",
      "Step 2000 loss: 0.704989\n",
      "Step 2000 accuracy: 80.7%\n",
      "Step 2500 loss: 0.581142\n",
      "Step 2500 accuracy: 84.2%\n",
      "Step 3000 loss: 0.597645\n",
      "Step 3000 accuracy: 82.4%\n",
      "Step 3500 loss: 0.632517\n",
      "Step 3500 accuracy: 83.4%\n",
      "Step 4000 loss: 0.553715\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.551253\n",
      "Step 4500 accuracy: 84.6%\n",
      "Step 5000 loss: 0.597366\n",
      "Step 5000 accuracy: 84.2%\n",
      "Accuracy in test dataset: 91.92\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.330702\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.563371\n",
      "Step 500 accuracy: 83.6%\n",
      "Step 1000 loss: 0.582250\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.481690\n",
      "Step 1500 accuracy: 87.5%\n",
      "Step 2000 loss: 0.556043\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.459811\n",
      "Step 2500 accuracy: 86.7%\n",
      "Step 3000 loss: 0.456027\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.514611\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.413430\n",
      "Step 4000 accuracy: 88.7%\n",
      "Step 4500 loss: 0.401839\n",
      "Step 4500 accuracy: 88.5%\n",
      "Step 5000 loss: 0.425054\n",
      "Step 5000 accuracy: 88.1%\n",
      "Step 5500 loss: 0.446692\n",
      "Step 5500 accuracy: 86.7%\n",
      "Step 6000 loss: 0.377339\n",
      "Step 6000 accuracy: 89.3%\n",
      "Step 6500 loss: 0.441101\n",
      "Step 6500 accuracy: 87.1%\n",
      "Step 7000 loss: 0.431311\n",
      "Step 7000 accuracy: 87.5%\n",
      "Step 7500 loss: 0.393462\n",
      "Step 7500 accuracy: 89.3%\n",
      "Step 8000 loss: 0.350207\n",
      "Step 8000 accuracy: 89.6%\n",
      "Step 8500 loss: 0.315280\n",
      "Step 8500 accuracy: 90.4%\n",
      "Step 9000 loss: 0.316126\n",
      "Step 9000 accuracy: 90.2%\n",
      "Step 9500 loss: 0.404274\n",
      "Step 9500 accuracy: 88.5%\n",
      "Step 10000 loss: 0.339293\n",
      "Step 10000 accuracy: 91.6%\n",
      "Accuracy in test dataset: 94.35\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.357427\n",
      "Step 0 accuracy: 8.0%\n",
      "Step 500 loss: 0.593911\n",
      "Step 500 accuracy: 83.0%\n",
      "Step 1000 loss: 0.603724\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.513083\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.571746\n",
      "Step 2000 accuracy: 83.8%\n",
      "Step 2500 loss: 0.500375\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.489028\n",
      "Step 3000 accuracy: 85.5%\n",
      "Step 3500 loss: 0.530868\n",
      "Step 3500 accuracy: 85.4%\n",
      "Step 4000 loss: 0.451096\n",
      "Step 4000 accuracy: 86.9%\n",
      "Step 4500 loss: 0.430955\n",
      "Step 4500 accuracy: 87.5%\n",
      "Step 5000 loss: 0.457880\n",
      "Step 5000 accuracy: 87.3%\n",
      "Step 5500 loss: 0.474433\n",
      "Step 5500 accuracy: 86.9%\n",
      "Step 6000 loss: 0.400157\n",
      "Step 6000 accuracy: 88.5%\n",
      "Step 6500 loss: 0.489521\n",
      "Step 6500 accuracy: 85.5%\n",
      "Step 7000 loss: 0.465515\n",
      "Step 7000 accuracy: 86.1%\n",
      "Step 7500 loss: 0.425657\n",
      "Step 7500 accuracy: 88.1%\n",
      "Step 8000 loss: 0.394949\n",
      "Step 8000 accuracy: 88.3%\n",
      "Step 8500 loss: 0.341365\n",
      "Step 8500 accuracy: 89.1%\n",
      "Step 9000 loss: 0.370824\n",
      "Step 9000 accuracy: 87.7%\n",
      "Step 9500 loss: 0.430246\n",
      "Step 9500 accuracy: 86.9%\n",
      "Step 10000 loss: 0.362384\n",
      "Step 10000 accuracy: 90.4%\n",
      "Accuracy in test dataset: 94.11\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.399315\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.680337\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.651178\n",
      "Step 1000 accuracy: 81.1%\n",
      "Step 1500 loss: 0.563689\n",
      "Step 1500 accuracy: 83.4%\n",
      "Step 2000 loss: 0.612745\n",
      "Step 2000 accuracy: 82.2%\n",
      "Step 2500 loss: 0.536227\n",
      "Step 2500 accuracy: 85.0%\n",
      "Step 3000 loss: 0.536685\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.599997\n",
      "Step 3500 accuracy: 84.4%\n",
      "Step 4000 loss: 0.498001\n",
      "Step 4000 accuracy: 86.5%\n",
      "Step 4500 loss: 0.531641\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.523875\n",
      "Step 5000 accuracy: 86.5%\n",
      "Step 5500 loss: 0.563160\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.495596\n",
      "Step 6000 accuracy: 86.3%\n",
      "Step 6500 loss: 0.575656\n",
      "Step 6500 accuracy: 83.8%\n",
      "Step 7000 loss: 0.534089\n",
      "Step 7000 accuracy: 85.0%\n",
      "Step 7500 loss: 0.483481\n",
      "Step 7500 accuracy: 85.5%\n",
      "Step 8000 loss: 0.471159\n",
      "Step 8000 accuracy: 87.1%\n",
      "Step 8500 loss: 0.439453\n",
      "Step 8500 accuracy: 86.9%\n",
      "Step 9000 loss: 0.452507\n",
      "Step 9000 accuracy: 86.3%\n",
      "Step 9500 loss: 0.496787\n",
      "Step 9500 accuracy: 84.6%\n",
      "Step 10000 loss: 0.462860\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.26\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.442371\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.766454\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.710113\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.672781\n",
      "Step 1500 accuracy: 80.9%\n",
      "Step 2000 loss: 0.707100\n",
      "Step 2000 accuracy: 80.1%\n",
      "Step 2500 loss: 0.627384\n",
      "Step 2500 accuracy: 82.2%\n",
      "Step 3000 loss: 0.659838\n",
      "Step 3000 accuracy: 82.2%\n",
      "Step 3500 loss: 0.695953\n",
      "Step 3500 accuracy: 81.1%\n",
      "Step 4000 loss: 0.611914\n",
      "Step 4000 accuracy: 83.4%\n",
      "Step 4500 loss: 0.639721\n",
      "Step 4500 accuracy: 83.2%\n",
      "Step 5000 loss: 0.651671\n",
      "Step 5000 accuracy: 81.6%\n",
      "Step 5500 loss: 0.690991\n",
      "Step 5500 accuracy: 80.3%\n",
      "Step 6000 loss: 0.583605\n",
      "Step 6000 accuracy: 83.2%\n",
      "Step 6500 loss: 0.699525\n",
      "Step 6500 accuracy: 81.2%\n",
      "Step 7000 loss: 0.597533\n",
      "Step 7000 accuracy: 84.2%\n",
      "Step 7500 loss: 0.556039\n",
      "Step 7500 accuracy: 85.4%\n",
      "Step 8000 loss: 0.544429\n",
      "Step 8000 accuracy: 85.4%\n",
      "Step 8500 loss: 0.513938\n",
      "Step 8500 accuracy: 84.4%\n",
      "Step 9000 loss: 0.550052\n",
      "Step 9000 accuracy: 82.8%\n",
      "Step 9500 loss: 0.596727\n",
      "Step 9500 accuracy: 83.4%\n",
      "Step 10000 loss: 0.563024\n",
      "Step 10000 accuracy: 84.2%\n",
      "Accuracy in test dataset: 92.24\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.597986\n",
      "Step 0 accuracy: 11.1%\n",
      "Step 500 loss: 0.892232\n",
      "Step 500 accuracy: 76.0%\n",
      "Step 1000 loss: 0.862351\n",
      "Step 1000 accuracy: 75.4%\n",
      "Step 1500 loss: 0.780572\n",
      "Step 1500 accuracy: 77.9%\n",
      "Step 2000 loss: 0.835260\n",
      "Step 2000 accuracy: 76.0%\n",
      "Step 2500 loss: 0.789324\n",
      "Step 2500 accuracy: 77.5%\n",
      "Step 3000 loss: 0.746497\n",
      "Step 3000 accuracy: 77.7%\n",
      "Step 3500 loss: 0.831492\n",
      "Step 3500 accuracy: 76.4%\n",
      "Step 4000 loss: 0.710616\n",
      "Step 4000 accuracy: 80.1%\n",
      "Step 4500 loss: 0.770933\n",
      "Step 4500 accuracy: 79.1%\n",
      "Step 5000 loss: 0.759514\n",
      "Step 5000 accuracy: 77.7%\n",
      "Step 5500 loss: 0.798422\n",
      "Step 5500 accuracy: 76.4%\n",
      "Step 6000 loss: 0.666531\n",
      "Step 6000 accuracy: 81.4%\n",
      "Step 6500 loss: 0.787270\n",
      "Step 6500 accuracy: 78.7%\n",
      "Step 7000 loss: 0.744795\n",
      "Step 7000 accuracy: 78.9%\n",
      "Step 7500 loss: 0.641305\n",
      "Step 7500 accuracy: 83.0%\n",
      "Step 8000 loss: 0.692912\n",
      "Step 8000 accuracy: 81.1%\n",
      "Step 8500 loss: 0.644515\n",
      "Step 8500 accuracy: 79.7%\n",
      "Step 9000 loss: 0.661240\n",
      "Step 9000 accuracy: 80.1%\n",
      "Step 9500 loss: 0.685988\n",
      "Step 9500 accuracy: 79.5%\n",
      "Step 10000 loss: 0.642250\n",
      "Step 10000 accuracy: 80.1%\n",
      "Accuracy in test dataset: 90.94\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.319925\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.576216\n",
      "Step 500 accuracy: 83.8%\n",
      "Step 1000 loss: 0.574006\n",
      "Step 1000 accuracy: 83.4%\n",
      "Step 1500 loss: 0.484765\n",
      "Step 1500 accuracy: 87.1%\n",
      "Step 2000 loss: 0.546877\n",
      "Step 2000 accuracy: 84.6%\n",
      "Step 2500 loss: 0.468291\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.445652\n",
      "Step 3000 accuracy: 86.5%\n",
      "Step 3500 loss: 0.499577\n",
      "Step 3500 accuracy: 86.9%\n",
      "Step 4000 loss: 0.402868\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.394160\n",
      "Step 4500 accuracy: 88.5%\n",
      "Step 5000 loss: 0.428068\n",
      "Step 5000 accuracy: 88.7%\n",
      "Step 5500 loss: 0.439372\n",
      "Step 5500 accuracy: 86.1%\n",
      "Step 6000 loss: 0.362568\n",
      "Step 6000 accuracy: 90.6%\n",
      "Step 6500 loss: 0.428062\n",
      "Step 6500 accuracy: 87.3%\n",
      "Step 7000 loss: 0.418675\n",
      "Step 7000 accuracy: 87.7%\n",
      "Step 7500 loss: 0.378272\n",
      "Step 7500 accuracy: 90.2%\n",
      "Step 8000 loss: 0.341105\n",
      "Step 8000 accuracy: 90.0%\n",
      "Step 8500 loss: 0.303178\n",
      "Step 8500 accuracy: 91.2%\n",
      "Step 9000 loss: 0.308261\n",
      "Step 9000 accuracy: 91.0%\n",
      "Step 9500 loss: 0.388904\n",
      "Step 9500 accuracy: 89.6%\n",
      "Step 10000 loss: 0.330194\n",
      "Step 10000 accuracy: 91.2%\n",
      "Accuracy in test dataset: 94.59\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.332803\n",
      "Step 0 accuracy: 13.5%\n",
      "Step 500 loss: 0.581064\n",
      "Step 500 accuracy: 84.0%\n",
      "Step 1000 loss: 0.586890\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.501622\n",
      "Step 1500 accuracy: 85.5%\n",
      "Step 2000 loss: 0.545629\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.478558\n",
      "Step 2500 accuracy: 86.5%\n",
      "Step 3000 loss: 0.463108\n",
      "Step 3000 accuracy: 85.5%\n",
      "Step 3500 loss: 0.521228\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.418233\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.396851\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.455110\n",
      "Step 5000 accuracy: 87.7%\n",
      "Step 5500 loss: 0.480491\n",
      "Step 5500 accuracy: 85.2%\n",
      "Step 6000 loss: 0.388249\n",
      "Step 6000 accuracy: 89.5%\n",
      "Step 6500 loss: 0.466053\n",
      "Step 6500 accuracy: 85.4%\n",
      "Step 7000 loss: 0.454259\n",
      "Step 7000 accuracy: 86.3%\n",
      "Step 7500 loss: 0.406956\n",
      "Step 7500 accuracy: 88.9%\n",
      "Step 8000 loss: 0.371535\n",
      "Step 8000 accuracy: 90.6%\n",
      "Step 8500 loss: 0.338766\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.337273\n",
      "Step 9000 accuracy: 88.9%\n",
      "Step 9500 loss: 0.418984\n",
      "Step 9500 accuracy: 88.3%\n",
      "Step 10000 loss: 0.344904\n",
      "Step 10000 accuracy: 90.4%\n",
      "Accuracy in test dataset: 94.49\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.334092\n",
      "Step 0 accuracy: 11.3%\n",
      "Step 500 loss: 0.655306\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.638045\n",
      "Step 1000 accuracy: 80.1%\n",
      "Step 1500 loss: 0.553303\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.596061\n",
      "Step 2000 accuracy: 84.2%\n",
      "Step 2500 loss: 0.542713\n",
      "Step 2500 accuracy: 83.8%\n",
      "Step 3000 loss: 0.517033\n",
      "Step 3000 accuracy: 85.0%\n",
      "Step 3500 loss: 0.574781\n",
      "Step 3500 accuracy: 84.2%\n",
      "Step 4000 loss: 0.477925\n",
      "Step 4000 accuracy: 86.3%\n",
      "Step 4500 loss: 0.494186\n",
      "Step 4500 accuracy: 85.4%\n",
      "Step 5000 loss: 0.531513\n",
      "Step 5000 accuracy: 85.5%\n",
      "Step 5500 loss: 0.531452\n",
      "Step 5500 accuracy: 83.4%\n",
      "Step 6000 loss: 0.441733\n",
      "Step 6000 accuracy: 87.9%\n",
      "Step 6500 loss: 0.544888\n",
      "Step 6500 accuracy: 84.6%\n",
      "Step 7000 loss: 0.500315\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.457992\n",
      "Step 7500 accuracy: 87.7%\n",
      "Step 8000 loss: 0.424139\n",
      "Step 8000 accuracy: 88.7%\n",
      "Step 8500 loss: 0.381187\n",
      "Step 8500 accuracy: 88.3%\n",
      "Step 9000 loss: 0.411467\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.508698\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.402863\n",
      "Step 10000 accuracy: 89.5%\n",
      "Accuracy in test dataset: 93.64\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.496590\n",
      "Step 0 accuracy: 8.8%\n",
      "Step 500 loss: 0.687408\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.669702\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.620753\n",
      "Step 1500 accuracy: 83.8%\n",
      "Step 2000 loss: 0.682799\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.582775\n",
      "Step 2500 accuracy: 82.2%\n",
      "Step 3000 loss: 0.562783\n",
      "Step 3000 accuracy: 83.8%\n",
      "Step 3500 loss: 0.650574\n",
      "Step 3500 accuracy: 82.4%\n",
      "Step 4000 loss: 0.562586\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.514284\n",
      "Step 4500 accuracy: 84.6%\n",
      "Step 5000 loss: 0.597148\n",
      "Step 5000 accuracy: 83.4%\n",
      "Step 5500 loss: 0.612829\n",
      "Step 5500 accuracy: 81.2%\n",
      "Step 6000 loss: 0.509671\n",
      "Step 6000 accuracy: 85.4%\n",
      "Step 6500 loss: 0.575041\n",
      "Step 6500 accuracy: 85.2%\n",
      "Step 7000 loss: 0.584505\n",
      "Step 7000 accuracy: 83.6%\n",
      "Step 7500 loss: 0.521642\n",
      "Step 7500 accuracy: 85.5%\n",
      "Step 8000 loss: 0.519189\n",
      "Step 8000 accuracy: 86.1%\n",
      "Step 8500 loss: 0.442931\n",
      "Step 8500 accuracy: 86.9%\n",
      "Step 9000 loss: 0.463099\n",
      "Step 9000 accuracy: 85.5%\n",
      "Step 9500 loss: 0.538898\n",
      "Step 9500 accuracy: 84.0%\n",
      "Step 10000 loss: 0.476485\n",
      "Step 10000 accuracy: 86.3%\n",
      "Accuracy in test dataset: 92.87\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.548924\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.797948\n",
      "Step 500 accuracy: 76.6%\n",
      "Step 1000 loss: 0.787455\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.708160\n",
      "Step 1500 accuracy: 79.1%\n",
      "Step 2000 loss: 0.712981\n",
      "Step 2000 accuracy: 80.3%\n",
      "Step 2500 loss: 0.663952\n",
      "Step 2500 accuracy: 81.4%\n",
      "Step 3000 loss: 0.666541\n",
      "Step 3000 accuracy: 80.5%\n",
      "Step 3500 loss: 0.745874\n",
      "Step 3500 accuracy: 80.5%\n",
      "Step 4000 loss: 0.640961\n",
      "Step 4000 accuracy: 82.6%\n",
      "Step 4500 loss: 0.701442\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.628196\n",
      "Step 5000 accuracy: 84.4%\n",
      "Step 5500 loss: 0.692275\n",
      "Step 5500 accuracy: 79.1%\n",
      "Step 6000 loss: 0.616346\n",
      "Step 6000 accuracy: 81.8%\n",
      "Step 6500 loss: 0.767561\n",
      "Step 6500 accuracy: 78.3%\n",
      "Step 7000 loss: 0.626329\n",
      "Step 7000 accuracy: 81.8%\n",
      "Step 7500 loss: 0.581252\n",
      "Step 7500 accuracy: 83.2%\n",
      "Step 8000 loss: 0.618201\n",
      "Step 8000 accuracy: 82.4%\n",
      "Step 8500 loss: 0.555770\n",
      "Step 8500 accuracy: 83.8%\n",
      "Step 9000 loss: 0.589820\n",
      "Step 9000 accuracy: 82.0%\n",
      "Step 9500 loss: 0.638223\n",
      "Step 9500 accuracy: 82.0%\n",
      "Step 10000 loss: 0.597637\n",
      "Step 10000 accuracy: 83.2%\n",
      "Accuracy in test dataset: 91.78\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.325428\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.568636\n",
      "Step 500 accuracy: 84.0%\n",
      "Step 1000 loss: 0.572299\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.484630\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.542184\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.460590\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.442787\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.501946\n",
      "Step 3500 accuracy: 86.9%\n",
      "Step 4000 loss: 0.396309\n",
      "Step 4000 accuracy: 89.6%\n",
      "Step 4500 loss: 0.382129\n",
      "Step 4500 accuracy: 89.5%\n",
      "Step 5000 loss: 0.412760\n",
      "Step 5000 accuracy: 89.1%\n",
      "Step 5500 loss: 0.429810\n",
      "Step 5500 accuracy: 86.7%\n",
      "Step 6000 loss: 0.353622\n",
      "Step 6000 accuracy: 90.6%\n",
      "Step 6500 loss: 0.413200\n",
      "Step 6500 accuracy: 87.7%\n",
      "Step 7000 loss: 0.414700\n",
      "Step 7000 accuracy: 87.7%\n",
      "Step 7500 loss: 0.377843\n",
      "Step 7500 accuracy: 89.6%\n",
      "Step 8000 loss: 0.328419\n",
      "Step 8000 accuracy: 91.2%\n",
      "Step 8500 loss: 0.297015\n",
      "Step 8500 accuracy: 92.2%\n",
      "Step 9000 loss: 0.299724\n",
      "Step 9000 accuracy: 91.6%\n",
      "Step 9500 loss: 0.363060\n",
      "Step 9500 accuracy: 90.0%\n",
      "Step 10000 loss: 0.306889\n",
      "Step 10000 accuracy: 91.8%\n",
      "Accuracy in test dataset: 94.74\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.324603\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.577293\n",
      "Step 500 accuracy: 83.6%\n",
      "Step 1000 loss: 0.576986\n",
      "Step 1000 accuracy: 83.4%\n",
      "Step 1500 loss: 0.495181\n",
      "Step 1500 accuracy: 86.3%\n",
      "Step 2000 loss: 0.546312\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.474671\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.458288\n",
      "Step 3000 accuracy: 86.9%\n",
      "Step 3500 loss: 0.528495\n",
      "Step 3500 accuracy: 86.5%\n",
      "Step 4000 loss: 0.416776\n",
      "Step 4000 accuracy: 89.5%\n",
      "Step 4500 loss: 0.399957\n",
      "Step 4500 accuracy: 88.9%\n",
      "Step 5000 loss: 0.430296\n",
      "Step 5000 accuracy: 88.9%\n",
      "Step 5500 loss: 0.449769\n",
      "Step 5500 accuracy: 86.9%\n",
      "Step 6000 loss: 0.364387\n",
      "Step 6000 accuracy: 90.2%\n",
      "Step 6500 loss: 0.436138\n",
      "Step 6500 accuracy: 87.3%\n",
      "Step 7000 loss: 0.447136\n",
      "Step 7000 accuracy: 87.7%\n",
      "Step 7500 loss: 0.399761\n",
      "Step 7500 accuracy: 89.1%\n",
      "Step 8000 loss: 0.352190\n",
      "Step 8000 accuracy: 90.6%\n",
      "Step 8500 loss: 0.322608\n",
      "Step 8500 accuracy: 91.0%\n",
      "Step 9000 loss: 0.332083\n",
      "Step 9000 accuracy: 90.6%\n",
      "Step 9500 loss: 0.383861\n",
      "Step 9500 accuracy: 89.5%\n",
      "Step 10000 loss: 0.338610\n",
      "Step 10000 accuracy: 91.0%\n",
      "Accuracy in test dataset: 94.54\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.378488\n",
      "Step 0 accuracy: 11.5%\n",
      "Step 500 loss: 0.598457\n",
      "Step 500 accuracy: 82.6%\n",
      "Step 1000 loss: 0.610068\n",
      "Step 1000 accuracy: 82.6%\n",
      "Step 1500 loss: 0.524207\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.593214\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.516862\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.495278\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.589408\n",
      "Step 3500 accuracy: 84.0%\n",
      "Step 4000 loss: 0.464865\n",
      "Step 4000 accuracy: 87.9%\n",
      "Step 4500 loss: 0.463894\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.478608\n",
      "Step 5000 accuracy: 86.5%\n",
      "Step 5500 loss: 0.510809\n",
      "Step 5500 accuracy: 84.6%\n",
      "Step 6000 loss: 0.426896\n",
      "Step 6000 accuracy: 87.9%\n",
      "Step 6500 loss: 0.496672\n",
      "Step 6500 accuracy: 85.0%\n",
      "Step 7000 loss: 0.473395\n",
      "Step 7000 accuracy: 85.7%\n",
      "Step 7500 loss: 0.432846\n",
      "Step 7500 accuracy: 89.1%\n",
      "Step 8000 loss: 0.397142\n",
      "Step 8000 accuracy: 89.1%\n",
      "Step 8500 loss: 0.362041\n",
      "Step 8500 accuracy: 91.0%\n",
      "Step 9000 loss: 0.396910\n",
      "Step 9000 accuracy: 87.3%\n",
      "Step 9500 loss: 0.446538\n",
      "Step 9500 accuracy: 86.5%\n",
      "Step 10000 loss: 0.397512\n",
      "Step 10000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 94.13\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.506413\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.669102\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.675206\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.576775\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.651502\n",
      "Step 2000 accuracy: 81.8%\n",
      "Step 2500 loss: 0.556337\n",
      "Step 2500 accuracy: 82.4%\n",
      "Step 3000 loss: 0.544801\n",
      "Step 3000 accuracy: 83.8%\n",
      "Step 3500 loss: 0.624965\n",
      "Step 3500 accuracy: 83.2%\n",
      "Step 4000 loss: 0.508634\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.528229\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.536970\n",
      "Step 5000 accuracy: 86.1%\n",
      "Step 5500 loss: 0.546332\n",
      "Step 5500 accuracy: 84.0%\n",
      "Step 6000 loss: 0.462233\n",
      "Step 6000 accuracy: 87.3%\n",
      "Step 6500 loss: 0.570472\n",
      "Step 6500 accuracy: 83.0%\n",
      "Step 7000 loss: 0.562240\n",
      "Step 7000 accuracy: 84.2%\n",
      "Step 7500 loss: 0.472965\n",
      "Step 7500 accuracy: 86.5%\n",
      "Step 8000 loss: 0.452651\n",
      "Step 8000 accuracy: 88.3%\n",
      "Step 8500 loss: 0.429549\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.475439\n",
      "Step 9000 accuracy: 85.9%\n",
      "Step 9500 loss: 0.485791\n",
      "Step 9500 accuracy: 86.3%\n",
      "Step 10000 loss: 0.412878\n",
      "Step 10000 accuracy: 87.3%\n",
      "Accuracy in test dataset: 93.46\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.531161\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.750968\n",
      "Step 500 accuracy: 78.3%\n",
      "Step 1000 loss: 0.717429\n",
      "Step 1000 accuracy: 78.5%\n",
      "Step 1500 loss: 0.663206\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.709807\n",
      "Step 2000 accuracy: 80.1%\n",
      "Step 2500 loss: 0.613446\n",
      "Step 2500 accuracy: 83.4%\n",
      "Step 3000 loss: 0.624593\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.688352\n",
      "Step 3500 accuracy: 81.1%\n",
      "Step 4000 loss: 0.579558\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.613731\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.619935\n",
      "Step 5000 accuracy: 83.6%\n",
      "Step 5500 loss: 0.644235\n",
      "Step 5500 accuracy: 82.0%\n",
      "Step 6000 loss: 0.535492\n",
      "Step 6000 accuracy: 85.4%\n",
      "Step 6500 loss: 0.660592\n",
      "Step 6500 accuracy: 81.2%\n",
      "Step 7000 loss: 0.578454\n",
      "Step 7000 accuracy: 83.2%\n",
      "Step 7500 loss: 0.541357\n",
      "Step 7500 accuracy: 85.9%\n",
      "Step 8000 loss: 0.547623\n",
      "Step 8000 accuracy: 85.0%\n",
      "Step 8500 loss: 0.468723\n",
      "Step 8500 accuracy: 86.3%\n",
      "Step 9000 loss: 0.502262\n",
      "Step 9000 accuracy: 84.4%\n",
      "Step 9500 loss: 0.559570\n",
      "Step 9500 accuracy: 83.0%\n",
      "Step 10000 loss: 0.517439\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 92.55\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.301143\n",
      "Step 0 accuracy: 11.1%\n",
      "Step 500 loss: 0.557953\n",
      "Step 500 accuracy: 85.4%\n",
      "Step 1000 loss: 0.562793\n",
      "Step 1000 accuracy: 83.8%\n",
      "Step 1500 loss: 0.470594\n",
      "Step 1500 accuracy: 87.5%\n",
      "Step 2000 loss: 0.534474\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.453282\n",
      "Step 2500 accuracy: 86.5%\n",
      "Step 3000 loss: 0.433324\n",
      "Step 3000 accuracy: 86.9%\n",
      "Step 3500 loss: 0.489647\n",
      "Step 3500 accuracy: 87.3%\n",
      "Step 4000 loss: 0.380570\n",
      "Step 4000 accuracy: 90.0%\n",
      "Step 4500 loss: 0.366643\n",
      "Step 4500 accuracy: 89.6%\n",
      "Step 5000 loss: 0.406251\n",
      "Step 5000 accuracy: 89.1%\n",
      "Step 5500 loss: 0.417755\n",
      "Step 5500 accuracy: 87.3%\n",
      "Step 6000 loss: 0.340124\n",
      "Step 6000 accuracy: 90.8%\n",
      "Step 6500 loss: 0.412999\n",
      "Step 6500 accuracy: 87.1%\n",
      "Step 7000 loss: 0.412514\n",
      "Step 7000 accuracy: 88.1%\n",
      "Step 7500 loss: 0.357791\n",
      "Step 7500 accuracy: 90.2%\n",
      "Step 8000 loss: 0.307833\n",
      "Step 8000 accuracy: 91.6%\n",
      "Step 8500 loss: 0.284677\n",
      "Step 8500 accuracy: 92.6%\n",
      "Step 9000 loss: 0.282168\n",
      "Step 9000 accuracy: 91.8%\n",
      "Step 9500 loss: 0.358609\n",
      "Step 9500 accuracy: 89.5%\n",
      "Step 10000 loss: 0.301146\n",
      "Step 10000 accuracy: 91.6%\n",
      "Accuracy in test dataset: 94.88\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.376531\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.562215\n",
      "Step 500 accuracy: 84.6%\n",
      "Step 1000 loss: 0.572398\n",
      "Step 1000 accuracy: 82.6%\n",
      "Step 1500 loss: 0.486242\n",
      "Step 1500 accuracy: 86.9%\n",
      "Step 2000 loss: 0.551827\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.450214\n",
      "Step 2500 accuracy: 85.7%\n",
      "Step 3000 loss: 0.441283\n",
      "Step 3000 accuracy: 86.9%\n",
      "Step 3500 loss: 0.496558\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.395214\n",
      "Step 4000 accuracy: 90.2%\n",
      "Step 4500 loss: 0.382149\n",
      "Step 4500 accuracy: 89.1%\n",
      "Step 5000 loss: 0.412424\n",
      "Step 5000 accuracy: 89.1%\n",
      "Step 5500 loss: 0.433012\n",
      "Step 5500 accuracy: 87.1%\n",
      "Step 6000 loss: 0.357372\n",
      "Step 6000 accuracy: 90.0%\n",
      "Step 6500 loss: 0.424112\n",
      "Step 6500 accuracy: 87.3%\n",
      "Step 7000 loss: 0.442040\n",
      "Step 7000 accuracy: 87.7%\n",
      "Step 7500 loss: 0.386290\n",
      "Step 7500 accuracy: 89.1%\n",
      "Step 8000 loss: 0.338806\n",
      "Step 8000 accuracy: 90.0%\n",
      "Step 8500 loss: 0.305159\n",
      "Step 8500 accuracy: 90.6%\n",
      "Step 9000 loss: 0.309928\n",
      "Step 9000 accuracy: 90.6%\n",
      "Step 9500 loss: 0.375960\n",
      "Step 9500 accuracy: 88.9%\n",
      "Step 10000 loss: 0.341329\n",
      "Step 10000 accuracy: 90.8%\n",
      "Accuracy in test dataset: 94.86\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.338700\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.598989\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.590890\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.497107\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.580024\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.511192\n",
      "Step 2500 accuracy: 84.6%\n",
      "Step 3000 loss: 0.479137\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.541208\n",
      "Step 3500 accuracy: 85.4%\n",
      "Step 4000 loss: 0.425436\n",
      "Step 4000 accuracy: 88.1%\n",
      "Step 4500 loss: 0.430900\n",
      "Step 4500 accuracy: 87.3%\n",
      "Step 5000 loss: 0.473333\n",
      "Step 5000 accuracy: 87.7%\n",
      "Step 5500 loss: 0.482628\n",
      "Step 5500 accuracy: 85.7%\n",
      "Step 6000 loss: 0.390011\n",
      "Step 6000 accuracy: 89.3%\n",
      "Step 6500 loss: 0.486368\n",
      "Step 6500 accuracy: 85.2%\n",
      "Step 7000 loss: 0.484387\n",
      "Step 7000 accuracy: 86.5%\n",
      "Step 7500 loss: 0.401044\n",
      "Step 7500 accuracy: 88.9%\n",
      "Step 8000 loss: 0.381152\n",
      "Step 8000 accuracy: 89.1%\n",
      "Step 8500 loss: 0.343532\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.370451\n",
      "Step 9000 accuracy: 89.1%\n",
      "Step 9500 loss: 0.418675\n",
      "Step 9500 accuracy: 88.5%\n",
      "Step 10000 loss: 0.353345\n",
      "Step 10000 accuracy: 90.0%\n",
      "Accuracy in test dataset: 94.31\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.419148\n",
      "Step 0 accuracy: 11.3%\n",
      "Step 500 loss: 0.642397\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.634955\n",
      "Step 1000 accuracy: 81.4%\n",
      "Step 1500 loss: 0.548912\n",
      "Step 1500 accuracy: 85.7%\n",
      "Step 2000 loss: 0.635065\n",
      "Step 2000 accuracy: 81.8%\n",
      "Step 2500 loss: 0.522080\n",
      "Step 2500 accuracy: 85.9%\n",
      "Step 3000 loss: 0.543340\n",
      "Step 3000 accuracy: 84.0%\n",
      "Step 3500 loss: 0.611554\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.490602\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.475609\n",
      "Step 4500 accuracy: 87.1%\n",
      "Step 5000 loss: 0.500101\n",
      "Step 5000 accuracy: 86.1%\n",
      "Step 5500 loss: 0.562508\n",
      "Step 5500 accuracy: 82.8%\n",
      "Step 6000 loss: 0.420026\n",
      "Step 6000 accuracy: 88.7%\n",
      "Step 6500 loss: 0.526874\n",
      "Step 6500 accuracy: 83.2%\n",
      "Step 7000 loss: 0.523147\n",
      "Step 7000 accuracy: 84.6%\n",
      "Step 7500 loss: 0.455423\n",
      "Step 7500 accuracy: 87.3%\n",
      "Step 8000 loss: 0.431921\n",
      "Step 8000 accuracy: 89.5%\n",
      "Step 8500 loss: 0.381390\n",
      "Step 8500 accuracy: 88.7%\n",
      "Step 9000 loss: 0.424663\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.478939\n",
      "Step 9500 accuracy: 85.5%\n",
      "Step 10000 loss: 0.416215\n",
      "Step 10000 accuracy: 87.3%\n",
      "Accuracy in test dataset: 93.77\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 512 | over_fit : False |\n",
      "Step 0 loss: 2.570240\n",
      "Step 0 accuracy: 12.3%\n",
      "Step 500 loss: 0.680497\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.697282\n",
      "Step 1000 accuracy: 81.1%\n",
      "Step 1500 loss: 0.634357\n",
      "Step 1500 accuracy: 82.2%\n",
      "Step 2000 loss: 0.691606\n",
      "Step 2000 accuracy: 80.9%\n",
      "Step 2500 loss: 0.596943\n",
      "Step 2500 accuracy: 84.2%\n",
      "Step 3000 loss: 0.581466\n",
      "Step 3000 accuracy: 82.6%\n",
      "Step 3500 loss: 0.678915\n",
      "Step 3500 accuracy: 83.4%\n",
      "Step 4000 loss: 0.514085\n",
      "Step 4000 accuracy: 86.1%\n",
      "Step 4500 loss: 0.571072\n",
      "Step 4500 accuracy: 83.0%\n",
      "Step 5000 loss: 0.588869\n",
      "Step 5000 accuracy: 83.8%\n",
      "Step 5500 loss: 0.574907\n",
      "Step 5500 accuracy: 84.0%\n",
      "Step 6000 loss: 0.495447\n",
      "Step 6000 accuracy: 87.1%\n",
      "Step 6500 loss: 0.598380\n",
      "Step 6500 accuracy: 82.8%\n",
      "Step 7000 loss: 0.560522\n",
      "Step 7000 accuracy: 83.4%\n",
      "Step 7500 loss: 0.516596\n",
      "Step 7500 accuracy: 85.4%\n",
      "Step 8000 loss: 0.506778\n",
      "Step 8000 accuracy: 84.6%\n",
      "Step 8500 loss: 0.416835\n",
      "Step 8500 accuracy: 88.7%\n",
      "Step 9000 loss: 0.515676\n",
      "Step 9000 accuracy: 85.5%\n",
      "Step 9500 loss: 0.510696\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.484438\n",
      "Step 10000 accuracy: 87.7%\n",
      "Accuracy in test dataset: 92.97\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.367217\n",
      "Step 0 accuracy: 8.7%\n",
      "Step 500 loss: 0.595070\n",
      "Step 500 accuracy: 81.1%\n",
      "Step 1000 loss: 0.494257\n",
      "Step 1000 accuracy: 85.9%\n",
      "Step 1500 loss: 0.488486\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.485798\n",
      "Step 2000 accuracy: 86.1%\n",
      "Step 2500 loss: 0.538462\n",
      "Step 2500 accuracy: 84.3%\n",
      "Accuracy in test dataset: 91.73\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.337289\n",
      "Step 0 accuracy: 11.0%\n",
      "Step 500 loss: 0.611553\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.522168\n",
      "Step 1000 accuracy: 85.4%\n",
      "Step 1500 loss: 0.500697\n",
      "Step 1500 accuracy: 86.2%\n",
      "Step 2000 loss: 0.499699\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.546816\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 91.64\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.345076\n",
      "Step 0 accuracy: 11.1%\n",
      "Step 500 loss: 0.669812\n",
      "Step 500 accuracy: 79.4%\n",
      "Step 1000 loss: 0.580774\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.544225\n",
      "Step 1500 accuracy: 84.9%\n",
      "Step 2000 loss: 0.567845\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.619695\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 91.3\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.425979\n",
      "Step 0 accuracy: 11.4%\n",
      "Step 500 loss: 0.813317\n",
      "Step 500 accuracy: 77.1%\n",
      "Step 1000 loss: 0.667639\n",
      "Step 1000 accuracy: 80.8%\n",
      "Step 1500 loss: 0.650423\n",
      "Step 1500 accuracy: 81.8%\n",
      "Step 2000 loss: 0.634713\n",
      "Step 2000 accuracy: 83.1%\n",
      "Step 2500 loss: 0.701565\n",
      "Step 2500 accuracy: 80.6%\n",
      "Accuracy in test dataset: 90.63\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.614827\n",
      "Step 0 accuracy: 8.7%\n",
      "Step 500 loss: 0.963429\n",
      "Step 500 accuracy: 70.8%\n",
      "Step 1000 loss: 0.817310\n",
      "Step 1000 accuracy: 76.5%\n",
      "Step 1500 loss: 0.810407\n",
      "Step 1500 accuracy: 76.5%\n",
      "Step 2000 loss: 0.791997\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.833490\n",
      "Step 2500 accuracy: 76.4%\n",
      "Accuracy in test dataset: 90.21\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.320846\n",
      "Step 0 accuracy: 14.9%\n",
      "Step 500 loss: 0.595719\n",
      "Step 500 accuracy: 81.9%\n",
      "Step 1000 loss: 0.501163\n",
      "Step 1000 accuracy: 85.9%\n",
      "Step 1500 loss: 0.486326\n",
      "Step 1500 accuracy: 86.9%\n",
      "Step 2000 loss: 0.479819\n",
      "Step 2000 accuracy: 86.3%\n",
      "Step 2500 loss: 0.526185\n",
      "Step 2500 accuracy: 85.4%\n",
      "Accuracy in test dataset: 91.87\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.329121\n",
      "Step 0 accuracy: 11.4%\n",
      "Step 500 loss: 0.616424\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.505470\n",
      "Step 1000 accuracy: 86.0%\n",
      "Step 1500 loss: 0.502653\n",
      "Step 1500 accuracy: 86.1%\n",
      "Step 2000 loss: 0.497249\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.542908\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 91.83\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.392320\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.662935\n",
      "Step 500 accuracy: 81.1%\n",
      "Step 1000 loss: 0.547259\n",
      "Step 1000 accuracy: 83.8%\n",
      "Step 1500 loss: 0.539773\n",
      "Step 1500 accuracy: 85.5%\n",
      "Step 2000 loss: 0.532933\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.598563\n",
      "Step 2500 accuracy: 83.2%\n",
      "Accuracy in test dataset: 91.57\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.391749\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.700704\n",
      "Step 500 accuracy: 79.6%\n",
      "Step 1000 loss: 0.608718\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.619963\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.592284\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.660155\n",
      "Step 2500 accuracy: 80.6%\n",
      "Accuracy in test dataset: 91.17\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.609193\n",
      "Step 0 accuracy: 10.0%\n",
      "Step 500 loss: 0.813555\n",
      "Step 500 accuracy: 75.6%\n",
      "Step 1000 loss: 0.717842\n",
      "Step 1000 accuracy: 80.6%\n",
      "Step 1500 loss: 0.712095\n",
      "Step 1500 accuracy: 80.4%\n",
      "Step 2000 loss: 0.670337\n",
      "Step 2000 accuracy: 81.5%\n",
      "Step 2500 loss: 0.732557\n",
      "Step 2500 accuracy: 79.3%\n",
      "Accuracy in test dataset: 90.62\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.349179\n",
      "Step 0 accuracy: 12.3%\n",
      "Step 500 loss: 0.594415\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.497663\n",
      "Step 1000 accuracy: 85.8%\n",
      "Step 1500 loss: 0.488637\n",
      "Step 1500 accuracy: 87.4%\n",
      "Step 2000 loss: 0.477249\n",
      "Step 2000 accuracy: 86.5%\n",
      "Step 2500 loss: 0.518024\n",
      "Step 2500 accuracy: 85.0%\n",
      "Accuracy in test dataset: 92.08\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.350706\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.596013\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.498492\n",
      "Step 1000 accuracy: 86.1%\n",
      "Step 1500 loss: 0.493926\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.482386\n",
      "Step 2000 accuracy: 86.5%\n",
      "Step 2500 loss: 0.532555\n",
      "Step 2500 accuracy: 84.9%\n",
      "Accuracy in test dataset: 91.86\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.436414\n",
      "Step 0 accuracy: 6.4%\n",
      "Step 500 loss: 0.630852\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.526839\n",
      "Step 1000 accuracy: 85.6%\n",
      "Step 1500 loss: 0.521860\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.537498\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.577804\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 91.72\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.426130\n",
      "Step 0 accuracy: 9.5%\n",
      "Step 500 loss: 0.701377\n",
      "Step 500 accuracy: 79.5%\n",
      "Step 1000 loss: 0.587408\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.561774\n",
      "Step 1500 accuracy: 84.7%\n",
      "Step 2000 loss: 0.565541\n",
      "Step 2000 accuracy: 84.3%\n",
      "Step 2500 loss: 0.622203\n",
      "Step 2500 accuracy: 82.6%\n",
      "Accuracy in test dataset: 91.29\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.583306\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.780508\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.609334\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.614236\n",
      "Step 1500 accuracy: 83.5%\n",
      "Step 2000 loss: 0.636748\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.680459\n",
      "Step 2500 accuracy: 80.7%\n",
      "Accuracy in test dataset: 91.04\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.253093\n",
      "Step 0 accuracy: 15.4%\n",
      "Step 500 loss: 0.578182\n",
      "Step 500 accuracy: 82.6%\n",
      "Step 1000 loss: 0.484864\n",
      "Step 1000 accuracy: 86.2%\n",
      "Step 1500 loss: 0.473870\n",
      "Step 1500 accuracy: 86.8%\n",
      "Step 2000 loss: 0.473640\n",
      "Step 2000 accuracy: 86.1%\n",
      "Step 2500 loss: 0.512316\n",
      "Step 2500 accuracy: 85.7%\n",
      "Accuracy in test dataset: 92.13\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.318876\n",
      "Step 0 accuracy: 8.4%\n",
      "Step 500 loss: 0.587558\n",
      "Step 500 accuracy: 82.3%\n",
      "Step 1000 loss: 0.490571\n",
      "Step 1000 accuracy: 86.2%\n",
      "Step 1500 loss: 0.486618\n",
      "Step 1500 accuracy: 86.5%\n",
      "Step 2000 loss: 0.479290\n",
      "Step 2000 accuracy: 86.2%\n",
      "Step 2500 loss: 0.518506\n",
      "Step 2500 accuracy: 85.9%\n",
      "Accuracy in test dataset: 92.18\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.338703\n",
      "Step 0 accuracy: 10.6%\n",
      "Step 500 loss: 0.603937\n",
      "Step 500 accuracy: 82.3%\n",
      "Step 1000 loss: 0.520427\n",
      "Step 1000 accuracy: 85.4%\n",
      "Step 1500 loss: 0.513517\n",
      "Step 1500 accuracy: 85.8%\n",
      "Step 2000 loss: 0.511593\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.546759\n",
      "Step 2500 accuracy: 84.1%\n",
      "Accuracy in test dataset: 91.98\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.452427\n",
      "Step 0 accuracy: 8.8%\n",
      "Step 500 loss: 0.661040\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.571696\n",
      "Step 1000 accuracy: 84.3%\n",
      "Step 1500 loss: 0.544826\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.557469\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.592285\n",
      "Step 2500 accuracy: 82.3%\n",
      "Accuracy in test dataset: 91.71\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.554183\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.720582\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.619918\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.612778\n",
      "Step 1500 accuracy: 84.1%\n",
      "Step 2000 loss: 0.580574\n",
      "Step 2000 accuracy: 82.9%\n",
      "Step 2500 loss: 0.661833\n",
      "Step 2500 accuracy: 81.4%\n",
      "Accuracy in test dataset: 91.27\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.341038\n",
      "Step 0 accuracy: 8.8%\n",
      "Step 500 loss: 0.596087\n",
      "Step 500 accuracy: 81.5%\n",
      "Step 1000 loss: 0.507379\n",
      "Step 1000 accuracy: 85.8%\n",
      "Step 1500 loss: 0.498472\n",
      "Step 1500 accuracy: 86.6%\n",
      "Step 2000 loss: 0.479331\n",
      "Step 2000 accuracy: 86.1%\n",
      "Step 2500 loss: 0.530040\n",
      "Step 2500 accuracy: 84.7%\n",
      "Step 3000 loss: 0.456178\n",
      "Step 3000 accuracy: 87.0%\n",
      "Step 3500 loss: 0.474658\n",
      "Step 3500 accuracy: 86.6%\n",
      "Step 4000 loss: 0.467004\n",
      "Step 4000 accuracy: 86.3%\n",
      "Step 4500 loss: 0.396020\n",
      "Step 4500 accuracy: 87.7%\n",
      "Step 5000 loss: 0.409745\n",
      "Step 5000 accuracy: 88.8%\n",
      "Accuracy in test dataset: 93.17\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.297740\n",
      "Step 0 accuracy: 14.0%\n",
      "Step 500 loss: 0.611953\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.510418\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.509468\n",
      "Step 1500 accuracy: 86.3%\n",
      "Step 2000 loss: 0.508139\n",
      "Step 2000 accuracy: 85.3%\n",
      "Step 2500 loss: 0.547634\n",
      "Step 2500 accuracy: 84.2%\n",
      "Step 3000 loss: 0.464541\n",
      "Step 3000 accuracy: 86.6%\n",
      "Step 3500 loss: 0.486182\n",
      "Step 3500 accuracy: 85.4%\n",
      "Step 4000 loss: 0.484245\n",
      "Step 4000 accuracy: 86.3%\n",
      "Step 4500 loss: 0.420875\n",
      "Step 4500 accuracy: 87.6%\n",
      "Step 5000 loss: 0.445197\n",
      "Step 5000 accuracy: 87.1%\n",
      "Accuracy in test dataset: 92.69\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.414668\n",
      "Step 0 accuracy: 8.5%\n",
      "Step 500 loss: 0.670301\n",
      "Step 500 accuracy: 79.2%\n",
      "Step 1000 loss: 0.589140\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.575662\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.571113\n",
      "Step 2000 accuracy: 83.4%\n",
      "Step 2500 loss: 0.613853\n",
      "Step 2500 accuracy: 82.7%\n",
      "Step 3000 loss: 0.534534\n",
      "Step 3000 accuracy: 84.7%\n",
      "Step 3500 loss: 0.575962\n",
      "Step 3500 accuracy: 83.5%\n",
      "Step 4000 loss: 0.579916\n",
      "Step 4000 accuracy: 84.1%\n",
      "Step 4500 loss: 0.496480\n",
      "Step 4500 accuracy: 85.3%\n",
      "Step 5000 loss: 0.504956\n",
      "Step 5000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 92.1\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.457118\n",
      "Step 0 accuracy: 10.4%\n",
      "Step 500 loss: 0.751252\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.636108\n",
      "Step 1000 accuracy: 81.4%\n",
      "Step 1500 loss: 0.652165\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.633793\n",
      "Step 2000 accuracy: 82.1%\n",
      "Step 2500 loss: 0.701375\n",
      "Step 2500 accuracy: 80.9%\n",
      "Step 3000 loss: 0.640200\n",
      "Step 3000 accuracy: 83.2%\n",
      "Step 3500 loss: 0.671201\n",
      "Step 3500 accuracy: 81.4%\n",
      "Step 4000 loss: 0.628480\n",
      "Step 4000 accuracy: 82.9%\n",
      "Step 4500 loss: 0.594592\n",
      "Step 4500 accuracy: 84.6%\n",
      "Step 5000 loss: 0.589558\n",
      "Step 5000 accuracy: 83.3%\n",
      "Accuracy in test dataset: 91.29\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.624467\n",
      "Step 0 accuracy: 11.5%\n",
      "Step 500 loss: 0.942069\n",
      "Step 500 accuracy: 71.3%\n",
      "Step 1000 loss: 0.775332\n",
      "Step 1000 accuracy: 76.5%\n",
      "Step 1500 loss: 0.793724\n",
      "Step 1500 accuracy: 77.9%\n",
      "Step 2000 loss: 0.734963\n",
      "Step 2000 accuracy: 78.7%\n",
      "Step 2500 loss: 0.818738\n",
      "Step 2500 accuracy: 76.3%\n",
      "Step 3000 loss: 0.751902\n",
      "Step 3000 accuracy: 79.2%\n",
      "Step 3500 loss: 0.801690\n",
      "Step 3500 accuracy: 76.3%\n",
      "Step 4000 loss: 0.783849\n",
      "Step 4000 accuracy: 78.7%\n",
      "Step 4500 loss: 0.681211\n",
      "Step 4500 accuracy: 80.5%\n",
      "Step 5000 loss: 0.724869\n",
      "Step 5000 accuracy: 79.8%\n",
      "Accuracy in test dataset: 90.77\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.340925\n",
      "Step 0 accuracy: 8.5%\n",
      "Step 500 loss: 0.591601\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.498133\n",
      "Step 1000 accuracy: 86.5%\n",
      "Step 1500 loss: 0.490991\n",
      "Step 1500 accuracy: 86.9%\n",
      "Step 2000 loss: 0.481066\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.526403\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.444959\n",
      "Step 3000 accuracy: 86.8%\n",
      "Step 3500 loss: 0.453237\n",
      "Step 3500 accuracy: 87.3%\n",
      "Step 4000 loss: 0.450988\n",
      "Step 4000 accuracy: 86.9%\n",
      "Step 4500 loss: 0.389575\n",
      "Step 4500 accuracy: 88.1%\n",
      "Step 5000 loss: 0.398738\n",
      "Step 5000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 93.22\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.297441\n",
      "Step 0 accuracy: 12.2%\n",
      "Step 500 loss: 0.606412\n",
      "Step 500 accuracy: 81.5%\n",
      "Step 1000 loss: 0.509888\n",
      "Step 1000 accuracy: 85.7%\n",
      "Step 1500 loss: 0.497260\n",
      "Step 1500 accuracy: 86.9%\n",
      "Step 2000 loss: 0.500039\n",
      "Step 2000 accuracy: 85.7%\n",
      "Step 2500 loss: 0.539179\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.468590\n",
      "Step 3000 accuracy: 87.0%\n",
      "Step 3500 loss: 0.482356\n",
      "Step 3500 accuracy: 86.4%\n",
      "Step 4000 loss: 0.465761\n",
      "Step 4000 accuracy: 85.7%\n",
      "Step 4500 loss: 0.417396\n",
      "Step 4500 accuracy: 87.3%\n",
      "Step 5000 loss: 0.425437\n",
      "Step 5000 accuracy: 87.3%\n",
      "Accuracy in test dataset: 93.31\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.387692\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.653166\n",
      "Step 500 accuracy: 80.0%\n",
      "Step 1000 loss: 0.556090\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.546027\n",
      "Step 1500 accuracy: 85.7%\n",
      "Step 2000 loss: 0.533284\n",
      "Step 2000 accuracy: 85.0%\n",
      "Step 2500 loss: 0.582614\n",
      "Step 2500 accuracy: 83.0%\n",
      "Step 3000 loss: 0.528797\n",
      "Step 3000 accuracy: 84.7%\n",
      "Step 3500 loss: 0.539055\n",
      "Step 3500 accuracy: 84.0%\n",
      "Step 4000 loss: 0.522073\n",
      "Step 4000 accuracy: 85.8%\n",
      "Step 4500 loss: 0.473130\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.470223\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 92.43\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.418764\n",
      "Step 0 accuracy: 9.1%\n",
      "Step 500 loss: 0.710363\n",
      "Step 500 accuracy: 78.3%\n",
      "Step 1000 loss: 0.604355\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.612352\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.593696\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.655986\n",
      "Step 2500 accuracy: 81.3%\n",
      "Step 3000 loss: 0.574142\n",
      "Step 3000 accuracy: 84.1%\n",
      "Step 3500 loss: 0.641446\n",
      "Step 3500 accuracy: 81.2%\n",
      "Step 4000 loss: 0.609852\n",
      "Step 4000 accuracy: 84.3%\n",
      "Step 4500 loss: 0.523007\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.560942\n",
      "Step 5000 accuracy: 84.6%\n",
      "Accuracy in test dataset: 91.86\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.569119\n",
      "Step 0 accuracy: 10.7%\n",
      "Step 500 loss: 0.811169\n",
      "Step 500 accuracy: 75.1%\n",
      "Step 1000 loss: 0.680842\n",
      "Step 1000 accuracy: 80.6%\n",
      "Step 1500 loss: 0.712985\n",
      "Step 1500 accuracy: 81.7%\n",
      "Step 2000 loss: 0.692522\n",
      "Step 2000 accuracy: 80.9%\n",
      "Step 2500 loss: 0.738394\n",
      "Step 2500 accuracy: 79.6%\n",
      "Step 3000 loss: 0.658028\n",
      "Step 3000 accuracy: 81.5%\n",
      "Step 3500 loss: 0.687284\n",
      "Step 3500 accuracy: 80.1%\n",
      "Step 4000 loss: 0.650903\n",
      "Step 4000 accuracy: 81.8%\n",
      "Step 4500 loss: 0.582006\n",
      "Step 4500 accuracy: 84.1%\n",
      "Step 5000 loss: 0.627511\n",
      "Step 5000 accuracy: 83.4%\n",
      "Accuracy in test dataset: 91.26\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.306533\n",
      "Step 0 accuracy: 9.3%\n",
      "Step 500 loss: 0.588109\n",
      "Step 500 accuracy: 82.3%\n",
      "Step 1000 loss: 0.491971\n",
      "Step 1000 accuracy: 85.8%\n",
      "Step 1500 loss: 0.480448\n",
      "Step 1500 accuracy: 86.5%\n",
      "Step 2000 loss: 0.476499\n",
      "Step 2000 accuracy: 85.6%\n",
      "Step 2500 loss: 0.517545\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.442848\n",
      "Step 3000 accuracy: 87.8%\n",
      "Step 3500 loss: 0.446887\n",
      "Step 3500 accuracy: 87.3%\n",
      "Step 4000 loss: 0.444916\n",
      "Step 4000 accuracy: 87.1%\n",
      "Step 4500 loss: 0.378452\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.382864\n",
      "Step 5000 accuracy: 89.5%\n",
      "Accuracy in test dataset: 93.55\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.297963\n",
      "Step 0 accuracy: 16.0%\n",
      "Step 500 loss: 0.599102\n",
      "Step 500 accuracy: 81.8%\n",
      "Step 1000 loss: 0.509011\n",
      "Step 1000 accuracy: 85.3%\n",
      "Step 1500 loss: 0.497270\n",
      "Step 1500 accuracy: 86.5%\n",
      "Step 2000 loss: 0.489185\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.531938\n",
      "Step 2500 accuracy: 84.5%\n",
      "Step 3000 loss: 0.460272\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.463048\n",
      "Step 3500 accuracy: 87.1%\n",
      "Step 4000 loss: 0.463391\n",
      "Step 4000 accuracy: 86.8%\n",
      "Step 4500 loss: 0.390920\n",
      "Step 4500 accuracy: 88.2%\n",
      "Step 5000 loss: 0.416573\n",
      "Step 5000 accuracy: 88.2%\n",
      "Accuracy in test dataset: 93.29\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.381369\n",
      "Step 0 accuracy: 9.9%\n",
      "Step 500 loss: 0.633067\n",
      "Step 500 accuracy: 81.1%\n",
      "Step 1000 loss: 0.522099\n",
      "Step 1000 accuracy: 85.7%\n",
      "Step 1500 loss: 0.513685\n",
      "Step 1500 accuracy: 85.8%\n",
      "Step 2000 loss: 0.532856\n",
      "Step 2000 accuracy: 85.0%\n",
      "Step 2500 loss: 0.578677\n",
      "Step 2500 accuracy: 83.0%\n",
      "Step 3000 loss: 0.496940\n",
      "Step 3000 accuracy: 85.4%\n",
      "Step 3500 loss: 0.527668\n",
      "Step 3500 accuracy: 84.6%\n",
      "Step 4000 loss: 0.510693\n",
      "Step 4000 accuracy: 85.1%\n",
      "Step 4500 loss: 0.432493\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.456448\n",
      "Step 5000 accuracy: 86.5%\n",
      "Accuracy in test dataset: 92.95\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.399883\n",
      "Step 0 accuracy: 11.0%\n",
      "Step 500 loss: 0.677659\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.580463\n",
      "Step 1000 accuracy: 82.9%\n",
      "Step 1500 loss: 0.578543\n",
      "Step 1500 accuracy: 84.7%\n",
      "Step 2000 loss: 0.569598\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.633988\n",
      "Step 2500 accuracy: 82.3%\n",
      "Step 3000 loss: 0.557134\n",
      "Step 3000 accuracy: 83.4%\n",
      "Step 3500 loss: 0.580419\n",
      "Step 3500 accuracy: 83.2%\n",
      "Step 4000 loss: 0.554993\n",
      "Step 4000 accuracy: 84.5%\n",
      "Step 4500 loss: 0.486106\n",
      "Step 4500 accuracy: 85.8%\n",
      "Step 5000 loss: 0.513805\n",
      "Step 5000 accuracy: 86.0%\n",
      "Accuracy in test dataset: 92.27\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.519882\n",
      "Step 0 accuracy: 11.6%\n",
      "Step 500 loss: 0.755343\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.674699\n",
      "Step 1000 accuracy: 80.4%\n",
      "Step 1500 loss: 0.618301\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.643342\n",
      "Step 2000 accuracy: 82.3%\n",
      "Step 2500 loss: 0.703352\n",
      "Step 2500 accuracy: 80.1%\n",
      "Step 3000 loss: 0.611809\n",
      "Step 3000 accuracy: 83.5%\n",
      "Step 3500 loss: 0.661536\n",
      "Step 3500 accuracy: 81.6%\n",
      "Step 4000 loss: 0.608824\n",
      "Step 4000 accuracy: 84.1%\n",
      "Step 4500 loss: 0.545621\n",
      "Step 4500 accuracy: 84.7%\n",
      "Step 5000 loss: 0.586569\n",
      "Step 5000 accuracy: 84.8%\n",
      "Accuracy in test dataset: 91.6\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.324829\n",
      "Step 0 accuracy: 10.4%\n",
      "Step 500 loss: 0.573920\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.482251\n",
      "Step 1000 accuracy: 86.6%\n",
      "Step 1500 loss: 0.474963\n",
      "Step 1500 accuracy: 86.9%\n",
      "Step 2000 loss: 0.467376\n",
      "Step 2000 accuracy: 86.4%\n",
      "Step 2500 loss: 0.503367\n",
      "Step 2500 accuracy: 86.0%\n",
      "Step 3000 loss: 0.433792\n",
      "Step 3000 accuracy: 87.6%\n",
      "Step 3500 loss: 0.430378\n",
      "Step 3500 accuracy: 88.1%\n",
      "Step 4000 loss: 0.435047\n",
      "Step 4000 accuracy: 88.0%\n",
      "Step 4500 loss: 0.366636\n",
      "Step 4500 accuracy: 88.9%\n",
      "Step 5000 loss: 0.375650\n",
      "Step 5000 accuracy: 89.8%\n",
      "Accuracy in test dataset: 93.76\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.320397\n",
      "Step 0 accuracy: 8.9%\n",
      "Step 500 loss: 0.590778\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.491187\n",
      "Step 1000 accuracy: 86.1%\n",
      "Step 1500 loss: 0.487384\n",
      "Step 1500 accuracy: 87.5%\n",
      "Step 2000 loss: 0.478833\n",
      "Step 2000 accuracy: 86.6%\n",
      "Step 2500 loss: 0.512757\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.445101\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.444395\n",
      "Step 3500 accuracy: 86.9%\n",
      "Step 4000 loss: 0.447200\n",
      "Step 4000 accuracy: 86.9%\n",
      "Step 4500 loss: 0.389969\n",
      "Step 4500 accuracy: 87.8%\n",
      "Step 5000 loss: 0.398347\n",
      "Step 5000 accuracy: 89.4%\n",
      "Accuracy in test dataset: 93.67\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.341773\n",
      "Step 0 accuracy: 11.4%\n",
      "Step 500 loss: 0.615327\n",
      "Step 500 accuracy: 80.7%\n",
      "Step 1000 loss: 0.521261\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.511770\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.502578\n",
      "Step 2000 accuracy: 85.7%\n",
      "Step 2500 loss: 0.535143\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.486345\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.494004\n",
      "Step 3500 accuracy: 84.7%\n",
      "Step 4000 loss: 0.485531\n",
      "Step 4000 accuracy: 85.4%\n",
      "Step 4500 loss: 0.412473\n",
      "Step 4500 accuracy: 88.0%\n",
      "Step 5000 loss: 0.439181\n",
      "Step 5000 accuracy: 87.7%\n",
      "Accuracy in test dataset: 93.32\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.449828\n",
      "Step 0 accuracy: 9.3%\n",
      "Step 500 loss: 0.658975\n",
      "Step 500 accuracy: 80.7%\n",
      "Step 1000 loss: 0.562155\n",
      "Step 1000 accuracy: 83.7%\n",
      "Step 1500 loss: 0.537695\n",
      "Step 1500 accuracy: 85.7%\n",
      "Step 2000 loss: 0.540174\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.589573\n",
      "Step 2500 accuracy: 83.3%\n",
      "Step 3000 loss: 0.520462\n",
      "Step 3000 accuracy: 84.9%\n",
      "Step 3500 loss: 0.534163\n",
      "Step 3500 accuracy: 86.0%\n",
      "Step 4000 loss: 0.546868\n",
      "Step 4000 accuracy: 84.5%\n",
      "Step 4500 loss: 0.480363\n",
      "Step 4500 accuracy: 85.5%\n",
      "Step 5000 loss: 0.490412\n",
      "Step 5000 accuracy: 86.1%\n",
      "Accuracy in test dataset: 92.6\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.547874\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.697533\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.632407\n",
      "Step 1000 accuracy: 81.5%\n",
      "Step 1500 loss: 0.600253\n",
      "Step 1500 accuracy: 82.9%\n",
      "Step 2000 loss: 0.598982\n",
      "Step 2000 accuracy: 83.1%\n",
      "Step 2500 loss: 0.644443\n",
      "Step 2500 accuracy: 81.5%\n",
      "Step 3000 loss: 0.557378\n",
      "Step 3000 accuracy: 83.4%\n",
      "Step 3500 loss: 0.602200\n",
      "Step 3500 accuracy: 82.9%\n",
      "Step 4000 loss: 0.572483\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.530609\n",
      "Step 4500 accuracy: 84.9%\n",
      "Step 5000 loss: 0.540819\n",
      "Step 5000 accuracy: 85.0%\n",
      "Accuracy in test dataset: 92.05\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.304767\n",
      "Step 0 accuracy: 12.4%\n",
      "Step 500 loss: 0.596147\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.501604\n",
      "Step 1000 accuracy: 85.6%\n",
      "Step 1500 loss: 0.495561\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.487629\n",
      "Step 2000 accuracy: 86.4%\n",
      "Step 2500 loss: 0.530192\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.454432\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.461693\n",
      "Step 3500 accuracy: 86.8%\n",
      "Step 4000 loss: 0.461485\n",
      "Step 4000 accuracy: 86.3%\n",
      "Step 4500 loss: 0.392732\n",
      "Step 4500 accuracy: 87.8%\n",
      "Step 5000 loss: 0.412856\n",
      "Step 5000 accuracy: 88.9%\n",
      "Step 5500 loss: 0.419513\n",
      "Step 5500 accuracy: 88.2%\n",
      "Step 6000 loss: 0.420248\n",
      "Step 6000 accuracy: 87.7%\n",
      "Step 6500 loss: 0.398257\n",
      "Step 6500 accuracy: 89.5%\n",
      "Step 7000 loss: 0.373008\n",
      "Step 7000 accuracy: 89.5%\n",
      "Step 7500 loss: 0.365090\n",
      "Step 7500 accuracy: 90.2%\n",
      "Step 8000 loss: 0.405605\n",
      "Step 8000 accuracy: 88.2%\n",
      "Step 8500 loss: 0.373375\n",
      "Step 8500 accuracy: 89.6%\n",
      "Step 9000 loss: 0.320766\n",
      "Step 9000 accuracy: 90.3%\n",
      "Step 9500 loss: 0.341873\n",
      "Step 9500 accuracy: 90.4%\n",
      "Step 10000 loss: 0.315130\n",
      "Step 10000 accuracy: 91.4%\n",
      "Accuracy in test dataset: 94.47\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.356742\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.607690\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.519764\n",
      "Step 1000 accuracy: 85.7%\n",
      "Step 1500 loss: 0.509303\n",
      "Step 1500 accuracy: 86.5%\n",
      "Step 2000 loss: 0.515957\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.544458\n",
      "Step 2500 accuracy: 84.7%\n",
      "Step 3000 loss: 0.480063\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.502287\n",
      "Step 3500 accuracy: 85.6%\n",
      "Step 4000 loss: 0.491490\n",
      "Step 4000 accuracy: 85.7%\n",
      "Step 4500 loss: 0.425748\n",
      "Step 4500 accuracy: 86.5%\n",
      "Step 5000 loss: 0.444378\n",
      "Step 5000 accuracy: 87.1%\n",
      "Step 5500 loss: 0.448305\n",
      "Step 5500 accuracy: 87.4%\n",
      "Step 6000 loss: 0.469636\n",
      "Step 6000 accuracy: 85.6%\n",
      "Step 6500 loss: 0.427013\n",
      "Step 6500 accuracy: 88.3%\n",
      "Step 7000 loss: 0.414254\n",
      "Step 7000 accuracy: 88.3%\n",
      "Step 7500 loss: 0.408700\n",
      "Step 7500 accuracy: 88.7%\n",
      "Step 8000 loss: 0.449414\n",
      "Step 8000 accuracy: 87.1%\n",
      "Step 8500 loss: 0.410210\n",
      "Step 8500 accuracy: 88.0%\n",
      "Step 9000 loss: 0.356402\n",
      "Step 9000 accuracy: 89.4%\n",
      "Step 9500 loss: 0.379912\n",
      "Step 9500 accuracy: 88.5%\n",
      "Step 10000 loss: 0.361806\n",
      "Step 10000 accuracy: 88.9%\n",
      "Accuracy in test dataset: 93.97\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.344476\n",
      "Step 0 accuracy: 11.6%\n",
      "Step 500 loss: 0.691976\n",
      "Step 500 accuracy: 79.1%\n",
      "Step 1000 loss: 0.581640\n",
      "Step 1000 accuracy: 82.9%\n",
      "Step 1500 loss: 0.578736\n",
      "Step 1500 accuracy: 83.9%\n",
      "Step 2000 loss: 0.565787\n",
      "Step 2000 accuracy: 83.7%\n",
      "Step 2500 loss: 0.605849\n",
      "Step 2500 accuracy: 82.1%\n",
      "Step 3000 loss: 0.538419\n",
      "Step 3000 accuracy: 84.6%\n",
      "Step 3500 loss: 0.589688\n",
      "Step 3500 accuracy: 83.3%\n",
      "Step 4000 loss: 0.584438\n",
      "Step 4000 accuracy: 83.4%\n",
      "Step 4500 loss: 0.497915\n",
      "Step 4500 accuracy: 85.5%\n",
      "Step 5000 loss: 0.519154\n",
      "Step 5000 accuracy: 84.6%\n",
      "Step 5500 loss: 0.518799\n",
      "Step 5500 accuracy: 83.8%\n",
      "Step 6000 loss: 0.528378\n",
      "Step 6000 accuracy: 84.3%\n",
      "Step 6500 loss: 0.530709\n",
      "Step 6500 accuracy: 84.8%\n",
      "Step 7000 loss: 0.486604\n",
      "Step 7000 accuracy: 86.5%\n",
      "Step 7500 loss: 0.483024\n",
      "Step 7500 accuracy: 87.3%\n",
      "Step 8000 loss: 0.514477\n",
      "Step 8000 accuracy: 85.2%\n",
      "Step 8500 loss: 0.486551\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.443462\n",
      "Step 9000 accuracy: 85.7%\n",
      "Step 9500 loss: 0.470168\n",
      "Step 9500 accuracy: 86.6%\n",
      "Step 10000 loss: 0.440675\n",
      "Step 10000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 93.09\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.444475\n",
      "Step 0 accuracy: 9.3%\n",
      "Step 500 loss: 0.786461\n",
      "Step 500 accuracy: 76.8%\n",
      "Step 1000 loss: 0.675488\n",
      "Step 1000 accuracy: 80.1%\n",
      "Step 1500 loss: 0.640867\n",
      "Step 1500 accuracy: 82.4%\n",
      "Step 2000 loss: 0.641391\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.692760\n",
      "Step 2500 accuracy: 80.9%\n",
      "Step 3000 loss: 0.629471\n",
      "Step 3000 accuracy: 83.0%\n",
      "Step 3500 loss: 0.659920\n",
      "Step 3500 accuracy: 81.0%\n",
      "Step 4000 loss: 0.644076\n",
      "Step 4000 accuracy: 83.1%\n",
      "Step 4500 loss: 0.573314\n",
      "Step 4500 accuracy: 84.1%\n",
      "Step 5000 loss: 0.592273\n",
      "Step 5000 accuracy: 83.3%\n",
      "Step 5500 loss: 0.640382\n",
      "Step 5500 accuracy: 80.9%\n",
      "Step 6000 loss: 0.621738\n",
      "Step 6000 accuracy: 81.1%\n",
      "Step 6500 loss: 0.593683\n",
      "Step 6500 accuracy: 84.2%\n",
      "Step 7000 loss: 0.573070\n",
      "Step 7000 accuracy: 84.1%\n",
      "Step 7500 loss: 0.554715\n",
      "Step 7500 accuracy: 85.1%\n",
      "Step 8000 loss: 0.645891\n",
      "Step 8000 accuracy: 82.3%\n",
      "Step 8500 loss: 0.557391\n",
      "Step 8500 accuracy: 83.2%\n",
      "Step 9000 loss: 0.531636\n",
      "Step 9000 accuracy: 84.1%\n",
      "Step 9500 loss: 0.591848\n",
      "Step 9500 accuracy: 83.9%\n",
      "Step 10000 loss: 0.543396\n",
      "Step 10000 accuracy: 84.2%\n",
      "Accuracy in test dataset: 92.12\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.633245\n",
      "Step 0 accuracy: 8.7%\n",
      "Step 500 loss: 0.957852\n",
      "Step 500 accuracy: 70.4%\n",
      "Step 1000 loss: 0.819027\n",
      "Step 1000 accuracy: 75.7%\n",
      "Step 1500 loss: 0.787991\n",
      "Step 1500 accuracy: 78.3%\n",
      "Step 2000 loss: 0.723673\n",
      "Step 2000 accuracy: 80.0%\n",
      "Step 2500 loss: 0.803886\n",
      "Step 2500 accuracy: 75.9%\n",
      "Step 3000 loss: 0.743074\n",
      "Step 3000 accuracy: 78.5%\n",
      "Step 3500 loss: 0.854048\n",
      "Step 3500 accuracy: 74.7%\n",
      "Step 4000 loss: 0.777237\n",
      "Step 4000 accuracy: 78.3%\n",
      "Step 4500 loss: 0.671645\n",
      "Step 4500 accuracy: 81.3%\n",
      "Step 5000 loss: 0.723010\n",
      "Step 5000 accuracy: 78.5%\n",
      "Step 5500 loss: 0.750216\n",
      "Step 5500 accuracy: 78.6%\n",
      "Step 6000 loss: 0.788230\n",
      "Step 6000 accuracy: 76.4%\n",
      "Step 6500 loss: 0.706299\n",
      "Step 6500 accuracy: 81.2%\n",
      "Step 7000 loss: 0.703922\n",
      "Step 7000 accuracy: 81.0%\n",
      "Step 7500 loss: 0.684625\n",
      "Step 7500 accuracy: 79.8%\n",
      "Step 8000 loss: 0.750126\n",
      "Step 8000 accuracy: 79.5%\n",
      "Step 8500 loss: 0.676081\n",
      "Step 8500 accuracy: 79.6%\n",
      "Step 9000 loss: 0.632714\n",
      "Step 9000 accuracy: 81.2%\n",
      "Step 9500 loss: 0.751520\n",
      "Step 9500 accuracy: 78.7%\n",
      "Step 10000 loss: 0.709368\n",
      "Step 10000 accuracy: 79.2%\n",
      "Accuracy in test dataset: 90.93\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.296498\n",
      "Step 0 accuracy: 10.0%\n",
      "Step 500 loss: 0.591662\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.495134\n",
      "Step 1000 accuracy: 86.3%\n",
      "Step 1500 loss: 0.490768\n",
      "Step 1500 accuracy: 86.8%\n",
      "Step 2000 loss: 0.485810\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.527859\n",
      "Step 2500 accuracy: 85.3%\n",
      "Step 3000 loss: 0.453806\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.460481\n",
      "Step 3500 accuracy: 87.4%\n",
      "Step 4000 loss: 0.456047\n",
      "Step 4000 accuracy: 86.9%\n",
      "Step 4500 loss: 0.390418\n",
      "Step 4500 accuracy: 88.1%\n",
      "Step 5000 loss: 0.391606\n",
      "Step 5000 accuracy: 89.0%\n",
      "Step 5500 loss: 0.407406\n",
      "Step 5500 accuracy: 88.2%\n",
      "Step 6000 loss: 0.416202\n",
      "Step 6000 accuracy: 88.0%\n",
      "Step 6500 loss: 0.374650\n",
      "Step 6500 accuracy: 89.9%\n",
      "Step 7000 loss: 0.363892\n",
      "Step 7000 accuracy: 89.6%\n",
      "Step 7500 loss: 0.358675\n",
      "Step 7500 accuracy: 90.2%\n",
      "Step 8000 loss: 0.387749\n",
      "Step 8000 accuracy: 89.0%\n",
      "Step 8500 loss: 0.361549\n",
      "Step 8500 accuracy: 89.7%\n",
      "Step 9000 loss: 0.301304\n",
      "Step 9000 accuracy: 91.1%\n",
      "Step 9500 loss: 0.335903\n",
      "Step 9500 accuracy: 90.0%\n",
      "Step 10000 loss: 0.303794\n",
      "Step 10000 accuracy: 92.3%\n",
      "Accuracy in test dataset: 94.65\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.353825\n",
      "Step 0 accuracy: 6.7%\n",
      "Step 500 loss: 0.598324\n",
      "Step 500 accuracy: 81.5%\n",
      "Step 1000 loss: 0.506769\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.500829\n",
      "Step 1500 accuracy: 86.1%\n",
      "Step 2000 loss: 0.491247\n",
      "Step 2000 accuracy: 85.7%\n",
      "Step 2500 loss: 0.543713\n",
      "Step 2500 accuracy: 83.7%\n",
      "Step 3000 loss: 0.464966\n",
      "Step 3000 accuracy: 86.4%\n",
      "Step 3500 loss: 0.485815\n",
      "Step 3500 accuracy: 85.6%\n",
      "Step 4000 loss: 0.475504\n",
      "Step 4000 accuracy: 85.7%\n",
      "Step 4500 loss: 0.397322\n",
      "Step 4500 accuracy: 87.8%\n",
      "Step 5000 loss: 0.425964\n",
      "Step 5000 accuracy: 87.8%\n",
      "Step 5500 loss: 0.440182\n",
      "Step 5500 accuracy: 86.6%\n",
      "Step 6000 loss: 0.432236\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.399618\n",
      "Step 6500 accuracy: 89.3%\n",
      "Step 7000 loss: 0.391534\n",
      "Step 7000 accuracy: 89.4%\n",
      "Step 7500 loss: 0.400386\n",
      "Step 7500 accuracy: 89.6%\n",
      "Step 8000 loss: 0.413948\n",
      "Step 8000 accuracy: 87.4%\n",
      "Step 8500 loss: 0.382799\n",
      "Step 8500 accuracy: 89.0%\n",
      "Step 9000 loss: 0.327632\n",
      "Step 9000 accuracy: 90.4%\n",
      "Step 9500 loss: 0.369068\n",
      "Step 9500 accuracy: 88.9%\n",
      "Step 10000 loss: 0.333021\n",
      "Step 10000 accuracy: 91.2%\n",
      "Accuracy in test dataset: 94.37\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.392511\n",
      "Step 0 accuracy: 8.1%\n",
      "Step 500 loss: 0.655919\n",
      "Step 500 accuracy: 80.3%\n",
      "Step 1000 loss: 0.539857\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.539703\n",
      "Step 1500 accuracy: 86.3%\n",
      "Step 2000 loss: 0.522146\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.588066\n",
      "Step 2500 accuracy: 82.3%\n",
      "Step 3000 loss: 0.519218\n",
      "Step 3000 accuracy: 84.9%\n",
      "Step 3500 loss: 0.542978\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.546072\n",
      "Step 4000 accuracy: 84.5%\n",
      "Step 4500 loss: 0.463488\n",
      "Step 4500 accuracy: 86.0%\n",
      "Step 5000 loss: 0.461743\n",
      "Step 5000 accuracy: 86.5%\n",
      "Step 5500 loss: 0.508680\n",
      "Step 5500 accuracy: 85.0%\n",
      "Step 6000 loss: 0.508680\n",
      "Step 6000 accuracy: 84.4%\n",
      "Step 6500 loss: 0.454278\n",
      "Step 6500 accuracy: 87.4%\n",
      "Step 7000 loss: 0.471338\n",
      "Step 7000 accuracy: 87.0%\n",
      "Step 7500 loss: 0.458859\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.504720\n",
      "Step 8000 accuracy: 85.3%\n",
      "Step 8500 loss: 0.454326\n",
      "Step 8500 accuracy: 87.3%\n",
      "Step 9000 loss: 0.407855\n",
      "Step 9000 accuracy: 86.5%\n",
      "Step 9500 loss: 0.438447\n",
      "Step 9500 accuracy: 87.6%\n",
      "Step 10000 loss: 0.408497\n",
      "Step 10000 accuracy: 87.7%\n",
      "Accuracy in test dataset: 93.73\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.452916\n",
      "Step 0 accuracy: 9.7%\n",
      "Step 500 loss: 0.716904\n",
      "Step 500 accuracy: 78.3%\n",
      "Step 1000 loss: 0.611010\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.592677\n",
      "Step 1500 accuracy: 83.7%\n",
      "Step 2000 loss: 0.588090\n",
      "Step 2000 accuracy: 83.3%\n",
      "Step 2500 loss: 0.642307\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.577843\n",
      "Step 3000 accuracy: 83.9%\n",
      "Step 3500 loss: 0.611638\n",
      "Step 3500 accuracy: 82.2%\n",
      "Step 4000 loss: 0.608339\n",
      "Step 4000 accuracy: 83.3%\n",
      "Step 4500 loss: 0.514368\n",
      "Step 4500 accuracy: 85.8%\n",
      "Step 5000 loss: 0.544165\n",
      "Step 5000 accuracy: 84.9%\n",
      "Step 5500 loss: 0.544469\n",
      "Step 5500 accuracy: 83.0%\n",
      "Step 6000 loss: 0.576393\n",
      "Step 6000 accuracy: 82.4%\n",
      "Step 6500 loss: 0.506930\n",
      "Step 6500 accuracy: 84.7%\n",
      "Step 7000 loss: 0.507944\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.500254\n",
      "Step 7500 accuracy: 86.4%\n",
      "Step 8000 loss: 0.578939\n",
      "Step 8000 accuracy: 84.2%\n",
      "Step 8500 loss: 0.511318\n",
      "Step 8500 accuracy: 84.6%\n",
      "Step 9000 loss: 0.462837\n",
      "Step 9000 accuracy: 85.5%\n",
      "Step 9500 loss: 0.512320\n",
      "Step 9500 accuracy: 85.4%\n",
      "Step 10000 loss: 0.475704\n",
      "Step 10000 accuracy: 85.4%\n",
      "Accuracy in test dataset: 92.81\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.525861\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.866281\n",
      "Step 500 accuracy: 74.9%\n",
      "Step 1000 loss: 0.682403\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.666376\n",
      "Step 1500 accuracy: 81.9%\n",
      "Step 2000 loss: 0.652585\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.720162\n",
      "Step 2500 accuracy: 80.1%\n",
      "Step 3000 loss: 0.643759\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.734954\n",
      "Step 3500 accuracy: 78.3%\n",
      "Step 4000 loss: 0.690258\n",
      "Step 4000 accuracy: 80.8%\n",
      "Step 4500 loss: 0.595265\n",
      "Step 4500 accuracy: 84.0%\n",
      "Step 5000 loss: 0.629114\n",
      "Step 5000 accuracy: 82.6%\n",
      "Step 5500 loss: 0.653415\n",
      "Step 5500 accuracy: 81.5%\n",
      "Step 6000 loss: 0.670859\n",
      "Step 6000 accuracy: 80.4%\n",
      "Step 6500 loss: 0.608663\n",
      "Step 6500 accuracy: 83.5%\n",
      "Step 7000 loss: 0.622892\n",
      "Step 7000 accuracy: 82.5%\n",
      "Step 7500 loss: 0.607220\n",
      "Step 7500 accuracy: 84.2%\n",
      "Step 8000 loss: 0.658699\n",
      "Step 8000 accuracy: 80.6%\n",
      "Step 8500 loss: 0.604025\n",
      "Step 8500 accuracy: 82.4%\n",
      "Step 9000 loss: 0.567944\n",
      "Step 9000 accuracy: 82.2%\n",
      "Step 9500 loss: 0.645145\n",
      "Step 9500 accuracy: 82.3%\n",
      "Step 10000 loss: 0.580477\n",
      "Step 10000 accuracy: 83.2%\n",
      "Accuracy in test dataset: 91.88\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.329809\n",
      "Step 0 accuracy: 12.0%\n",
      "Step 500 loss: 0.587018\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.495328\n",
      "Step 1000 accuracy: 85.8%\n",
      "Step 1500 loss: 0.483408\n",
      "Step 1500 accuracy: 87.0%\n",
      "Step 2000 loss: 0.474704\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.523124\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.444310\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.445133\n",
      "Step 3500 accuracy: 87.2%\n",
      "Step 4000 loss: 0.444043\n",
      "Step 4000 accuracy: 87.4%\n",
      "Step 4500 loss: 0.379383\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.389381\n",
      "Step 5000 accuracy: 88.8%\n",
      "Step 5500 loss: 0.397349\n",
      "Step 5500 accuracy: 88.2%\n",
      "Step 6000 loss: 0.401500\n",
      "Step 6000 accuracy: 88.6%\n",
      "Step 6500 loss: 0.357935\n",
      "Step 6500 accuracy: 90.5%\n",
      "Step 7000 loss: 0.359082\n",
      "Step 7000 accuracy: 89.1%\n",
      "Step 7500 loss: 0.347212\n",
      "Step 7500 accuracy: 90.8%\n",
      "Step 8000 loss: 0.375268\n",
      "Step 8000 accuracy: 89.7%\n",
      "Step 8500 loss: 0.352632\n",
      "Step 8500 accuracy: 89.7%\n",
      "Step 9000 loss: 0.291712\n",
      "Step 9000 accuracy: 91.3%\n",
      "Step 9500 loss: 0.323923\n",
      "Step 9500 accuracy: 91.2%\n",
      "Step 10000 loss: 0.299389\n",
      "Step 10000 accuracy: 91.8%\n",
      "Accuracy in test dataset: 94.8\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.356053\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.603358\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.508533\n",
      "Step 1000 accuracy: 85.9%\n",
      "Step 1500 loss: 0.492765\n",
      "Step 1500 accuracy: 86.6%\n",
      "Step 2000 loss: 0.491434\n",
      "Step 2000 accuracy: 85.7%\n",
      "Step 2500 loss: 0.527522\n",
      "Step 2500 accuracy: 84.6%\n",
      "Step 3000 loss: 0.463468\n",
      "Step 3000 accuracy: 85.7%\n",
      "Step 3500 loss: 0.467851\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.467406\n",
      "Step 4000 accuracy: 86.2%\n",
      "Step 4500 loss: 0.403222\n",
      "Step 4500 accuracy: 87.6%\n",
      "Step 5000 loss: 0.401309\n",
      "Step 5000 accuracy: 88.3%\n",
      "Step 5500 loss: 0.420316\n",
      "Step 5500 accuracy: 87.6%\n",
      "Step 6000 loss: 0.438547\n",
      "Step 6000 accuracy: 87.0%\n",
      "Step 6500 loss: 0.381724\n",
      "Step 6500 accuracy: 89.4%\n",
      "Step 7000 loss: 0.379003\n",
      "Step 7000 accuracy: 89.4%\n",
      "Step 7500 loss: 0.381073\n",
      "Step 7500 accuracy: 89.2%\n",
      "Step 8000 loss: 0.399923\n",
      "Step 8000 accuracy: 88.7%\n",
      "Step 8500 loss: 0.386200\n",
      "Step 8500 accuracy: 89.6%\n",
      "Step 9000 loss: 0.315501\n",
      "Step 9000 accuracy: 90.1%\n",
      "Step 9500 loss: 0.344301\n",
      "Step 9500 accuracy: 89.4%\n",
      "Step 10000 loss: 0.324368\n",
      "Step 10000 accuracy: 89.8%\n",
      "Accuracy in test dataset: 94.67\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.399190\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.626874\n",
      "Step 500 accuracy: 81.3%\n",
      "Step 1000 loss: 0.531885\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.542923\n",
      "Step 1500 accuracy: 84.7%\n",
      "Step 2000 loss: 0.533876\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.570209\n",
      "Step 2500 accuracy: 83.9%\n",
      "Step 3000 loss: 0.502889\n",
      "Step 3000 accuracy: 86.1%\n",
      "Step 3500 loss: 0.512149\n",
      "Step 3500 accuracy: 85.4%\n",
      "Step 4000 loss: 0.510003\n",
      "Step 4000 accuracy: 84.9%\n",
      "Step 4500 loss: 0.448437\n",
      "Step 4500 accuracy: 86.8%\n",
      "Step 5000 loss: 0.447302\n",
      "Step 5000 accuracy: 86.8%\n",
      "Step 5500 loss: 0.472466\n",
      "Step 5500 accuracy: 85.4%\n",
      "Step 6000 loss: 0.492269\n",
      "Step 6000 accuracy: 84.0%\n",
      "Step 6500 loss: 0.453547\n",
      "Step 6500 accuracy: 87.1%\n",
      "Step 7000 loss: 0.426985\n",
      "Step 7000 accuracy: 87.6%\n",
      "Step 7500 loss: 0.440798\n",
      "Step 7500 accuracy: 87.5%\n",
      "Step 8000 loss: 0.463804\n",
      "Step 8000 accuracy: 87.0%\n",
      "Step 8500 loss: 0.423593\n",
      "Step 8500 accuracy: 87.8%\n",
      "Step 9000 loss: 0.386253\n",
      "Step 9000 accuracy: 87.9%\n",
      "Step 9500 loss: 0.408040\n",
      "Step 9500 accuracy: 88.1%\n",
      "Step 10000 loss: 0.393101\n",
      "Step 10000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 94.11\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.437978\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.684894\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.588158\n",
      "Step 1000 accuracy: 83.7%\n",
      "Step 1500 loss: 0.569063\n",
      "Step 1500 accuracy: 84.1%\n",
      "Step 2000 loss: 0.575164\n",
      "Step 2000 accuracy: 83.8%\n",
      "Step 2500 loss: 0.621191\n",
      "Step 2500 accuracy: 82.1%\n",
      "Step 3000 loss: 0.552906\n",
      "Step 3000 accuracy: 84.9%\n",
      "Step 3500 loss: 0.574954\n",
      "Step 3500 accuracy: 82.8%\n",
      "Step 4000 loss: 0.547012\n",
      "Step 4000 accuracy: 83.4%\n",
      "Step 4500 loss: 0.499970\n",
      "Step 4500 accuracy: 85.4%\n",
      "Step 5000 loss: 0.505826\n",
      "Step 5000 accuracy: 85.6%\n",
      "Step 5500 loss: 0.536576\n",
      "Step 5500 accuracy: 83.8%\n",
      "Step 6000 loss: 0.539785\n",
      "Step 6000 accuracy: 83.2%\n",
      "Step 6500 loss: 0.488719\n",
      "Step 6500 accuracy: 86.0%\n",
      "Step 7000 loss: 0.485270\n",
      "Step 7000 accuracy: 86.8%\n",
      "Step 7500 loss: 0.482629\n",
      "Step 7500 accuracy: 87.5%\n",
      "Step 8000 loss: 0.513113\n",
      "Step 8000 accuracy: 84.8%\n",
      "Step 8500 loss: 0.496335\n",
      "Step 8500 accuracy: 86.3%\n",
      "Step 9000 loss: 0.428814\n",
      "Step 9000 accuracy: 87.3%\n",
      "Step 9500 loss: 0.502139\n",
      "Step 9500 accuracy: 85.8%\n",
      "Step 10000 loss: 0.453386\n",
      "Step 10000 accuracy: 86.8%\n",
      "Accuracy in test dataset: 93.52\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.598477\n",
      "Step 0 accuracy: 8.8%\n",
      "Step 500 loss: 0.772072\n",
      "Step 500 accuracy: 76.2%\n",
      "Step 1000 loss: 0.641991\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.630275\n",
      "Step 1500 accuracy: 81.7%\n",
      "Step 2000 loss: 0.615003\n",
      "Step 2000 accuracy: 82.3%\n",
      "Step 2500 loss: 0.661464\n",
      "Step 2500 accuracy: 80.2%\n",
      "Step 3000 loss: 0.586697\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.638638\n",
      "Step 3500 accuracy: 81.5%\n",
      "Step 4000 loss: 0.619413\n",
      "Step 4000 accuracy: 83.0%\n",
      "Step 4500 loss: 0.543082\n",
      "Step 4500 accuracy: 85.8%\n",
      "Step 5000 loss: 0.563862\n",
      "Step 5000 accuracy: 83.8%\n",
      "Step 5500 loss: 0.604190\n",
      "Step 5500 accuracy: 82.6%\n",
      "Step 6000 loss: 0.609627\n",
      "Step 6000 accuracy: 82.2%\n",
      "Step 6500 loss: 0.581982\n",
      "Step 6500 accuracy: 84.3%\n",
      "Step 7000 loss: 0.551504\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.571941\n",
      "Step 7500 accuracy: 84.1%\n",
      "Step 8000 loss: 0.564077\n",
      "Step 8000 accuracy: 83.7%\n",
      "Step 8500 loss: 0.534019\n",
      "Step 8500 accuracy: 85.0%\n",
      "Step 9000 loss: 0.493750\n",
      "Step 9000 accuracy: 85.4%\n",
      "Step 9500 loss: 0.563542\n",
      "Step 9500 accuracy: 84.7%\n",
      "Step 10000 loss: 0.525824\n",
      "Step 10000 accuracy: 83.5%\n",
      "Accuracy in test dataset: 92.34\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.281417\n",
      "Step 0 accuracy: 13.5%\n",
      "Step 500 loss: 0.577727\n",
      "Step 500 accuracy: 82.5%\n",
      "Step 1000 loss: 0.486208\n",
      "Step 1000 accuracy: 86.5%\n",
      "Step 1500 loss: 0.473923\n",
      "Step 1500 accuracy: 87.0%\n",
      "Step 2000 loss: 0.468147\n",
      "Step 2000 accuracy: 86.4%\n",
      "Step 2500 loss: 0.509085\n",
      "Step 2500 accuracy: 86.0%\n",
      "Step 3000 loss: 0.434244\n",
      "Step 3000 accuracy: 87.6%\n",
      "Step 3500 loss: 0.432792\n",
      "Step 3500 accuracy: 87.7%\n",
      "Step 4000 loss: 0.436214\n",
      "Step 4000 accuracy: 87.9%\n",
      "Step 4500 loss: 0.373112\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.373825\n",
      "Step 5000 accuracy: 89.8%\n",
      "Step 5500 loss: 0.387481\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.392486\n",
      "Step 6000 accuracy: 88.0%\n",
      "Step 6500 loss: 0.348480\n",
      "Step 6500 accuracy: 90.4%\n",
      "Step 7000 loss: 0.347656\n",
      "Step 7000 accuracy: 90.1%\n",
      "Step 7500 loss: 0.340800\n",
      "Step 7500 accuracy: 90.7%\n",
      "Step 8000 loss: 0.363496\n",
      "Step 8000 accuracy: 89.6%\n",
      "Step 8500 loss: 0.343407\n",
      "Step 8500 accuracy: 90.6%\n",
      "Step 9000 loss: 0.292397\n",
      "Step 9000 accuracy: 91.2%\n",
      "Step 9500 loss: 0.315870\n",
      "Step 9500 accuracy: 91.1%\n",
      "Step 10000 loss: 0.285284\n",
      "Step 10000 accuracy: 92.3%\n",
      "Accuracy in test dataset: 95.0\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.307015\n",
      "Step 0 accuracy: 11.5%\n",
      "Step 500 loss: 0.587312\n",
      "Step 500 accuracy: 81.9%\n",
      "Step 1000 loss: 0.496631\n",
      "Step 1000 accuracy: 86.0%\n",
      "Step 1500 loss: 0.479866\n",
      "Step 1500 accuracy: 86.4%\n",
      "Step 2000 loss: 0.473035\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.516296\n",
      "Step 2500 accuracy: 85.1%\n",
      "Step 3000 loss: 0.442684\n",
      "Step 3000 accuracy: 87.3%\n",
      "Step 3500 loss: 0.440769\n",
      "Step 3500 accuracy: 87.4%\n",
      "Step 4000 loss: 0.445233\n",
      "Step 4000 accuracy: 87.1%\n",
      "Step 4500 loss: 0.385105\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.390732\n",
      "Step 5000 accuracy: 89.0%\n",
      "Step 5500 loss: 0.406497\n",
      "Step 5500 accuracy: 88.2%\n",
      "Step 6000 loss: 0.407654\n",
      "Step 6000 accuracy: 87.6%\n",
      "Step 6500 loss: 0.373929\n",
      "Step 6500 accuracy: 89.7%\n",
      "Step 7000 loss: 0.359756\n",
      "Step 7000 accuracy: 90.0%\n",
      "Step 7500 loss: 0.353209\n",
      "Step 7500 accuracy: 89.8%\n",
      "Step 8000 loss: 0.385683\n",
      "Step 8000 accuracy: 88.6%\n",
      "Step 8500 loss: 0.358123\n",
      "Step 8500 accuracy: 89.4%\n",
      "Step 9000 loss: 0.307451\n",
      "Step 9000 accuracy: 91.0%\n",
      "Step 9500 loss: 0.338782\n",
      "Step 9500 accuracy: 90.5%\n",
      "Step 10000 loss: 0.304003\n",
      "Step 10000 accuracy: 91.8%\n",
      "Accuracy in test dataset: 94.68\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.337338\n",
      "Step 0 accuracy: 11.1%\n",
      "Step 500 loss: 0.609473\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.535167\n",
      "Step 1000 accuracy: 84.7%\n",
      "Step 1500 loss: 0.518299\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.496570\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.546804\n",
      "Step 2500 accuracy: 84.5%\n",
      "Step 3000 loss: 0.486242\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.495889\n",
      "Step 3500 accuracy: 86.1%\n",
      "Step 4000 loss: 0.482976\n",
      "Step 4000 accuracy: 86.5%\n",
      "Step 4500 loss: 0.425593\n",
      "Step 4500 accuracy: 87.0%\n",
      "Step 5000 loss: 0.439946\n",
      "Step 5000 accuracy: 87.6%\n",
      "Step 5500 loss: 0.444978\n",
      "Step 5500 accuracy: 86.2%\n",
      "Step 6000 loss: 0.439639\n",
      "Step 6000 accuracy: 87.8%\n",
      "Step 6500 loss: 0.428649\n",
      "Step 6500 accuracy: 88.5%\n",
      "Step 7000 loss: 0.426598\n",
      "Step 7000 accuracy: 88.3%\n",
      "Step 7500 loss: 0.417677\n",
      "Step 7500 accuracy: 89.2%\n",
      "Step 8000 loss: 0.435821\n",
      "Step 8000 accuracy: 87.3%\n",
      "Step 8500 loss: 0.425383\n",
      "Step 8500 accuracy: 88.3%\n",
      "Step 9000 loss: 0.360541\n",
      "Step 9000 accuracy: 88.7%\n",
      "Step 9500 loss: 0.385304\n",
      "Step 9500 accuracy: 89.3%\n",
      "Step 10000 loss: 0.356351\n",
      "Step 10000 accuracy: 89.8%\n",
      "Accuracy in test dataset: 94.49\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.457185\n",
      "Step 0 accuracy: 10.8%\n",
      "Step 500 loss: 0.646277\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.568679\n",
      "Step 1000 accuracy: 84.3%\n",
      "Step 1500 loss: 0.558136\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.533723\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.594853\n",
      "Step 2500 accuracy: 82.7%\n",
      "Step 3000 loss: 0.519444\n",
      "Step 3000 accuracy: 84.6%\n",
      "Step 3500 loss: 0.565799\n",
      "Step 3500 accuracy: 84.4%\n",
      "Step 4000 loss: 0.532963\n",
      "Step 4000 accuracy: 85.3%\n",
      "Step 4500 loss: 0.464306\n",
      "Step 4500 accuracy: 86.1%\n",
      "Step 5000 loss: 0.487830\n",
      "Step 5000 accuracy: 85.5%\n",
      "Step 5500 loss: 0.492985\n",
      "Step 5500 accuracy: 84.4%\n",
      "Step 6000 loss: 0.497716\n",
      "Step 6000 accuracy: 84.6%\n",
      "Step 6500 loss: 0.489957\n",
      "Step 6500 accuracy: 86.4%\n",
      "Step 7000 loss: 0.468669\n",
      "Step 7000 accuracy: 86.2%\n",
      "Step 7500 loss: 0.456897\n",
      "Step 7500 accuracy: 87.1%\n",
      "Step 8000 loss: 0.499941\n",
      "Step 8000 accuracy: 85.8%\n",
      "Step 8500 loss: 0.477431\n",
      "Step 8500 accuracy: 86.8%\n",
      "Step 9000 loss: 0.394740\n",
      "Step 9000 accuracy: 88.2%\n",
      "Step 9500 loss: 0.456741\n",
      "Step 9500 accuracy: 86.4%\n",
      "Step 10000 loss: 0.422430\n",
      "Step 10000 accuracy: 87.1%\n",
      "Accuracy in test dataset: 93.93\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 1024 | over_fit : False |\n",
      "Step 0 loss: 2.578091\n",
      "Step 0 accuracy: 10.8%\n",
      "Step 500 loss: 0.741477\n",
      "Step 500 accuracy: 78.3%\n",
      "Step 1000 loss: 0.615374\n",
      "Step 1000 accuracy: 82.3%\n",
      "Step 1500 loss: 0.596087\n",
      "Step 1500 accuracy: 83.1%\n",
      "Step 2000 loss: 0.591416\n",
      "Step 2000 accuracy: 83.3%\n",
      "Step 2500 loss: 0.657323\n",
      "Step 2500 accuracy: 81.7%\n",
      "Step 3000 loss: 0.568744\n",
      "Step 3000 accuracy: 84.2%\n",
      "Step 3500 loss: 0.586670\n",
      "Step 3500 accuracy: 84.2%\n",
      "Step 4000 loss: 0.605155\n",
      "Step 4000 accuracy: 82.3%\n",
      "Step 4500 loss: 0.526377\n",
      "Step 4500 accuracy: 85.0%\n",
      "Step 5000 loss: 0.543213\n",
      "Step 5000 accuracy: 85.2%\n",
      "Step 5500 loss: 0.545475\n",
      "Step 5500 accuracy: 84.2%\n",
      "Step 6000 loss: 0.579467\n",
      "Step 6000 accuracy: 81.3%\n",
      "Step 6500 loss: 0.539053\n",
      "Step 6500 accuracy: 85.0%\n",
      "Step 7000 loss: 0.511616\n",
      "Step 7000 accuracy: 85.3%\n",
      "Step 7500 loss: 0.515486\n",
      "Step 7500 accuracy: 85.4%\n",
      "Step 8000 loss: 0.569337\n",
      "Step 8000 accuracy: 83.7%\n",
      "Step 8500 loss: 0.508102\n",
      "Step 8500 accuracy: 85.3%\n",
      "Step 9000 loss: 0.465624\n",
      "Step 9000 accuracy: 86.5%\n",
      "Step 9500 loss: 0.503295\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.479961\n",
      "Step 10000 accuracy: 85.3%\n",
      "Accuracy in test dataset: 92.96\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.375199\n",
      "Step 0 accuracy: 6.9%\n",
      "Step 500 loss: 0.626300\n",
      "Step 500 accuracy: 82.1%\n",
      "Step 1000 loss: 0.541450\n",
      "Step 1000 accuracy: 83.7%\n",
      "Step 1500 loss: 0.523438\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.517548\n",
      "Step 2000 accuracy: 85.1%\n",
      "Step 2500 loss: 0.472886\n",
      "Step 2500 accuracy: 86.2%\n",
      "Accuracy in test dataset: 91.77\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.322182\n",
      "Step 0 accuracy: 11.1%\n",
      "Step 500 loss: 0.644336\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.568505\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.540973\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.529385\n",
      "Step 2000 accuracy: 85.0%\n",
      "Step 2500 loss: 0.498372\n",
      "Step 2500 accuracy: 85.4%\n",
      "Accuracy in test dataset: 91.77\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.356176\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.721636\n",
      "Step 500 accuracy: 80.2%\n",
      "Step 1000 loss: 0.628631\n",
      "Step 1000 accuracy: 81.8%\n",
      "Step 1500 loss: 0.600524\n",
      "Step 1500 accuracy: 83.0%\n",
      "Step 2000 loss: 0.588528\n",
      "Step 2000 accuracy: 83.5%\n",
      "Step 2500 loss: 0.578499\n",
      "Step 2500 accuracy: 83.4%\n",
      "Accuracy in test dataset: 91.25\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.439361\n",
      "Step 0 accuracy: 8.9%\n",
      "Step 500 loss: 0.808859\n",
      "Step 500 accuracy: 77.4%\n",
      "Step 1000 loss: 0.711919\n",
      "Step 1000 accuracy: 79.5%\n",
      "Step 1500 loss: 0.683556\n",
      "Step 1500 accuracy: 81.3%\n",
      "Step 2000 loss: 0.680540\n",
      "Step 2000 accuracy: 81.7%\n",
      "Step 2500 loss: 0.619096\n",
      "Step 2500 accuracy: 81.7%\n",
      "Accuracy in test dataset: 90.91\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.588387\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.942773\n",
      "Step 500 accuracy: 72.8%\n",
      "Step 1000 loss: 0.834774\n",
      "Step 1000 accuracy: 75.8%\n",
      "Step 1500 loss: 0.810845\n",
      "Step 1500 accuracy: 77.0%\n",
      "Step 2000 loss: 0.795786\n",
      "Step 2000 accuracy: 78.4%\n",
      "Step 2500 loss: 0.754839\n",
      "Step 2500 accuracy: 78.4%\n",
      "Accuracy in test dataset: 90.27\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.317044\n",
      "Step 0 accuracy: 12.4%\n",
      "Step 500 loss: 0.625578\n",
      "Step 500 accuracy: 81.8%\n",
      "Step 1000 loss: 0.539198\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.514184\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.506897\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.468007\n",
      "Step 2500 accuracy: 86.2%\n",
      "Accuracy in test dataset: 91.86\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.320378\n",
      "Step 0 accuracy: 13.2%\n",
      "Step 500 loss: 0.635576\n",
      "Step 500 accuracy: 81.7%\n",
      "Step 1000 loss: 0.552366\n",
      "Step 1000 accuracy: 83.4%\n",
      "Step 1500 loss: 0.525794\n",
      "Step 1500 accuracy: 84.6%\n",
      "Step 2000 loss: 0.520859\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.489494\n",
      "Step 2500 accuracy: 85.8%\n",
      "Accuracy in test dataset: 91.85\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.399039\n",
      "Step 0 accuracy: 8.9%\n",
      "Step 500 loss: 0.685376\n",
      "Step 500 accuracy: 80.8%\n",
      "Step 1000 loss: 0.607093\n",
      "Step 1000 accuracy: 81.8%\n",
      "Step 1500 loss: 0.566965\n",
      "Step 1500 accuracy: 83.4%\n",
      "Step 2000 loss: 0.567593\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.524779\n",
      "Step 2500 accuracy: 84.9%\n",
      "Accuracy in test dataset: 91.56\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.445819\n",
      "Step 0 accuracy: 10.6%\n",
      "Step 500 loss: 0.751926\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.638412\n",
      "Step 1000 accuracy: 81.4%\n",
      "Step 1500 loss: 0.626099\n",
      "Step 1500 accuracy: 82.6%\n",
      "Step 2000 loss: 0.621492\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.593576\n",
      "Step 2500 accuracy: 83.1%\n",
      "Accuracy in test dataset: 91.06\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.561781\n",
      "Step 0 accuracy: 10.0%\n",
      "Step 500 loss: 0.815874\n",
      "Step 500 accuracy: 76.8%\n",
      "Step 1000 loss: 0.757221\n",
      "Step 1000 accuracy: 78.2%\n",
      "Step 1500 loss: 0.716421\n",
      "Step 1500 accuracy: 80.2%\n",
      "Step 2000 loss: 0.713163\n",
      "Step 2000 accuracy: 80.9%\n",
      "Step 2500 loss: 0.668486\n",
      "Step 2500 accuracy: 81.0%\n",
      "Accuracy in test dataset: 90.64\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.356933\n",
      "Step 0 accuracy: 9.2%\n",
      "Step 500 loss: 0.614437\n",
      "Step 500 accuracy: 82.5%\n",
      "Step 1000 loss: 0.532535\n",
      "Step 1000 accuracy: 84.3%\n",
      "Step 1500 loss: 0.512152\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.497962\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.465618\n",
      "Step 2500 accuracy: 86.2%\n",
      "Accuracy in test dataset: 92.11\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.340498\n",
      "Step 0 accuracy: 8.4%\n",
      "Step 500 loss: 0.626982\n",
      "Step 500 accuracy: 81.8%\n",
      "Step 1000 loss: 0.538667\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.522785\n",
      "Step 1500 accuracy: 84.5%\n",
      "Step 2000 loss: 0.512895\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.477512\n",
      "Step 2500 accuracy: 86.5%\n",
      "Accuracy in test dataset: 92.11\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.318082\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.663732\n",
      "Step 500 accuracy: 80.7%\n",
      "Step 1000 loss: 0.577872\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.549304\n",
      "Step 1500 accuracy: 84.2%\n",
      "Step 2000 loss: 0.547282\n",
      "Step 2000 accuracy: 84.2%\n",
      "Step 2500 loss: 0.517694\n",
      "Step 2500 accuracy: 84.5%\n",
      "Accuracy in test dataset: 91.72\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.447300\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.725068\n",
      "Step 500 accuracy: 79.0%\n",
      "Step 1000 loss: 0.625155\n",
      "Step 1000 accuracy: 82.3%\n",
      "Step 1500 loss: 0.596919\n",
      "Step 1500 accuracy: 82.3%\n",
      "Step 2000 loss: 0.598851\n",
      "Step 2000 accuracy: 83.5%\n",
      "Step 2500 loss: 0.568666\n",
      "Step 2500 accuracy: 83.1%\n",
      "Accuracy in test dataset: 91.41\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.538939\n",
      "Step 0 accuracy: 11.2%\n",
      "Step 500 loss: 0.785917\n",
      "Step 500 accuracy: 78.4%\n",
      "Step 1000 loss: 0.703952\n",
      "Step 1000 accuracy: 80.0%\n",
      "Step 1500 loss: 0.665697\n",
      "Step 1500 accuracy: 81.4%\n",
      "Step 2000 loss: 0.646012\n",
      "Step 2000 accuracy: 82.6%\n",
      "Step 2500 loss: 0.628071\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 91.0\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.363321\n",
      "Step 0 accuracy: 6.5%\n",
      "Step 500 loss: 0.608192\n",
      "Step 500 accuracy: 82.5%\n",
      "Step 1000 loss: 0.526070\n",
      "Step 1000 accuracy: 84.2%\n",
      "Step 1500 loss: 0.499767\n",
      "Step 1500 accuracy: 85.5%\n",
      "Step 2000 loss: 0.485410\n",
      "Step 2000 accuracy: 86.3%\n",
      "Step 2500 loss: 0.453385\n",
      "Step 2500 accuracy: 87.1%\n",
      "Accuracy in test dataset: 92.24\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.384416\n",
      "Step 0 accuracy: 7.2%\n",
      "Step 500 loss: 0.621729\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.532763\n",
      "Step 1000 accuracy: 84.2%\n",
      "Step 1500 loss: 0.512946\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.494738\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.467621\n",
      "Step 2500 accuracy: 86.2%\n",
      "Accuracy in test dataset: 92.21\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.353761\n",
      "Step 0 accuracy: 10.4%\n",
      "Step 500 loss: 0.652100\n",
      "Step 500 accuracy: 81.5%\n",
      "Step 1000 loss: 0.556804\n",
      "Step 1000 accuracy: 83.3%\n",
      "Step 1500 loss: 0.539110\n",
      "Step 1500 accuracy: 84.6%\n",
      "Step 2000 loss: 0.531024\n",
      "Step 2000 accuracy: 85.1%\n",
      "Step 2500 loss: 0.495775\n",
      "Step 2500 accuracy: 85.4%\n",
      "Accuracy in test dataset: 92.13\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.459501\n",
      "Step 0 accuracy: 10.1%\n",
      "Step 500 loss: 0.695113\n",
      "Step 500 accuracy: 81.1%\n",
      "Step 1000 loss: 0.600485\n",
      "Step 1000 accuracy: 83.1%\n",
      "Step 1500 loss: 0.577618\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.562314\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.540051\n",
      "Step 2500 accuracy: 84.4%\n",
      "Accuracy in test dataset: 91.77\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 2560 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.574923\n",
      "Step 0 accuracy: 10.0%\n",
      "Step 500 loss: 0.742036\n",
      "Step 500 accuracy: 78.2%\n",
      "Step 1000 loss: 0.662735\n",
      "Step 1000 accuracy: 80.4%\n",
      "Step 1500 loss: 0.639252\n",
      "Step 1500 accuracy: 82.2%\n",
      "Step 2000 loss: 0.620438\n",
      "Step 2000 accuracy: 83.7%\n",
      "Step 2500 loss: 0.615517\n",
      "Step 2500 accuracy: 82.3%\n",
      "Accuracy in test dataset: 91.16\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.307804\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.620169\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.541799\n",
      "Step 1000 accuracy: 84.1%\n",
      "Step 1500 loss: 0.520849\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.512591\n",
      "Step 2000 accuracy: 85.6%\n",
      "Step 2500 loss: 0.475041\n",
      "Step 2500 accuracy: 86.1%\n",
      "Step 3000 loss: 0.452638\n",
      "Step 3000 accuracy: 86.2%\n",
      "Step 3500 loss: 0.427748\n",
      "Step 3500 accuracy: 87.7%\n",
      "Step 4000 loss: 0.481302\n",
      "Step 4000 accuracy: 86.1%\n",
      "Step 4500 loss: 0.429324\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.414076\n",
      "Step 5000 accuracy: 87.4%\n",
      "Accuracy in test dataset: 93.01\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.357451\n",
      "Step 0 accuracy: 9.1%\n",
      "Step 500 loss: 0.646724\n",
      "Step 500 accuracy: 81.7%\n",
      "Step 1000 loss: 0.560973\n",
      "Step 1000 accuracy: 83.5%\n",
      "Step 1500 loss: 0.539148\n",
      "Step 1500 accuracy: 84.6%\n",
      "Step 2000 loss: 0.537019\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.498475\n",
      "Step 2500 accuracy: 85.7%\n",
      "Step 3000 loss: 0.470096\n",
      "Step 3000 accuracy: 85.5%\n",
      "Step 3500 loss: 0.456362\n",
      "Step 3500 accuracy: 86.8%\n",
      "Step 4000 loss: 0.507654\n",
      "Step 4000 accuracy: 85.5%\n",
      "Step 4500 loss: 0.468698\n",
      "Step 4500 accuracy: 86.0%\n",
      "Step 5000 loss: 0.452128\n",
      "Step 5000 accuracy: 86.8%\n",
      "Accuracy in test dataset: 92.78\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.371461\n",
      "Step 0 accuracy: 11.5%\n",
      "Step 500 loss: 0.705345\n",
      "Step 500 accuracy: 80.7%\n",
      "Step 1000 loss: 0.624686\n",
      "Step 1000 accuracy: 81.3%\n",
      "Step 1500 loss: 0.603360\n",
      "Step 1500 accuracy: 82.2%\n",
      "Step 2000 loss: 0.586686\n",
      "Step 2000 accuracy: 84.2%\n",
      "Step 2500 loss: 0.572299\n",
      "Step 2500 accuracy: 83.2%\n",
      "Step 3000 loss: 0.544689\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.529815\n",
      "Step 3500 accuracy: 84.6%\n",
      "Step 4000 loss: 0.584955\n",
      "Step 4000 accuracy: 83.9%\n",
      "Step 4500 loss: 0.549728\n",
      "Step 4500 accuracy: 84.3%\n",
      "Step 5000 loss: 0.532779\n",
      "Step 5000 accuracy: 84.9%\n",
      "Accuracy in test dataset: 92.03\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.441831\n",
      "Step 0 accuracy: 9.2%\n",
      "Step 500 loss: 0.811060\n",
      "Step 500 accuracy: 76.8%\n",
      "Step 1000 loss: 0.720294\n",
      "Step 1000 accuracy: 78.4%\n",
      "Step 1500 loss: 0.675945\n",
      "Step 1500 accuracy: 80.6%\n",
      "Step 2000 loss: 0.677365\n",
      "Step 2000 accuracy: 81.7%\n",
      "Step 2500 loss: 0.645490\n",
      "Step 2500 accuracy: 81.6%\n",
      "Step 3000 loss: 0.624503\n",
      "Step 3000 accuracy: 82.1%\n",
      "Step 3500 loss: 0.592384\n",
      "Step 3500 accuracy: 83.4%\n",
      "Step 4000 loss: 0.654808\n",
      "Step 4000 accuracy: 82.3%\n",
      "Step 4500 loss: 0.631133\n",
      "Step 4500 accuracy: 83.0%\n",
      "Step 5000 loss: 0.608149\n",
      "Step 5000 accuracy: 81.9%\n",
      "Accuracy in test dataset: 91.46\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.596352\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.929866\n",
      "Step 500 accuracy: 73.2%\n",
      "Step 1000 loss: 0.856098\n",
      "Step 1000 accuracy: 76.1%\n",
      "Step 1500 loss: 0.801832\n",
      "Step 1500 accuracy: 77.2%\n",
      "Step 2000 loss: 0.810432\n",
      "Step 2000 accuracy: 77.8%\n",
      "Step 2500 loss: 0.756902\n",
      "Step 2500 accuracy: 78.4%\n",
      "Step 3000 loss: 0.775732\n",
      "Step 3000 accuracy: 77.1%\n",
      "Step 3500 loss: 0.743388\n",
      "Step 3500 accuracy: 77.9%\n",
      "Step 4000 loss: 0.759065\n",
      "Step 4000 accuracy: 79.8%\n",
      "Step 4500 loss: 0.753979\n",
      "Step 4500 accuracy: 77.8%\n",
      "Step 5000 loss: 0.738785\n",
      "Step 5000 accuracy: 78.5%\n",
      "Accuracy in test dataset: 90.53\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.287391\n",
      "Step 0 accuracy: 14.2%\n",
      "Step 500 loss: 0.618676\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.540471\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.516593\n",
      "Step 1500 accuracy: 85.3%\n",
      "Step 2000 loss: 0.505465\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.471643\n",
      "Step 2500 accuracy: 85.9%\n",
      "Step 3000 loss: 0.439831\n",
      "Step 3000 accuracy: 87.2%\n",
      "Step 3500 loss: 0.421626\n",
      "Step 3500 accuracy: 87.3%\n",
      "Step 4000 loss: 0.467566\n",
      "Step 4000 accuracy: 86.5%\n",
      "Step 4500 loss: 0.423703\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.403830\n",
      "Step 5000 accuracy: 88.2%\n",
      "Accuracy in test dataset: 93.38\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.319055\n",
      "Step 0 accuracy: 11.9%\n",
      "Step 500 loss: 0.632712\n",
      "Step 500 accuracy: 81.8%\n",
      "Step 1000 loss: 0.552237\n",
      "Step 1000 accuracy: 83.5%\n",
      "Step 1500 loss: 0.533483\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.527355\n",
      "Step 2000 accuracy: 85.7%\n",
      "Step 2500 loss: 0.484642\n",
      "Step 2500 accuracy: 86.2%\n",
      "Step 3000 loss: 0.460669\n",
      "Step 3000 accuracy: 86.0%\n",
      "Step 3500 loss: 0.449376\n",
      "Step 3500 accuracy: 86.8%\n",
      "Step 4000 loss: 0.496121\n",
      "Step 4000 accuracy: 85.8%\n",
      "Step 4500 loss: 0.450549\n",
      "Step 4500 accuracy: 87.2%\n",
      "Step 5000 loss: 0.429668\n",
      "Step 5000 accuracy: 87.1%\n",
      "Accuracy in test dataset: 93.22\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.374455\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.691097\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.588845\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.574450\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.559869\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.523078\n",
      "Step 2500 accuracy: 84.7%\n",
      "Step 3000 loss: 0.522165\n",
      "Step 3000 accuracy: 84.6%\n",
      "Step 3500 loss: 0.486347\n",
      "Step 3500 accuracy: 85.8%\n",
      "Step 4000 loss: 0.550724\n",
      "Step 4000 accuracy: 84.9%\n",
      "Step 4500 loss: 0.511393\n",
      "Step 4500 accuracy: 85.4%\n",
      "Step 5000 loss: 0.491988\n",
      "Step 5000 accuracy: 85.8%\n",
      "Accuracy in test dataset: 92.44\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.469820\n",
      "Step 0 accuracy: 9.1%\n",
      "Step 500 loss: 0.754989\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.661537\n",
      "Step 1000 accuracy: 80.2%\n",
      "Step 1500 loss: 0.636929\n",
      "Step 1500 accuracy: 81.8%\n",
      "Step 2000 loss: 0.617741\n",
      "Step 2000 accuracy: 82.7%\n",
      "Step 2500 loss: 0.593920\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.566410\n",
      "Step 3000 accuracy: 83.5%\n",
      "Step 3500 loss: 0.556170\n",
      "Step 3500 accuracy: 84.2%\n",
      "Step 4000 loss: 0.619393\n",
      "Step 4000 accuracy: 82.6%\n",
      "Step 4500 loss: 0.579786\n",
      "Step 4500 accuracy: 83.5%\n",
      "Step 5000 loss: 0.556184\n",
      "Step 5000 accuracy: 84.3%\n",
      "Accuracy in test dataset: 91.95\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.511357\n",
      "Step 0 accuracy: 11.6%\n",
      "Step 500 loss: 0.856070\n",
      "Step 500 accuracy: 76.4%\n",
      "Step 1000 loss: 0.768908\n",
      "Step 1000 accuracy: 78.2%\n",
      "Step 1500 loss: 0.724047\n",
      "Step 1500 accuracy: 80.3%\n",
      "Step 2000 loss: 0.709273\n",
      "Step 2000 accuracy: 80.8%\n",
      "Step 2500 loss: 0.656286\n",
      "Step 2500 accuracy: 81.3%\n",
      "Step 3000 loss: 0.679414\n",
      "Step 3000 accuracy: 81.3%\n",
      "Step 3500 loss: 0.652232\n",
      "Step 3500 accuracy: 82.3%\n",
      "Step 4000 loss: 0.710490\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.658927\n",
      "Step 4500 accuracy: 82.5%\n",
      "Step 5000 loss: 0.645252\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 91.16\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.293029\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.613674\n",
      "Step 500 accuracy: 82.6%\n",
      "Step 1000 loss: 0.530631\n",
      "Step 1000 accuracy: 84.2%\n",
      "Step 1500 loss: 0.508888\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.495644\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.462369\n",
      "Step 2500 accuracy: 86.9%\n",
      "Step 3000 loss: 0.434727\n",
      "Step 3000 accuracy: 87.2%\n",
      "Step 3500 loss: 0.412841\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.463873\n",
      "Step 4000 accuracy: 86.4%\n",
      "Step 4500 loss: 0.408601\n",
      "Step 4500 accuracy: 89.1%\n",
      "Step 5000 loss: 0.396379\n",
      "Step 5000 accuracy: 88.4%\n",
      "Accuracy in test dataset: 93.43\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.359298\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.629799\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.544546\n",
      "Step 1000 accuracy: 83.9%\n",
      "Step 1500 loss: 0.519905\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.507169\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.484724\n",
      "Step 2500 accuracy: 85.8%\n",
      "Step 3000 loss: 0.455540\n",
      "Step 3000 accuracy: 86.4%\n",
      "Step 3500 loss: 0.429516\n",
      "Step 3500 accuracy: 87.1%\n",
      "Step 4000 loss: 0.480103\n",
      "Step 4000 accuracy: 86.3%\n",
      "Step 4500 loss: 0.431397\n",
      "Step 4500 accuracy: 87.8%\n",
      "Step 5000 loss: 0.417465\n",
      "Step 5000 accuracy: 87.7%\n",
      "Accuracy in test dataset: 93.48\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.334162\n",
      "Step 0 accuracy: 11.8%\n",
      "Step 500 loss: 0.674714\n",
      "Step 500 accuracy: 80.7%\n",
      "Step 1000 loss: 0.584044\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.556301\n",
      "Step 1500 accuracy: 84.2%\n",
      "Step 2000 loss: 0.558619\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.513930\n",
      "Step 2500 accuracy: 85.1%\n",
      "Step 3000 loss: 0.489773\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.482530\n",
      "Step 3500 accuracy: 85.7%\n",
      "Step 4000 loss: 0.535593\n",
      "Step 4000 accuracy: 84.8%\n",
      "Step 4500 loss: 0.478702\n",
      "Step 4500 accuracy: 86.0%\n",
      "Step 5000 loss: 0.459305\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 92.86\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.431809\n",
      "Step 0 accuracy: 11.3%\n",
      "Step 500 loss: 0.714399\n",
      "Step 500 accuracy: 79.0%\n",
      "Step 1000 loss: 0.624500\n",
      "Step 1000 accuracy: 81.4%\n",
      "Step 1500 loss: 0.604633\n",
      "Step 1500 accuracy: 81.8%\n",
      "Step 2000 loss: 0.591497\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.573845\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.543462\n",
      "Step 3000 accuracy: 83.9%\n",
      "Step 3500 loss: 0.522618\n",
      "Step 3500 accuracy: 84.9%\n",
      "Step 4000 loss: 0.574081\n",
      "Step 4000 accuracy: 83.2%\n",
      "Step 4500 loss: 0.539923\n",
      "Step 4500 accuracy: 85.1%\n",
      "Step 5000 loss: 0.510486\n",
      "Step 5000 accuracy: 84.7%\n",
      "Accuracy in test dataset: 92.43\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.576302\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.788498\n",
      "Step 500 accuracy: 77.8%\n",
      "Step 1000 loss: 0.699246\n",
      "Step 1000 accuracy: 80.4%\n",
      "Step 1500 loss: 0.664345\n",
      "Step 1500 accuracy: 81.3%\n",
      "Step 2000 loss: 0.658732\n",
      "Step 2000 accuracy: 81.3%\n",
      "Step 2500 loss: 0.621417\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.609992\n",
      "Step 3000 accuracy: 81.8%\n",
      "Step 3500 loss: 0.591246\n",
      "Step 3500 accuracy: 83.0%\n",
      "Step 4000 loss: 0.664220\n",
      "Step 4000 accuracy: 81.7%\n",
      "Step 4500 loss: 0.598997\n",
      "Step 4500 accuracy: 82.6%\n",
      "Step 5000 loss: 0.580801\n",
      "Step 5000 accuracy: 83.9%\n",
      "Accuracy in test dataset: 91.73\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.332551\n",
      "Step 0 accuracy: 7.6%\n",
      "Step 500 loss: 0.605079\n",
      "Step 500 accuracy: 82.6%\n",
      "Step 1000 loss: 0.522704\n",
      "Step 1000 accuracy: 84.3%\n",
      "Step 1500 loss: 0.500018\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.484480\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.453421\n",
      "Step 2500 accuracy: 86.8%\n",
      "Step 3000 loss: 0.427547\n",
      "Step 3000 accuracy: 87.8%\n",
      "Step 3500 loss: 0.402261\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.453206\n",
      "Step 4000 accuracy: 87.1%\n",
      "Step 4500 loss: 0.403045\n",
      "Step 4500 accuracy: 89.0%\n",
      "Step 5000 loss: 0.386196\n",
      "Step 5000 accuracy: 88.8%\n",
      "Accuracy in test dataset: 93.63\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.337646\n",
      "Step 0 accuracy: 9.6%\n",
      "Step 500 loss: 0.613125\n",
      "Step 500 accuracy: 82.7%\n",
      "Step 1000 loss: 0.530136\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.507232\n",
      "Step 1500 accuracy: 85.7%\n",
      "Step 2000 loss: 0.498971\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.468870\n",
      "Step 2500 accuracy: 86.3%\n",
      "Step 3000 loss: 0.438321\n",
      "Step 3000 accuracy: 87.4%\n",
      "Step 3500 loss: 0.416826\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.471663\n",
      "Step 4000 accuracy: 86.0%\n",
      "Step 4500 loss: 0.418466\n",
      "Step 4500 accuracy: 88.1%\n",
      "Step 5000 loss: 0.403656\n",
      "Step 5000 accuracy: 88.0%\n",
      "Accuracy in test dataset: 93.48\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.343080\n",
      "Step 0 accuracy: 10.6%\n",
      "Step 500 loss: 0.653356\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.568615\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.533981\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.533307\n",
      "Step 2000 accuracy: 84.6%\n",
      "Step 2500 loss: 0.490521\n",
      "Step 2500 accuracy: 85.7%\n",
      "Step 3000 loss: 0.482170\n",
      "Step 3000 accuracy: 86.1%\n",
      "Step 3500 loss: 0.452878\n",
      "Step 3500 accuracy: 87.1%\n",
      "Step 4000 loss: 0.508042\n",
      "Step 4000 accuracy: 85.4%\n",
      "Step 4500 loss: 0.458931\n",
      "Step 4500 accuracy: 87.2%\n",
      "Step 5000 loss: 0.449982\n",
      "Step 5000 accuracy: 86.8%\n",
      "Accuracy in test dataset: 93.31\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.400204\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.683030\n",
      "Step 500 accuracy: 81.0%\n",
      "Step 1000 loss: 0.606357\n",
      "Step 1000 accuracy: 82.5%\n",
      "Step 1500 loss: 0.578520\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.572710\n",
      "Step 2000 accuracy: 83.7%\n",
      "Step 2500 loss: 0.542190\n",
      "Step 2500 accuracy: 84.7%\n",
      "Step 3000 loss: 0.509175\n",
      "Step 3000 accuracy: 85.1%\n",
      "Step 3500 loss: 0.498825\n",
      "Step 3500 accuracy: 85.8%\n",
      "Step 4000 loss: 0.565196\n",
      "Step 4000 accuracy: 83.8%\n",
      "Step 4500 loss: 0.500793\n",
      "Step 4500 accuracy: 85.8%\n",
      "Step 5000 loss: 0.481225\n",
      "Step 5000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 92.5\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 5120 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.560725\n",
      "Step 0 accuracy: 10.7%\n",
      "Step 500 loss: 0.756959\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.663541\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.639658\n",
      "Step 1500 accuracy: 82.1%\n",
      "Step 2000 loss: 0.618603\n",
      "Step 2000 accuracy: 83.4%\n",
      "Step 2500 loss: 0.601443\n",
      "Step 2500 accuracy: 82.7%\n",
      "Step 3000 loss: 0.573852\n",
      "Step 3000 accuracy: 83.2%\n",
      "Step 3500 loss: 0.551920\n",
      "Step 3500 accuracy: 84.8%\n",
      "Step 4000 loss: 0.617213\n",
      "Step 4000 accuracy: 82.3%\n",
      "Step 4500 loss: 0.573614\n",
      "Step 4500 accuracy: 83.7%\n",
      "Step 5000 loss: 0.539591\n",
      "Step 5000 accuracy: 84.1%\n",
      "Accuracy in test dataset: 92.02\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.348056\n",
      "Step 0 accuracy: 7.9%\n",
      "Step 500 loss: 0.624897\n",
      "Step 500 accuracy: 82.1%\n",
      "Step 1000 loss: 0.541998\n",
      "Step 1000 accuracy: 83.8%\n",
      "Step 1500 loss: 0.517962\n",
      "Step 1500 accuracy: 85.1%\n",
      "Step 2000 loss: 0.512082\n",
      "Step 2000 accuracy: 85.4%\n",
      "Step 2500 loss: 0.475802\n",
      "Step 2500 accuracy: 86.2%\n",
      "Step 3000 loss: 0.455788\n",
      "Step 3000 accuracy: 86.2%\n",
      "Step 3500 loss: 0.431736\n",
      "Step 3500 accuracy: 87.3%\n",
      "Step 4000 loss: 0.479049\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.439368\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.418333\n",
      "Step 5000 accuracy: 87.7%\n",
      "Step 5500 loss: 0.433423\n",
      "Step 5500 accuracy: 86.8%\n",
      "Step 6000 loss: 0.429359\n",
      "Step 6000 accuracy: 87.4%\n",
      "Step 6500 loss: 0.391128\n",
      "Step 6500 accuracy: 88.9%\n",
      "Step 7000 loss: 0.365626\n",
      "Step 7000 accuracy: 89.0%\n",
      "Step 7500 loss: 0.396017\n",
      "Step 7500 accuracy: 89.2%\n",
      "Step 8000 loss: 0.352632\n",
      "Step 8000 accuracy: 89.3%\n",
      "Step 8500 loss: 0.386610\n",
      "Step 8500 accuracy: 88.9%\n",
      "Step 9000 loss: 0.364513\n",
      "Step 9000 accuracy: 90.4%\n",
      "Step 9500 loss: 0.352137\n",
      "Step 9500 accuracy: 89.6%\n",
      "Step 10000 loss: 0.372886\n",
      "Step 10000 accuracy: 89.6%\n",
      "Accuracy in test dataset: 94.45\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.320403\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.647771\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.564959\n",
      "Step 1000 accuracy: 83.1%\n",
      "Step 1500 loss: 0.546776\n",
      "Step 1500 accuracy: 83.8%\n",
      "Step 2000 loss: 0.525474\n",
      "Step 2000 accuracy: 85.6%\n",
      "Step 2500 loss: 0.496708\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.472717\n",
      "Step 3000 accuracy: 85.4%\n",
      "Step 3500 loss: 0.453741\n",
      "Step 3500 accuracy: 86.5%\n",
      "Step 4000 loss: 0.513448\n",
      "Step 4000 accuracy: 85.1%\n",
      "Step 4500 loss: 0.471176\n",
      "Step 4500 accuracy: 86.0%\n",
      "Step 5000 loss: 0.452056\n",
      "Step 5000 accuracy: 86.6%\n",
      "Step 5500 loss: 0.455718\n",
      "Step 5500 accuracy: 86.3%\n",
      "Step 6000 loss: 0.467599\n",
      "Step 6000 accuracy: 86.8%\n",
      "Step 6500 loss: 0.428359\n",
      "Step 6500 accuracy: 87.7%\n",
      "Step 7000 loss: 0.403671\n",
      "Step 7000 accuracy: 87.9%\n",
      "Step 7500 loss: 0.444511\n",
      "Step 7500 accuracy: 87.1%\n",
      "Step 8000 loss: 0.381444\n",
      "Step 8000 accuracy: 88.8%\n",
      "Step 8500 loss: 0.435760\n",
      "Step 8500 accuracy: 87.1%\n",
      "Step 9000 loss: 0.400146\n",
      "Step 9000 accuracy: 89.5%\n",
      "Step 9500 loss: 0.386605\n",
      "Step 9500 accuracy: 88.3%\n",
      "Step 10000 loss: 0.418825\n",
      "Step 10000 accuracy: 88.3%\n",
      "Accuracy in test dataset: 94.02\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.369541\n",
      "Step 0 accuracy: 10.0%\n",
      "Step 500 loss: 0.700797\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.620011\n",
      "Step 1000 accuracy: 82.1%\n",
      "Step 1500 loss: 0.607063\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.588740\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.559694\n",
      "Step 2500 accuracy: 84.2%\n",
      "Step 3000 loss: 0.531202\n",
      "Step 3000 accuracy: 84.1%\n",
      "Step 3500 loss: 0.522801\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.577603\n",
      "Step 4000 accuracy: 82.9%\n",
      "Step 4500 loss: 0.539201\n",
      "Step 4500 accuracy: 84.9%\n",
      "Step 5000 loss: 0.511984\n",
      "Step 5000 accuracy: 85.1%\n",
      "Step 5500 loss: 0.548698\n",
      "Step 5500 accuracy: 84.6%\n",
      "Step 6000 loss: 0.517697\n",
      "Step 6000 accuracy: 85.0%\n",
      "Step 6500 loss: 0.498974\n",
      "Step 6500 accuracy: 85.8%\n",
      "Step 7000 loss: 0.483060\n",
      "Step 7000 accuracy: 85.9%\n",
      "Step 7500 loss: 0.511514\n",
      "Step 7500 accuracy: 85.2%\n",
      "Step 8000 loss: 0.449500\n",
      "Step 8000 accuracy: 87.1%\n",
      "Step 8500 loss: 0.519932\n",
      "Step 8500 accuracy: 85.3%\n",
      "Step 9000 loss: 0.488346\n",
      "Step 9000 accuracy: 86.6%\n",
      "Step 9500 loss: 0.475640\n",
      "Step 9500 accuracy: 86.8%\n",
      "Step 10000 loss: 0.515754\n",
      "Step 10000 accuracy: 85.7%\n",
      "Accuracy in test dataset: 93.21\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.402232\n",
      "Step 0 accuracy: 11.4%\n",
      "Step 500 loss: 0.785561\n",
      "Step 500 accuracy: 77.8%\n",
      "Step 1000 loss: 0.717917\n",
      "Step 1000 accuracy: 79.5%\n",
      "Step 1500 loss: 0.668197\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.671730\n",
      "Step 2000 accuracy: 81.5%\n",
      "Step 2500 loss: 0.629285\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.635884\n",
      "Step 3000 accuracy: 81.8%\n",
      "Step 3500 loss: 0.598061\n",
      "Step 3500 accuracy: 83.2%\n",
      "Step 4000 loss: 0.666455\n",
      "Step 4000 accuracy: 81.3%\n",
      "Step 4500 loss: 0.625304\n",
      "Step 4500 accuracy: 82.6%\n",
      "Step 5000 loss: 0.600303\n",
      "Step 5000 accuracy: 83.2%\n",
      "Step 5500 loss: 0.616965\n",
      "Step 5500 accuracy: 83.1%\n",
      "Step 6000 loss: 0.630779\n",
      "Step 6000 accuracy: 82.3%\n",
      "Step 6500 loss: 0.590417\n",
      "Step 6500 accuracy: 82.3%\n",
      "Step 7000 loss: 0.591599\n",
      "Step 7000 accuracy: 83.6%\n",
      "Step 7500 loss: 0.609129\n",
      "Step 7500 accuracy: 83.6%\n",
      "Step 8000 loss: 0.550444\n",
      "Step 8000 accuracy: 84.0%\n",
      "Step 8500 loss: 0.607818\n",
      "Step 8500 accuracy: 82.0%\n",
      "Step 9000 loss: 0.580192\n",
      "Step 9000 accuracy: 83.9%\n",
      "Step 9500 loss: 0.563913\n",
      "Step 9500 accuracy: 84.6%\n",
      "Step 10000 loss: 0.620569\n",
      "Step 10000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 92.16\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.567898\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.955714\n",
      "Step 500 accuracy: 73.6%\n",
      "Step 1000 loss: 0.833009\n",
      "Step 1000 accuracy: 76.6%\n",
      "Step 1500 loss: 0.821050\n",
      "Step 1500 accuracy: 76.8%\n",
      "Step 2000 loss: 0.772126\n",
      "Step 2000 accuracy: 78.6%\n",
      "Step 2500 loss: 0.765783\n",
      "Step 2500 accuracy: 78.2%\n",
      "Step 3000 loss: 0.759872\n",
      "Step 3000 accuracy: 78.6%\n",
      "Step 3500 loss: 0.732282\n",
      "Step 3500 accuracy: 79.0%\n",
      "Step 4000 loss: 0.779972\n",
      "Step 4000 accuracy: 77.8%\n",
      "Step 4500 loss: 0.775418\n",
      "Step 4500 accuracy: 78.6%\n",
      "Step 5000 loss: 0.736376\n",
      "Step 5000 accuracy: 79.4%\n",
      "Step 5500 loss: 0.757014\n",
      "Step 5500 accuracy: 79.0%\n",
      "Step 6000 loss: 0.764345\n",
      "Step 6000 accuracy: 78.8%\n",
      "Step 6500 loss: 0.719548\n",
      "Step 6500 accuracy: 78.2%\n",
      "Step 7000 loss: 0.721847\n",
      "Step 7000 accuracy: 79.2%\n",
      "Step 7500 loss: 0.709259\n",
      "Step 7500 accuracy: 79.4%\n",
      "Step 8000 loss: 0.666054\n",
      "Step 8000 accuracy: 81.1%\n",
      "Step 8500 loss: 0.757064\n",
      "Step 8500 accuracy: 77.8%\n",
      "Step 9000 loss: 0.703150\n",
      "Step 9000 accuracy: 80.3%\n",
      "Step 9500 loss: 0.717897\n",
      "Step 9500 accuracy: 80.5%\n",
      "Step 10000 loss: 0.762915\n",
      "Step 10000 accuracy: 77.3%\n",
      "Accuracy in test dataset: 91.11\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.343492\n",
      "Step 0 accuracy: 10.1%\n",
      "Step 500 loss: 0.618548\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.535704\n",
      "Step 1000 accuracy: 84.3%\n",
      "Step 1500 loss: 0.519457\n",
      "Step 1500 accuracy: 84.9%\n",
      "Step 2000 loss: 0.504905\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.470027\n",
      "Step 2500 accuracy: 86.1%\n",
      "Step 3000 loss: 0.448307\n",
      "Step 3000 accuracy: 86.2%\n",
      "Step 3500 loss: 0.423678\n",
      "Step 3500 accuracy: 87.7%\n",
      "Step 4000 loss: 0.468107\n",
      "Step 4000 accuracy: 86.2%\n",
      "Step 4500 loss: 0.424106\n",
      "Step 4500 accuracy: 88.6%\n",
      "Step 5000 loss: 0.405433\n",
      "Step 5000 accuracy: 88.2%\n",
      "Step 5500 loss: 0.425479\n",
      "Step 5500 accuracy: 87.1%\n",
      "Step 6000 loss: 0.414009\n",
      "Step 6000 accuracy: 87.8%\n",
      "Step 6500 loss: 0.388110\n",
      "Step 6500 accuracy: 88.9%\n",
      "Step 7000 loss: 0.352887\n",
      "Step 7000 accuracy: 89.5%\n",
      "Step 7500 loss: 0.385230\n",
      "Step 7500 accuracy: 88.8%\n",
      "Step 8000 loss: 0.334998\n",
      "Step 8000 accuracy: 90.0%\n",
      "Step 8500 loss: 0.380569\n",
      "Step 8500 accuracy: 88.7%\n",
      "Step 9000 loss: 0.349324\n",
      "Step 9000 accuracy: 90.5%\n",
      "Step 9500 loss: 0.333694\n",
      "Step 9500 accuracy: 90.2%\n",
      "Step 10000 loss: 0.356960\n",
      "Step 10000 accuracy: 90.2%\n",
      "Accuracy in test dataset: 94.67\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.311922\n",
      "Step 0 accuracy: 13.8%\n",
      "Step 500 loss: 0.638077\n",
      "Step 500 accuracy: 81.7%\n",
      "Step 1000 loss: 0.555774\n",
      "Step 1000 accuracy: 83.0%\n",
      "Step 1500 loss: 0.535612\n",
      "Step 1500 accuracy: 84.2%\n",
      "Step 2000 loss: 0.530832\n",
      "Step 2000 accuracy: 84.6%\n",
      "Step 2500 loss: 0.496957\n",
      "Step 2500 accuracy: 85.8%\n",
      "Step 3000 loss: 0.470634\n",
      "Step 3000 accuracy: 85.4%\n",
      "Step 3500 loss: 0.447662\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.487746\n",
      "Step 4000 accuracy: 85.7%\n",
      "Step 4500 loss: 0.455848\n",
      "Step 4500 accuracy: 86.8%\n",
      "Step 5000 loss: 0.429595\n",
      "Step 5000 accuracy: 87.8%\n",
      "Step 5500 loss: 0.453562\n",
      "Step 5500 accuracy: 86.9%\n",
      "Step 6000 loss: 0.441870\n",
      "Step 6000 accuracy: 87.3%\n",
      "Step 6500 loss: 0.415313\n",
      "Step 6500 accuracy: 88.5%\n",
      "Step 7000 loss: 0.390817\n",
      "Step 7000 accuracy: 88.7%\n",
      "Step 7500 loss: 0.417998\n",
      "Step 7500 accuracy: 87.8%\n",
      "Step 8000 loss: 0.361461\n",
      "Step 8000 accuracy: 89.1%\n",
      "Step 8500 loss: 0.417182\n",
      "Step 8500 accuracy: 87.4%\n",
      "Step 9000 loss: 0.379565\n",
      "Step 9000 accuracy: 89.5%\n",
      "Step 9500 loss: 0.379799\n",
      "Step 9500 accuracy: 88.7%\n",
      "Step 10000 loss: 0.393804\n",
      "Step 10000 accuracy: 88.2%\n",
      "Accuracy in test dataset: 94.44\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.368894\n",
      "Step 0 accuracy: 10.3%\n",
      "Step 500 loss: 0.683433\n",
      "Step 500 accuracy: 80.2%\n",
      "Step 1000 loss: 0.591647\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.568950\n",
      "Step 1500 accuracy: 83.4%\n",
      "Step 2000 loss: 0.575844\n",
      "Step 2000 accuracy: 83.9%\n",
      "Step 2500 loss: 0.536642\n",
      "Step 2500 accuracy: 84.3%\n",
      "Step 3000 loss: 0.517911\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.500674\n",
      "Step 3500 accuracy: 85.7%\n",
      "Step 4000 loss: 0.550612\n",
      "Step 4000 accuracy: 84.6%\n",
      "Step 4500 loss: 0.507700\n",
      "Step 4500 accuracy: 85.8%\n",
      "Step 5000 loss: 0.487349\n",
      "Step 5000 accuracy: 85.8%\n",
      "Step 5500 loss: 0.514818\n",
      "Step 5500 accuracy: 84.9%\n",
      "Step 6000 loss: 0.498152\n",
      "Step 6000 accuracy: 85.7%\n",
      "Step 6500 loss: 0.470929\n",
      "Step 6500 accuracy: 85.8%\n",
      "Step 7000 loss: 0.458735\n",
      "Step 7000 accuracy: 86.0%\n",
      "Step 7500 loss: 0.475641\n",
      "Step 7500 accuracy: 86.0%\n",
      "Step 8000 loss: 0.439274\n",
      "Step 8000 accuracy: 87.3%\n",
      "Step 8500 loss: 0.484299\n",
      "Step 8500 accuracy: 86.1%\n",
      "Step 9000 loss: 0.454423\n",
      "Step 9000 accuracy: 87.7%\n",
      "Step 9500 loss: 0.442310\n",
      "Step 9500 accuracy: 87.8%\n",
      "Step 10000 loss: 0.471501\n",
      "Step 10000 accuracy: 87.2%\n",
      "Accuracy in test dataset: 93.77\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.448518\n",
      "Step 0 accuracy: 11.2%\n",
      "Step 500 loss: 0.750571\n",
      "Step 500 accuracy: 78.2%\n",
      "Step 1000 loss: 0.672734\n",
      "Step 1000 accuracy: 79.8%\n",
      "Step 1500 loss: 0.638995\n",
      "Step 1500 accuracy: 81.8%\n",
      "Step 2000 loss: 0.622779\n",
      "Step 2000 accuracy: 82.9%\n",
      "Step 2500 loss: 0.591744\n",
      "Step 2500 accuracy: 83.0%\n",
      "Step 3000 loss: 0.590407\n",
      "Step 3000 accuracy: 83.2%\n",
      "Step 3500 loss: 0.555066\n",
      "Step 3500 accuracy: 84.7%\n",
      "Step 4000 loss: 0.616086\n",
      "Step 4000 accuracy: 82.9%\n",
      "Step 4500 loss: 0.580781\n",
      "Step 4500 accuracy: 83.8%\n",
      "Step 5000 loss: 0.563529\n",
      "Step 5000 accuracy: 84.6%\n",
      "Step 5500 loss: 0.562822\n",
      "Step 5500 accuracy: 83.4%\n",
      "Step 6000 loss: 0.564117\n",
      "Step 6000 accuracy: 84.6%\n",
      "Step 6500 loss: 0.538670\n",
      "Step 6500 accuracy: 83.5%\n",
      "Step 7000 loss: 0.514323\n",
      "Step 7000 accuracy: 84.8%\n",
      "Step 7500 loss: 0.542719\n",
      "Step 7500 accuracy: 84.3%\n",
      "Step 8000 loss: 0.498503\n",
      "Step 8000 accuracy: 85.1%\n",
      "Step 8500 loss: 0.573896\n",
      "Step 8500 accuracy: 83.5%\n",
      "Step 9000 loss: 0.509034\n",
      "Step 9000 accuracy: 85.2%\n",
      "Step 9500 loss: 0.517974\n",
      "Step 9500 accuracy: 85.3%\n",
      "Step 10000 loss: 0.549424\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.69\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.623722\n",
      "Step 0 accuracy: 8.7%\n",
      "Step 500 loss: 0.843257\n",
      "Step 500 accuracy: 75.8%\n",
      "Step 1000 loss: 0.738111\n",
      "Step 1000 accuracy: 78.3%\n",
      "Step 1500 loss: 0.695635\n",
      "Step 1500 accuracy: 80.2%\n",
      "Step 2000 loss: 0.683174\n",
      "Step 2000 accuracy: 80.7%\n",
      "Step 2500 loss: 0.681858\n",
      "Step 2500 accuracy: 80.4%\n",
      "Step 3000 loss: 0.652144\n",
      "Step 3000 accuracy: 81.7%\n",
      "Step 3500 loss: 0.635261\n",
      "Step 3500 accuracy: 82.3%\n",
      "Step 4000 loss: 0.691886\n",
      "Step 4000 accuracy: 81.0%\n",
      "Step 4500 loss: 0.646288\n",
      "Step 4500 accuracy: 82.3%\n",
      "Step 5000 loss: 0.641057\n",
      "Step 5000 accuracy: 82.5%\n",
      "Step 5500 loss: 0.638979\n",
      "Step 5500 accuracy: 82.1%\n",
      "Step 6000 loss: 0.632565\n",
      "Step 6000 accuracy: 82.5%\n",
      "Step 6500 loss: 0.603595\n",
      "Step 6500 accuracy: 81.8%\n",
      "Step 7000 loss: 0.615377\n",
      "Step 7000 accuracy: 82.3%\n",
      "Step 7500 loss: 0.624187\n",
      "Step 7500 accuracy: 82.6%\n",
      "Step 8000 loss: 0.576074\n",
      "Step 8000 accuracy: 83.6%\n",
      "Step 8500 loss: 0.657512\n",
      "Step 8500 accuracy: 80.8%\n",
      "Step 9000 loss: 0.608968\n",
      "Step 9000 accuracy: 83.1%\n",
      "Step 9500 loss: 0.612515\n",
      "Step 9500 accuracy: 83.0%\n",
      "Step 10000 loss: 0.653055\n",
      "Step 10000 accuracy: 82.3%\n",
      "Accuracy in test dataset: 91.9\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.323150\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.611809\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.527810\n",
      "Step 1000 accuracy: 84.5%\n",
      "Step 1500 loss: 0.511585\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.499212\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.465019\n",
      "Step 2500 accuracy: 86.5%\n",
      "Step 3000 loss: 0.437576\n",
      "Step 3000 accuracy: 87.2%\n",
      "Step 3500 loss: 0.414053\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.463097\n",
      "Step 4000 accuracy: 86.4%\n",
      "Step 4500 loss: 0.416263\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.397160\n",
      "Step 5000 accuracy: 88.8%\n",
      "Step 5500 loss: 0.413907\n",
      "Step 5500 accuracy: 87.2%\n",
      "Step 6000 loss: 0.400157\n",
      "Step 6000 accuracy: 88.3%\n",
      "Step 6500 loss: 0.376395\n",
      "Step 6500 accuracy: 89.1%\n",
      "Step 7000 loss: 0.346358\n",
      "Step 7000 accuracy: 89.6%\n",
      "Step 7500 loss: 0.373478\n",
      "Step 7500 accuracy: 89.3%\n",
      "Step 8000 loss: 0.331639\n",
      "Step 8000 accuracy: 90.1%\n",
      "Step 8500 loss: 0.372851\n",
      "Step 8500 accuracy: 89.5%\n",
      "Step 9000 loss: 0.339639\n",
      "Step 9000 accuracy: 90.9%\n",
      "Step 9500 loss: 0.322530\n",
      "Step 9500 accuracy: 90.2%\n",
      "Step 10000 loss: 0.348335\n",
      "Step 10000 accuracy: 89.9%\n",
      "Accuracy in test dataset: 94.72\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.331317\n",
      "Step 0 accuracy: 11.4%\n",
      "Step 500 loss: 0.628997\n",
      "Step 500 accuracy: 82.2%\n",
      "Step 1000 loss: 0.540345\n",
      "Step 1000 accuracy: 84.3%\n",
      "Step 1500 loss: 0.519253\n",
      "Step 1500 accuracy: 84.9%\n",
      "Step 2000 loss: 0.506422\n",
      "Step 2000 accuracy: 85.8%\n",
      "Step 2500 loss: 0.478845\n",
      "Step 2500 accuracy: 86.0%\n",
      "Step 3000 loss: 0.449878\n",
      "Step 3000 accuracy: 86.6%\n",
      "Step 3500 loss: 0.424358\n",
      "Step 3500 accuracy: 87.7%\n",
      "Step 4000 loss: 0.474168\n",
      "Step 4000 accuracy: 85.8%\n",
      "Step 4500 loss: 0.435294\n",
      "Step 4500 accuracy: 88.1%\n",
      "Step 5000 loss: 0.412995\n",
      "Step 5000 accuracy: 88.2%\n",
      "Step 5500 loss: 0.432068\n",
      "Step 5500 accuracy: 87.2%\n",
      "Step 6000 loss: 0.424375\n",
      "Step 6000 accuracy: 87.6%\n",
      "Step 6500 loss: 0.400469\n",
      "Step 6500 accuracy: 88.6%\n",
      "Step 7000 loss: 0.365284\n",
      "Step 7000 accuracy: 88.9%\n",
      "Step 7500 loss: 0.396876\n",
      "Step 7500 accuracy: 88.7%\n",
      "Step 8000 loss: 0.349036\n",
      "Step 8000 accuracy: 89.6%\n",
      "Step 8500 loss: 0.391557\n",
      "Step 8500 accuracy: 88.7%\n",
      "Step 9000 loss: 0.372694\n",
      "Step 9000 accuracy: 90.2%\n",
      "Step 9500 loss: 0.350448\n",
      "Step 9500 accuracy: 89.9%\n",
      "Step 10000 loss: 0.371816\n",
      "Step 10000 accuracy: 89.4%\n",
      "Accuracy in test dataset: 94.77\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.355793\n",
      "Step 0 accuracy: 9.7%\n",
      "Step 500 loss: 0.664407\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.584441\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.557542\n",
      "Step 1500 accuracy: 83.4%\n",
      "Step 2000 loss: 0.545130\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.515020\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.499908\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.472071\n",
      "Step 3500 accuracy: 86.5%\n",
      "Step 4000 loss: 0.521353\n",
      "Step 4000 accuracy: 84.7%\n",
      "Step 4500 loss: 0.480513\n",
      "Step 4500 accuracy: 85.8%\n",
      "Step 5000 loss: 0.459019\n",
      "Step 5000 accuracy: 86.4%\n",
      "Step 5500 loss: 0.478831\n",
      "Step 5500 accuracy: 85.3%\n",
      "Step 6000 loss: 0.482114\n",
      "Step 6000 accuracy: 86.1%\n",
      "Step 6500 loss: 0.447676\n",
      "Step 6500 accuracy: 87.0%\n",
      "Step 7000 loss: 0.421503\n",
      "Step 7000 accuracy: 87.4%\n",
      "Step 7500 loss: 0.445961\n",
      "Step 7500 accuracy: 87.4%\n",
      "Step 8000 loss: 0.405148\n",
      "Step 8000 accuracy: 87.6%\n",
      "Step 8500 loss: 0.460559\n",
      "Step 8500 accuracy: 86.5%\n",
      "Step 9000 loss: 0.436485\n",
      "Step 9000 accuracy: 88.1%\n",
      "Step 9500 loss: 0.418554\n",
      "Step 9500 accuracy: 88.2%\n",
      "Step 10000 loss: 0.434548\n",
      "Step 10000 accuracy: 87.6%\n",
      "Accuracy in test dataset: 94.01\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.421944\n",
      "Step 0 accuracy: 8.9%\n",
      "Step 500 loss: 0.715391\n",
      "Step 500 accuracy: 80.7%\n",
      "Step 1000 loss: 0.625601\n",
      "Step 1000 accuracy: 81.8%\n",
      "Step 1500 loss: 0.598352\n",
      "Step 1500 accuracy: 82.5%\n",
      "Step 2000 loss: 0.598965\n",
      "Step 2000 accuracy: 83.4%\n",
      "Step 2500 loss: 0.555348\n",
      "Step 2500 accuracy: 83.9%\n",
      "Step 3000 loss: 0.551580\n",
      "Step 3000 accuracy: 84.7%\n",
      "Step 3500 loss: 0.523572\n",
      "Step 3500 accuracy: 84.6%\n",
      "Step 4000 loss: 0.582416\n",
      "Step 4000 accuracy: 83.4%\n",
      "Step 4500 loss: 0.527272\n",
      "Step 4500 accuracy: 85.0%\n",
      "Step 5000 loss: 0.510630\n",
      "Step 5000 accuracy: 84.8%\n",
      "Step 5500 loss: 0.535299\n",
      "Step 5500 accuracy: 84.8%\n",
      "Step 6000 loss: 0.529006\n",
      "Step 6000 accuracy: 84.5%\n",
      "Step 6500 loss: 0.503156\n",
      "Step 6500 accuracy: 85.0%\n",
      "Step 7000 loss: 0.491576\n",
      "Step 7000 accuracy: 85.5%\n",
      "Step 7500 loss: 0.498968\n",
      "Step 7500 accuracy: 85.7%\n",
      "Step 8000 loss: 0.453295\n",
      "Step 8000 accuracy: 86.7%\n",
      "Step 8500 loss: 0.509971\n",
      "Step 8500 accuracy: 85.3%\n",
      "Step 9000 loss: 0.490750\n",
      "Step 9000 accuracy: 86.6%\n",
      "Step 9500 loss: 0.474458\n",
      "Step 9500 accuracy: 86.4%\n",
      "Step 10000 loss: 0.492810\n",
      "Step 10000 accuracy: 86.8%\n",
      "Accuracy in test dataset: 93.5\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.533817\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.778994\n",
      "Step 500 accuracy: 79.0%\n",
      "Step 1000 loss: 0.693313\n",
      "Step 1000 accuracy: 80.7%\n",
      "Step 1500 loss: 0.646020\n",
      "Step 1500 accuracy: 81.4%\n",
      "Step 2000 loss: 0.655634\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.609131\n",
      "Step 2500 accuracy: 82.6%\n",
      "Step 3000 loss: 0.609546\n",
      "Step 3000 accuracy: 82.3%\n",
      "Step 3500 loss: 0.578486\n",
      "Step 3500 accuracy: 83.8%\n",
      "Step 4000 loss: 0.659640\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.615951\n",
      "Step 4500 accuracy: 82.4%\n",
      "Step 5000 loss: 0.587674\n",
      "Step 5000 accuracy: 83.0%\n",
      "Step 5500 loss: 0.600814\n",
      "Step 5500 accuracy: 83.2%\n",
      "Step 6000 loss: 0.590312\n",
      "Step 6000 accuracy: 82.8%\n",
      "Step 6500 loss: 0.581584\n",
      "Step 6500 accuracy: 83.1%\n",
      "Step 7000 loss: 0.563168\n",
      "Step 7000 accuracy: 83.6%\n",
      "Step 7500 loss: 0.572983\n",
      "Step 7500 accuracy: 82.9%\n",
      "Step 8000 loss: 0.534748\n",
      "Step 8000 accuracy: 84.8%\n",
      "Step 8500 loss: 0.581918\n",
      "Step 8500 accuracy: 82.8%\n",
      "Step 9000 loss: 0.563469\n",
      "Step 9000 accuracy: 84.6%\n",
      "Step 9500 loss: 0.561165\n",
      "Step 9500 accuracy: 84.2%\n",
      "Step 10000 loss: 0.594980\n",
      "Step 10000 accuracy: 82.6%\n",
      "Accuracy in test dataset: 92.38\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.310635\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.608518\n",
      "Step 500 accuracy: 82.7%\n",
      "Step 1000 loss: 0.524202\n",
      "Step 1000 accuracy: 85.1%\n",
      "Step 1500 loss: 0.501725\n",
      "Step 1500 accuracy: 85.4%\n",
      "Step 2000 loss: 0.485847\n",
      "Step 2000 accuracy: 86.0%\n",
      "Step 2500 loss: 0.451268\n",
      "Step 2500 accuracy: 86.8%\n",
      "Step 3000 loss: 0.428724\n",
      "Step 3000 accuracy: 87.2%\n",
      "Step 3500 loss: 0.400648\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.452376\n",
      "Step 4000 accuracy: 87.0%\n",
      "Step 4500 loss: 0.404328\n",
      "Step 4500 accuracy: 88.8%\n",
      "Step 5000 loss: 0.387511\n",
      "Step 5000 accuracy: 89.0%\n",
      "Step 5500 loss: 0.404801\n",
      "Step 5500 accuracy: 87.7%\n",
      "Step 6000 loss: 0.388323\n",
      "Step 6000 accuracy: 88.6%\n",
      "Step 6500 loss: 0.361433\n",
      "Step 6500 accuracy: 89.6%\n",
      "Step 7000 loss: 0.335686\n",
      "Step 7000 accuracy: 90.3%\n",
      "Step 7500 loss: 0.363007\n",
      "Step 7500 accuracy: 89.9%\n",
      "Step 8000 loss: 0.319336\n",
      "Step 8000 accuracy: 90.4%\n",
      "Step 8500 loss: 0.359539\n",
      "Step 8500 accuracy: 89.6%\n",
      "Step 9000 loss: 0.323938\n",
      "Step 9000 accuracy: 91.3%\n",
      "Step 9500 loss: 0.308002\n",
      "Step 9500 accuracy: 90.6%\n",
      "Step 10000 loss: 0.337088\n",
      "Step 10000 accuracy: 90.6%\n",
      "Accuracy in test dataset: 94.99\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.297567\n",
      "Step 0 accuracy: 13.2%\n",
      "Step 500 loss: 0.617545\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.531684\n",
      "Step 1000 accuracy: 84.2%\n",
      "Step 1500 loss: 0.506253\n",
      "Step 1500 accuracy: 85.5%\n",
      "Step 2000 loss: 0.495043\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.459057\n",
      "Step 2500 accuracy: 86.1%\n",
      "Step 3000 loss: 0.446695\n",
      "Step 3000 accuracy: 87.0%\n",
      "Step 3500 loss: 0.415996\n",
      "Step 3500 accuracy: 88.0%\n",
      "Step 4000 loss: 0.465006\n",
      "Step 4000 accuracy: 86.8%\n",
      "Step 4500 loss: 0.421110\n",
      "Step 4500 accuracy: 87.9%\n",
      "Step 5000 loss: 0.401000\n",
      "Step 5000 accuracy: 88.2%\n",
      "Step 5500 loss: 0.421493\n",
      "Step 5500 accuracy: 86.9%\n",
      "Step 6000 loss: 0.409607\n",
      "Step 6000 accuracy: 88.2%\n",
      "Step 6500 loss: 0.389145\n",
      "Step 6500 accuracy: 88.8%\n",
      "Step 7000 loss: 0.348216\n",
      "Step 7000 accuracy: 89.6%\n",
      "Step 7500 loss: 0.382559\n",
      "Step 7500 accuracy: 88.8%\n",
      "Step 8000 loss: 0.327411\n",
      "Step 8000 accuracy: 90.2%\n",
      "Step 8500 loss: 0.385664\n",
      "Step 8500 accuracy: 89.3%\n",
      "Step 9000 loss: 0.364501\n",
      "Step 9000 accuracy: 90.3%\n",
      "Step 9500 loss: 0.328779\n",
      "Step 9500 accuracy: 90.2%\n",
      "Step 10000 loss: 0.357017\n",
      "Step 10000 accuracy: 89.2%\n",
      "Accuracy in test dataset: 94.79\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.359092\n",
      "Step 0 accuracy: 12.3%\n",
      "Step 500 loss: 0.639711\n",
      "Step 500 accuracy: 81.4%\n",
      "Step 1000 loss: 0.561547\n",
      "Step 1000 accuracy: 83.4%\n",
      "Step 1500 loss: 0.536411\n",
      "Step 1500 accuracy: 84.2%\n",
      "Step 2000 loss: 0.526764\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.495138\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.467825\n",
      "Step 3000 accuracy: 85.6%\n",
      "Step 3500 loss: 0.450661\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.508927\n",
      "Step 4000 accuracy: 85.3%\n",
      "Step 4500 loss: 0.466319\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.447158\n",
      "Step 5000 accuracy: 87.5%\n",
      "Step 5500 loss: 0.467883\n",
      "Step 5500 accuracy: 85.5%\n",
      "Step 6000 loss: 0.458274\n",
      "Step 6000 accuracy: 86.8%\n",
      "Step 6500 loss: 0.423029\n",
      "Step 6500 accuracy: 87.4%\n",
      "Step 7000 loss: 0.404271\n",
      "Step 7000 accuracy: 87.4%\n",
      "Step 7500 loss: 0.421692\n",
      "Step 7500 accuracy: 88.1%\n",
      "Step 8000 loss: 0.392322\n",
      "Step 8000 accuracy: 88.1%\n",
      "Step 8500 loss: 0.438467\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.414016\n",
      "Step 9000 accuracy: 88.7%\n",
      "Step 9500 loss: 0.387666\n",
      "Step 9500 accuracy: 88.8%\n",
      "Step 10000 loss: 0.418043\n",
      "Step 10000 accuracy: 88.0%\n",
      "Accuracy in test dataset: 94.4\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.395446\n",
      "Step 0 accuracy: 12.0%\n",
      "Step 500 loss: 0.687199\n",
      "Step 500 accuracy: 80.2%\n",
      "Step 1000 loss: 0.607568\n",
      "Step 1000 accuracy: 82.2%\n",
      "Step 1500 loss: 0.590161\n",
      "Step 1500 accuracy: 82.5%\n",
      "Step 2000 loss: 0.568993\n",
      "Step 2000 accuracy: 84.1%\n",
      "Step 2500 loss: 0.525347\n",
      "Step 2500 accuracy: 85.0%\n",
      "Step 3000 loss: 0.518939\n",
      "Step 3000 accuracy: 84.6%\n",
      "Step 3500 loss: 0.502750\n",
      "Step 3500 accuracy: 85.3%\n",
      "Step 4000 loss: 0.546456\n",
      "Step 4000 accuracy: 84.7%\n",
      "Step 4500 loss: 0.515505\n",
      "Step 4500 accuracy: 85.3%\n",
      "Step 5000 loss: 0.495031\n",
      "Step 5000 accuracy: 85.2%\n",
      "Step 5500 loss: 0.514131\n",
      "Step 5500 accuracy: 85.1%\n",
      "Step 6000 loss: 0.512650\n",
      "Step 6000 accuracy: 85.4%\n",
      "Step 6500 loss: 0.469740\n",
      "Step 6500 accuracy: 86.3%\n",
      "Step 7000 loss: 0.462231\n",
      "Step 7000 accuracy: 85.7%\n",
      "Step 7500 loss: 0.485798\n",
      "Step 7500 accuracy: 86.0%\n",
      "Step 8000 loss: 0.439739\n",
      "Step 8000 accuracy: 87.2%\n",
      "Step 8500 loss: 0.482696\n",
      "Step 8500 accuracy: 85.5%\n",
      "Step 9000 loss: 0.456118\n",
      "Step 9000 accuracy: 87.2%\n",
      "Step 9500 loss: 0.437216\n",
      "Step 9500 accuracy: 87.0%\n",
      "Step 10000 loss: 0.471981\n",
      "Step 10000 accuracy: 86.9%\n",
      "Accuracy in test dataset: 93.78\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.05 | num_step: 10240 | batch_size: 2000 | over_fit : False |\n",
      "Step 0 loss: 2.570330\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.742612\n",
      "Step 500 accuracy: 78.6%\n",
      "Step 1000 loss: 0.661487\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.623171\n",
      "Step 1500 accuracy: 82.5%\n",
      "Step 2000 loss: 0.640961\n",
      "Step 2000 accuracy: 82.6%\n",
      "Step 2500 loss: 0.607549\n",
      "Step 2500 accuracy: 82.3%\n",
      "Step 3000 loss: 0.573080\n",
      "Step 3000 accuracy: 83.4%\n",
      "Step 3500 loss: 0.563956\n",
      "Step 3500 accuracy: 83.7%\n",
      "Step 4000 loss: 0.619461\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.552552\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.547659\n",
      "Step 5000 accuracy: 84.8%\n",
      "Step 5500 loss: 0.573451\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.563963\n",
      "Step 6000 accuracy: 84.1%\n",
      "Step 6500 loss: 0.525102\n",
      "Step 6500 accuracy: 84.3%\n",
      "Step 7000 loss: 0.515090\n",
      "Step 7000 accuracy: 84.6%\n",
      "Step 7500 loss: 0.524719\n",
      "Step 7500 accuracy: 85.3%\n",
      "Step 8000 loss: 0.472948\n",
      "Step 8000 accuracy: 86.0%\n",
      "Step 8500 loss: 0.558839\n",
      "Step 8500 accuracy: 83.7%\n",
      "Step 9000 loss: 0.520304\n",
      "Step 9000 accuracy: 85.9%\n",
      "Step 9500 loss: 0.503327\n",
      "Step 9500 accuracy: 85.9%\n",
      "Step 10000 loss: 0.547607\n",
      "Step 10000 accuracy: 83.9%\n",
      "Accuracy in test dataset: 92.87\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.404871\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.648115\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.641781\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.557532\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.561992\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.549363\n",
      "Step 2500 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.76\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.349731\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.683059\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.651963\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.596175\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.590683\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.598413\n",
      "Step 2500 accuracy: 85.2%\n",
      "Accuracy in test dataset: 92.4\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.385266\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.742257\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.753455\n",
      "Step 1000 accuracy: 75.8%\n",
      "Step 1500 loss: 0.603915\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.653641\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.716415\n",
      "Step 2500 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.98\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.436849\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.892878\n",
      "Step 500 accuracy: 72.7%\n",
      "Step 1000 loss: 0.776173\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.738876\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.746385\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.837323\n",
      "Step 2500 accuracy: 78.1%\n",
      "Accuracy in test dataset: 90.91\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.639291\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 1.026167\n",
      "Step 500 accuracy: 69.5%\n",
      "Step 1000 loss: 0.948869\n",
      "Step 1000 accuracy: 71.1%\n",
      "Step 1500 loss: 0.952930\n",
      "Step 1500 accuracy: 75.8%\n",
      "Step 2000 loss: 0.860283\n",
      "Step 2000 accuracy: 71.1%\n",
      "Step 2500 loss: 0.986655\n",
      "Step 2500 accuracy: 74.2%\n",
      "Accuracy in test dataset: 89.83\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.358922\n",
      "Step 0 accuracy: 4.7%\n",
      "Step 500 loss: 0.631852\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.651234\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.530440\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.571174\n",
      "Step 2000 accuracy: 86.7%\n",
      "Step 2500 loss: 0.562697\n",
      "Step 2500 accuracy: 81.2%\n",
      "Accuracy in test dataset: 93.03\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.339577\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.667161\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.642923\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.599892\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.598001\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.588927\n",
      "Step 2500 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.85\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.392956\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.730592\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.707183\n",
      "Step 1000 accuracy: 75.8%\n",
      "Step 1500 loss: 0.594761\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.618477\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.699733\n",
      "Step 2500 accuracy: 79.7%\n",
      "Accuracy in test dataset: 92.4\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.529230\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.777155\n",
      "Step 500 accuracy: 75.8%\n",
      "Step 1000 loss: 0.802777\n",
      "Step 1000 accuracy: 71.1%\n",
      "Step 1500 loss: 0.674352\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.748347\n",
      "Step 2000 accuracy: 75.8%\n",
      "Step 2500 loss: 0.787759\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 91.43\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.550712\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.887786\n",
      "Step 500 accuracy: 71.9%\n",
      "Step 1000 loss: 0.904831\n",
      "Step 1000 accuracy: 71.1%\n",
      "Step 1500 loss: 0.745573\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.935719\n",
      "Step 2000 accuracy: 75.8%\n",
      "Step 2500 loss: 0.884417\n",
      "Step 2500 accuracy: 71.9%\n",
      "Accuracy in test dataset: 90.69\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.304770\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.645896\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.625893\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.551460\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.565817\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.552699\n",
      "Step 2500 accuracy: 82.0%\n",
      "Accuracy in test dataset: 93.05\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.286896\n",
      "Step 0 accuracy: 23.4%\n",
      "Step 500 loss: 0.647450\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.642659\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.572976\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.584905\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.582455\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.07\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.399647\n",
      "Step 0 accuracy: 15.6%\n",
      "Step 500 loss: 0.692509\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.687786\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.628506\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.611048\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.685351\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 92.63\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.401986\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.823552\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.774618\n",
      "Step 1000 accuracy: 76.6%\n",
      "Step 1500 loss: 0.683532\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.720556\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.793609\n",
      "Step 2500 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.98\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.632393\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.871410\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.844677\n",
      "Step 1000 accuracy: 75.0%\n",
      "Step 1500 loss: 0.740207\n",
      "Step 1500 accuracy: 78.1%\n",
      "Step 2000 loss: 0.789980\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.798790\n",
      "Step 2500 accuracy: 76.6%\n",
      "Accuracy in test dataset: 91.22\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.291587\n",
      "Step 0 accuracy: 15.6%\n",
      "Step 500 loss: 0.627863\n",
      "Step 500 accuracy: 83.6%\n",
      "Step 1000 loss: 0.621558\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.547652\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.557633\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.523818\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.45\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.334687\n",
      "Step 0 accuracy: 4.7%\n",
      "Step 500 loss: 0.618389\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.651016\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.534765\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.567351\n",
      "Step 2000 accuracy: 86.7%\n",
      "Step 2500 loss: 0.571761\n",
      "Step 2500 accuracy: 82.8%\n",
      "Accuracy in test dataset: 93.37\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.353267\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.680917\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.666661\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.590851\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.592974\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.599834\n",
      "Step 2500 accuracy: 83.6%\n",
      "Accuracy in test dataset: 92.77\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.462357\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.788168\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.704927\n",
      "Step 1000 accuracy: 74.2%\n",
      "Step 1500 loss: 0.672936\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.700640\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.647439\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 92.17\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.713948\n",
      "Step 0 accuracy: 5.5%\n",
      "Step 500 loss: 0.838740\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.851561\n",
      "Step 1000 accuracy: 72.7%\n",
      "Step 1500 loss: 0.706536\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.846653\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.785627\n",
      "Step 2500 accuracy: 77.3%\n",
      "Accuracy in test dataset: 91.53\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.329504\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.648672\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.627459\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.549621\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.589827\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.581946\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.486326\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.423281\n",
      "Step 3500 accuracy: 89.8%\n",
      "Step 4000 loss: 0.337842\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.461338\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.473582\n",
      "Step 5000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 94.15\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.371949\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.718520\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.651037\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.560068\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.624857\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.583956\n",
      "Step 2500 accuracy: 83.6%\n",
      "Step 3000 loss: 0.502965\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.465236\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.377253\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.526858\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.460082\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.82\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.376191\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.768848\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.736695\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.669894\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.664820\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.739736\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.671228\n",
      "Step 3000 accuracy: 78.1%\n",
      "Step 3500 loss: 0.596981\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.467298\n",
      "Step 4000 accuracy: 86.7%\n",
      "Step 4500 loss: 0.531503\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.574593\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.67\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.440967\n",
      "Step 0 accuracy: 14.1%\n",
      "Step 500 loss: 0.841366\n",
      "Step 500 accuracy: 75.8%\n",
      "Step 1000 loss: 0.811360\n",
      "Step 1000 accuracy: 75.8%\n",
      "Step 1500 loss: 0.768174\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.780423\n",
      "Step 2000 accuracy: 78.9%\n",
      "Step 2500 loss: 0.848354\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.755013\n",
      "Step 3000 accuracy: 80.5%\n",
      "Step 3500 loss: 0.824486\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.537819\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.691065\n",
      "Step 4500 accuracy: 78.9%\n",
      "Step 5000 loss: 0.708410\n",
      "Step 5000 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.76\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.515795\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 1.001421\n",
      "Step 500 accuracy: 73.4%\n",
      "Step 1000 loss: 0.999155\n",
      "Step 1000 accuracy: 68.0%\n",
      "Step 1500 loss: 1.010484\n",
      "Step 1500 accuracy: 74.2%\n",
      "Step 2000 loss: 1.074636\n",
      "Step 2000 accuracy: 71.1%\n",
      "Step 2500 loss: 0.955718\n",
      "Step 2500 accuracy: 72.7%\n",
      "Step 3000 loss: 0.987408\n",
      "Step 3000 accuracy: 74.2%\n",
      "Step 3500 loss: 0.831465\n",
      "Step 3500 accuracy: 76.6%\n",
      "Step 4000 loss: 0.733845\n",
      "Step 4000 accuracy: 77.3%\n",
      "Step 4500 loss: 0.839916\n",
      "Step 4500 accuracy: 75.8%\n",
      "Step 5000 loss: 0.795923\n",
      "Step 5000 accuracy: 74.2%\n",
      "Accuracy in test dataset: 90.54\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.340502\n",
      "Step 0 accuracy: 5.5%\n",
      "Step 500 loss: 0.644406\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.628528\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.565489\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.562415\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.554504\n",
      "Step 2500 accuracy: 83.6%\n",
      "Step 3000 loss: 0.466449\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.406928\n",
      "Step 3500 accuracy: 89.8%\n",
      "Step 4000 loss: 0.324948\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.465559\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.444917\n",
      "Step 5000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 94.4\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.296993\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.682882\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.657134\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.597860\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.594895\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.594479\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.506947\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.471825\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.337765\n",
      "Step 4000 accuracy: 89.8%\n",
      "Step 4500 loss: 0.522742\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.469482\n",
      "Step 5000 accuracy: 88.3%\n",
      "Accuracy in test dataset: 94.12\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.364598\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.741789\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.725678\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.601958\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.643111\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.617164\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.582400\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.501438\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.436133\n",
      "Step 4000 accuracy: 88.3%\n",
      "Step 4500 loss: 0.509591\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.596729\n",
      "Step 5000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.39\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.326373\n",
      "Step 0 accuracy: 15.6%\n",
      "Step 500 loss: 0.826567\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.829969\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.765630\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.775321\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.717624\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.720821\n",
      "Step 3000 accuracy: 80.5%\n",
      "Step 3500 loss: 0.631637\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.523861\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.614721\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.610820\n",
      "Step 5000 accuracy: 82.0%\n",
      "Accuracy in test dataset: 92.13\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.703040\n",
      "Step 0 accuracy: 3.1%\n",
      "Step 500 loss: 0.844543\n",
      "Step 500 accuracy: 76.6%\n",
      "Step 1000 loss: 0.914683\n",
      "Step 1000 accuracy: 73.4%\n",
      "Step 1500 loss: 0.744353\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.976507\n",
      "Step 2000 accuracy: 69.5%\n",
      "Step 2500 loss: 0.863475\n",
      "Step 2500 accuracy: 76.6%\n",
      "Step 3000 loss: 0.800604\n",
      "Step 3000 accuracy: 76.6%\n",
      "Step 3500 loss: 0.707374\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.720305\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.691167\n",
      "Step 4500 accuracy: 76.6%\n",
      "Step 5000 loss: 0.772654\n",
      "Step 5000 accuracy: 76.6%\n",
      "Accuracy in test dataset: 91.19\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.304172\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.640447\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.642517\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.534385\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.546871\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.538717\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.465493\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.397260\n",
      "Step 3500 accuracy: 91.4%\n",
      "Step 4000 loss: 0.300471\n",
      "Step 4000 accuracy: 92.2%\n",
      "Step 4500 loss: 0.435115\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.428658\n",
      "Step 5000 accuracy: 88.3%\n",
      "Accuracy in test dataset: 94.4\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.270314\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.671277\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.656368\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.550297\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.567567\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.575095\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.510478\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.442061\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.326370\n",
      "Step 4000 accuracy: 91.4%\n",
      "Step 4500 loss: 0.446362\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.426928\n",
      "Step 5000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 94.37\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.319333\n",
      "Step 0 accuracy: 14.8%\n",
      "Step 500 loss: 0.686335\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.663727\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.620175\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.623252\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.647248\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.615321\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.510983\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.410732\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.568920\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.547872\n",
      "Step 5000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.74\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.514046\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.800861\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.773250\n",
      "Step 1000 accuracy: 75.8%\n",
      "Step 1500 loss: 0.647496\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.664903\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.778784\n",
      "Step 2500 accuracy: 78.9%\n",
      "Step 3000 loss: 0.590572\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.598675\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.521664\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.538224\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.638905\n",
      "Step 5000 accuracy: 78.9%\n",
      "Accuracy in test dataset: 92.97\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.564986\n",
      "Step 0 accuracy: 5.5%\n",
      "Step 500 loss: 0.941465\n",
      "Step 500 accuracy: 70.3%\n",
      "Step 1000 loss: 0.838116\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.781128\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.795158\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.967623\n",
      "Step 2500 accuracy: 73.4%\n",
      "Step 3000 loss: 0.712788\n",
      "Step 3000 accuracy: 80.5%\n",
      "Step 3500 loss: 0.713600\n",
      "Step 3500 accuracy: 81.2%\n",
      "Step 4000 loss: 0.584625\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.710482\n",
      "Step 4500 accuracy: 78.1%\n",
      "Step 5000 loss: 0.775883\n",
      "Step 5000 accuracy: 75.8%\n",
      "Accuracy in test dataset: 91.85\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.371384\n",
      "Step 0 accuracy: 5.5%\n",
      "Step 500 loss: 0.645946\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.626493\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.539944\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.563254\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.551039\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.454504\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.382600\n",
      "Step 3500 accuracy: 91.4%\n",
      "Step 4000 loss: 0.293546\n",
      "Step 4000 accuracy: 92.2%\n",
      "Step 4500 loss: 0.450436\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.421226\n",
      "Step 5000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 94.67\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.343374\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.649883\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.637260\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.546093\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.596541\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.530008\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.464545\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.399650\n",
      "Step 3500 accuracy: 89.8%\n",
      "Step 4000 loss: 0.314272\n",
      "Step 4000 accuracy: 90.6%\n",
      "Step 4500 loss: 0.482411\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.444367\n",
      "Step 5000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 94.38\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.281915\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.673375\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.702067\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.597242\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.607216\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.624526\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.518129\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.487129\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.406546\n",
      "Step 4000 accuracy: 85.9%\n",
      "Step 4500 loss: 0.470159\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.482109\n",
      "Step 5000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 94.13\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.425090\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.680007\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.683280\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.661138\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.669385\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.806632\n",
      "Step 2500 accuracy: 76.6%\n",
      "Step 3000 loss: 0.546452\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.587199\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.360950\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.602890\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.635033\n",
      "Step 5000 accuracy: 79.7%\n",
      "Accuracy in test dataset: 93.44\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.595905\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.836353\n",
      "Step 500 accuracy: 75.0%\n",
      "Step 1000 loss: 0.738096\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.782740\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.844578\n",
      "Step 2000 accuracy: 75.8%\n",
      "Step 2500 loss: 0.917035\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.687520\n",
      "Step 3000 accuracy: 80.5%\n",
      "Step 3500 loss: 0.686120\n",
      "Step 3500 accuracy: 84.4%\n",
      "Step 4000 loss: 0.626428\n",
      "Step 4000 accuracy: 84.4%\n",
      "Step 4500 loss: 0.602384\n",
      "Step 4500 accuracy: 82.0%\n",
      "Step 5000 loss: 0.699121\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.37\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.355138\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.660807\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.638115\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.578718\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.563023\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.574224\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.487986\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.418488\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.343680\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.475904\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.431847\n",
      "Step 5000 accuracy: 85.9%\n",
      "Step 5500 loss: 0.391710\n",
      "Step 5500 accuracy: 89.8%\n",
      "Step 6000 loss: 0.391829\n",
      "Step 6000 accuracy: 90.6%\n",
      "Step 6500 loss: 0.300134\n",
      "Step 6500 accuracy: 93.0%\n",
      "Step 7000 loss: 0.410396\n",
      "Step 7000 accuracy: 89.8%\n",
      "Step 7500 loss: 0.366560\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.216442\n",
      "Step 8000 accuracy: 92.2%\n",
      "Step 8500 loss: 0.282091\n",
      "Step 8500 accuracy: 93.8%\n",
      "Step 9000 loss: 0.292626\n",
      "Step 9000 accuracy: 93.0%\n",
      "Step 9500 loss: 0.324989\n",
      "Step 9500 accuracy: 93.0%\n",
      "Step 10000 loss: 0.548164\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 94.82\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.322061\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.677290\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.654112\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.576978\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.584189\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.649237\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.534411\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.496358\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.344814\n",
      "Step 4000 accuracy: 92.2%\n",
      "Step 4500 loss: 0.499989\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.528689\n",
      "Step 5000 accuracy: 82.8%\n",
      "Step 5500 loss: 0.398396\n",
      "Step 5500 accuracy: 89.8%\n",
      "Step 6000 loss: 0.437187\n",
      "Step 6000 accuracy: 91.4%\n",
      "Step 6500 loss: 0.342617\n",
      "Step 6500 accuracy: 87.5%\n",
      "Step 7000 loss: 0.437723\n",
      "Step 7000 accuracy: 89.8%\n",
      "Step 7500 loss: 0.428113\n",
      "Step 7500 accuracy: 86.7%\n",
      "Step 8000 loss: 0.232051\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.375049\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.382479\n",
      "Step 9000 accuracy: 89.1%\n",
      "Step 9500 loss: 0.392706\n",
      "Step 9500 accuracy: 87.5%\n",
      "Step 10000 loss: 0.551295\n",
      "Step 10000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 94.71\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.352991\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.788193\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.741212\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.666745\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.633083\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.692524\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.681351\n",
      "Step 3000 accuracy: 81.2%\n",
      "Step 3500 loss: 0.701061\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.501081\n",
      "Step 4000 accuracy: 86.7%\n",
      "Step 4500 loss: 0.503672\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.604895\n",
      "Step 5000 accuracy: 82.0%\n",
      "Step 5500 loss: 0.639803\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.555812\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.463993\n",
      "Step 6500 accuracy: 85.9%\n",
      "Step 7000 loss: 0.531510\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.575269\n",
      "Step 7500 accuracy: 82.8%\n",
      "Step 8000 loss: 0.353943\n",
      "Step 8000 accuracy: 89.1%\n",
      "Step 8500 loss: 0.467866\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.481909\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.445234\n",
      "Step 9500 accuracy: 85.9%\n",
      "Step 10000 loss: 0.628759\n",
      "Step 10000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 93.46\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.493457\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.835558\n",
      "Step 500 accuracy: 75.0%\n",
      "Step 1000 loss: 0.901598\n",
      "Step 1000 accuracy: 75.0%\n",
      "Step 1500 loss: 0.747945\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.820350\n",
      "Step 2000 accuracy: 75.8%\n",
      "Step 2500 loss: 0.821390\n",
      "Step 2500 accuracy: 78.1%\n",
      "Step 3000 loss: 0.771987\n",
      "Step 3000 accuracy: 82.0%\n",
      "Step 3500 loss: 0.826695\n",
      "Step 3500 accuracy: 81.2%\n",
      "Step 4000 loss: 0.613264\n",
      "Step 4000 accuracy: 85.2%\n",
      "Step 4500 loss: 0.627881\n",
      "Step 4500 accuracy: 78.9%\n",
      "Step 5000 loss: 0.727209\n",
      "Step 5000 accuracy: 79.7%\n",
      "Step 5500 loss: 0.669633\n",
      "Step 5500 accuracy: 78.9%\n",
      "Step 6000 loss: 0.586159\n",
      "Step 6000 accuracy: 82.8%\n",
      "Step 6500 loss: 0.639931\n",
      "Step 6500 accuracy: 79.7%\n",
      "Step 7000 loss: 0.635880\n",
      "Step 7000 accuracy: 80.5%\n",
      "Step 7500 loss: 0.687801\n",
      "Step 7500 accuracy: 80.5%\n",
      "Step 8000 loss: 0.386766\n",
      "Step 8000 accuracy: 89.8%\n",
      "Step 8500 loss: 0.469372\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.669257\n",
      "Step 9000 accuracy: 82.0%\n",
      "Step 9500 loss: 0.701500\n",
      "Step 9500 accuracy: 80.5%\n",
      "Step 10000 loss: 0.633487\n",
      "Step 10000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 92.23\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.661192\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.995992\n",
      "Step 500 accuracy: 75.8%\n",
      "Step 1000 loss: 1.050918\n",
      "Step 1000 accuracy: 73.4%\n",
      "Step 1500 loss: 0.816517\n",
      "Step 1500 accuracy: 77.3%\n",
      "Step 2000 loss: 0.919511\n",
      "Step 2000 accuracy: 75.0%\n",
      "Step 2500 loss: 1.080818\n",
      "Step 2500 accuracy: 73.4%\n",
      "Step 3000 loss: 0.971606\n",
      "Step 3000 accuracy: 75.0%\n",
      "Step 3500 loss: 0.887650\n",
      "Step 3500 accuracy: 72.7%\n",
      "Step 4000 loss: 0.729165\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.850754\n",
      "Step 4500 accuracy: 73.4%\n",
      "Step 5000 loss: 0.841025\n",
      "Step 5000 accuracy: 77.3%\n",
      "Step 5500 loss: 0.927895\n",
      "Step 5500 accuracy: 78.9%\n",
      "Step 6000 loss: 1.003397\n",
      "Step 6000 accuracy: 75.8%\n",
      "Step 6500 loss: 0.760542\n",
      "Step 6500 accuracy: 78.1%\n",
      "Step 7000 loss: 0.810370\n",
      "Step 7000 accuracy: 75.0%\n",
      "Step 7500 loss: 0.838290\n",
      "Step 7500 accuracy: 70.3%\n",
      "Step 8000 loss: 0.628763\n",
      "Step 8000 accuracy: 80.5%\n",
      "Step 8500 loss: 0.757858\n",
      "Step 8500 accuracy: 78.9%\n",
      "Step 9000 loss: 0.805661\n",
      "Step 9000 accuracy: 78.1%\n",
      "Step 9500 loss: 0.752354\n",
      "Step 9500 accuracy: 73.4%\n",
      "Step 10000 loss: 0.885435\n",
      "Step 10000 accuracy: 75.8%\n",
      "Accuracy in test dataset: 90.78\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.308142\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.654524\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.636174\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.542402\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.571702\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.547214\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.480923\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.413203\n",
      "Step 3500 accuracy: 90.6%\n",
      "Step 4000 loss: 0.315151\n",
      "Step 4000 accuracy: 90.6%\n",
      "Step 4500 loss: 0.459388\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.446947\n",
      "Step 5000 accuracy: 86.7%\n",
      "Step 5500 loss: 0.363738\n",
      "Step 5500 accuracy: 90.6%\n",
      "Step 6000 loss: 0.378067\n",
      "Step 6000 accuracy: 91.4%\n",
      "Step 6500 loss: 0.296570\n",
      "Step 6500 accuracy: 90.6%\n",
      "Step 7000 loss: 0.395314\n",
      "Step 7000 accuracy: 88.3%\n",
      "Step 7500 loss: 0.355807\n",
      "Step 7500 accuracy: 89.1%\n",
      "Step 8000 loss: 0.215106\n",
      "Step 8000 accuracy: 93.0%\n",
      "Step 8500 loss: 0.282854\n",
      "Step 8500 accuracy: 92.2%\n",
      "Step 9000 loss: 0.289117\n",
      "Step 9000 accuracy: 94.5%\n",
      "Step 9500 loss: 0.298091\n",
      "Step 9500 accuracy: 89.1%\n",
      "Step 10000 loss: 0.559832\n",
      "Step 10000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 94.97\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.265077\n",
      "Step 0 accuracy: 15.6%\n",
      "Step 500 loss: 0.653421\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.653056\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.538755\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.627585\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.582468\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.504982\n",
      "Step 3000 accuracy: 84.4%\n",
      "Step 3500 loss: 0.446527\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.334238\n",
      "Step 4000 accuracy: 91.4%\n",
      "Step 4500 loss: 0.476249\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.480170\n",
      "Step 5000 accuracy: 85.9%\n",
      "Step 5500 loss: 0.398740\n",
      "Step 5500 accuracy: 87.5%\n",
      "Step 6000 loss: 0.430072\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.363543\n",
      "Step 6500 accuracy: 89.1%\n",
      "Step 7000 loss: 0.406562\n",
      "Step 7000 accuracy: 86.7%\n",
      "Step 7500 loss: 0.400198\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.245559\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.307340\n",
      "Step 8500 accuracy: 91.4%\n",
      "Step 9000 loss: 0.328047\n",
      "Step 9000 accuracy: 93.8%\n",
      "Step 9500 loss: 0.346191\n",
      "Step 9500 accuracy: 89.8%\n",
      "Step 10000 loss: 0.564247\n",
      "Step 10000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 94.91\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.335572\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.711757\n",
      "Step 500 accuracy: 79.7%\n",
      "Step 1000 loss: 0.686854\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.671537\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.625538\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.683996\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.579105\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.526872\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.469766\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.518031\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.575917\n",
      "Step 5000 accuracy: 81.2%\n",
      "Step 5500 loss: 0.479881\n",
      "Step 5500 accuracy: 86.7%\n",
      "Step 6000 loss: 0.493192\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.423883\n",
      "Step 6500 accuracy: 87.5%\n",
      "Step 7000 loss: 0.525101\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.535913\n",
      "Step 7500 accuracy: 85.9%\n",
      "Step 8000 loss: 0.342991\n",
      "Step 8000 accuracy: 89.8%\n",
      "Step 8500 loss: 0.377027\n",
      "Step 8500 accuracy: 91.4%\n",
      "Step 9000 loss: 0.417628\n",
      "Step 9000 accuracy: 89.8%\n",
      "Step 9500 loss: 0.468759\n",
      "Step 9500 accuracy: 84.4%\n",
      "Step 10000 loss: 0.627964\n",
      "Step 10000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 94.18\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.475088\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.793712\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.871050\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.794517\n",
      "Step 1500 accuracy: 77.3%\n",
      "Step 2000 loss: 0.764130\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.694760\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.669475\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.610314\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.509165\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.621421\n",
      "Step 4500 accuracy: 84.4%\n",
      "Step 5000 loss: 0.719264\n",
      "Step 5000 accuracy: 79.7%\n",
      "Step 5500 loss: 0.595918\n",
      "Step 5500 accuracy: 87.5%\n",
      "Step 6000 loss: 0.586678\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.510975\n",
      "Step 6500 accuracy: 83.6%\n",
      "Step 7000 loss: 0.645364\n",
      "Step 7000 accuracy: 82.0%\n",
      "Step 7500 loss: 0.664782\n",
      "Step 7500 accuracy: 79.7%\n",
      "Step 8000 loss: 0.404687\n",
      "Step 8000 accuracy: 89.8%\n",
      "Step 8500 loss: 0.421602\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.540621\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.532412\n",
      "Step 9500 accuracy: 82.0%\n",
      "Step 10000 loss: 0.705019\n",
      "Step 10000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 93.11\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.620097\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.974662\n",
      "Step 500 accuracy: 71.9%\n",
      "Step 1000 loss: 1.018088\n",
      "Step 1000 accuracy: 67.2%\n",
      "Step 1500 loss: 0.831573\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.940640\n",
      "Step 2000 accuracy: 76.6%\n",
      "Step 2500 loss: 0.958230\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.858177\n",
      "Step 3000 accuracy: 75.8%\n",
      "Step 3500 loss: 0.690127\n",
      "Step 3500 accuracy: 82.8%\n",
      "Step 4000 loss: 0.692261\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.811967\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.767259\n",
      "Step 5000 accuracy: 80.5%\n",
      "Step 5500 loss: 0.691929\n",
      "Step 5500 accuracy: 80.5%\n",
      "Step 6000 loss: 0.665178\n",
      "Step 6000 accuracy: 82.0%\n",
      "Step 6500 loss: 0.629510\n",
      "Step 6500 accuracy: 80.5%\n",
      "Step 7000 loss: 0.703450\n",
      "Step 7000 accuracy: 79.7%\n",
      "Step 7500 loss: 0.710170\n",
      "Step 7500 accuracy: 77.3%\n",
      "Step 8000 loss: 0.508261\n",
      "Step 8000 accuracy: 84.4%\n",
      "Step 8500 loss: 0.521250\n",
      "Step 8500 accuracy: 83.6%\n",
      "Step 9000 loss: 0.732226\n",
      "Step 9000 accuracy: 81.2%\n",
      "Step 9500 loss: 0.594558\n",
      "Step 9500 accuracy: 80.5%\n",
      "Step 10000 loss: 0.902478\n",
      "Step 10000 accuracy: 78.9%\n",
      "Accuracy in test dataset: 91.95\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.338879\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.640010\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.633556\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.537176\n",
      "Step 1500 accuracy: 86.7%\n",
      "Step 2000 loss: 0.573386\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.566515\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.462161\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.402427\n",
      "Step 3500 accuracy: 91.4%\n",
      "Step 4000 loss: 0.305435\n",
      "Step 4000 accuracy: 92.2%\n",
      "Step 4500 loss: 0.466154\n",
      "Step 4500 accuracy: 85.9%\n",
      "Step 5000 loss: 0.440720\n",
      "Step 5000 accuracy: 86.7%\n",
      "Step 5500 loss: 0.351279\n",
      "Step 5500 accuracy: 89.8%\n",
      "Step 6000 loss: 0.354101\n",
      "Step 6000 accuracy: 89.8%\n",
      "Step 6500 loss: 0.292865\n",
      "Step 6500 accuracy: 93.0%\n",
      "Step 7000 loss: 0.371496\n",
      "Step 7000 accuracy: 89.8%\n",
      "Step 7500 loss: 0.334815\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.196576\n",
      "Step 8000 accuracy: 94.5%\n",
      "Step 8500 loss: 0.272013\n",
      "Step 8500 accuracy: 93.8%\n",
      "Step 9000 loss: 0.267572\n",
      "Step 9000 accuracy: 94.5%\n",
      "Step 9500 loss: 0.299924\n",
      "Step 9500 accuracy: 90.6%\n",
      "Step 10000 loss: 0.537399\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 95.19\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.313456\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.651368\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.648447\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.554490\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.588531\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.586470\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.526107\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.439091\n",
      "Step 3500 accuracy: 90.6%\n",
      "Step 4000 loss: 0.318478\n",
      "Step 4000 accuracy: 90.6%\n",
      "Step 4500 loss: 0.483854\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.467653\n",
      "Step 5000 accuracy: 85.2%\n",
      "Step 5500 loss: 0.383883\n",
      "Step 5500 accuracy: 89.1%\n",
      "Step 6000 loss: 0.393776\n",
      "Step 6000 accuracy: 89.1%\n",
      "Step 6500 loss: 0.328981\n",
      "Step 6500 accuracy: 91.4%\n",
      "Step 7000 loss: 0.377010\n",
      "Step 7000 accuracy: 87.5%\n",
      "Step 7500 loss: 0.372819\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.219980\n",
      "Step 8000 accuracy: 94.5%\n",
      "Step 8500 loss: 0.302998\n",
      "Step 8500 accuracy: 90.6%\n",
      "Step 9000 loss: 0.317108\n",
      "Step 9000 accuracy: 92.2%\n",
      "Step 9500 loss: 0.366185\n",
      "Step 9500 accuracy: 87.5%\n",
      "Step 10000 loss: 0.543047\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 95.18\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.338310\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.711604\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.694289\n",
      "Step 1000 accuracy: 79.7%\n",
      "Step 1500 loss: 0.650672\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.683217\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.645711\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.543090\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.558796\n",
      "Step 3500 accuracy: 87.5%\n",
      "Step 4000 loss: 0.373314\n",
      "Step 4000 accuracy: 89.1%\n",
      "Step 4500 loss: 0.546666\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.494052\n",
      "Step 5000 accuracy: 86.7%\n",
      "Step 5500 loss: 0.472596\n",
      "Step 5500 accuracy: 87.5%\n",
      "Step 6000 loss: 0.545078\n",
      "Step 6000 accuracy: 85.9%\n",
      "Step 6500 loss: 0.359646\n",
      "Step 6500 accuracy: 89.8%\n",
      "Step 7000 loss: 0.463803\n",
      "Step 7000 accuracy: 84.4%\n",
      "Step 7500 loss: 0.506474\n",
      "Step 7500 accuracy: 83.6%\n",
      "Step 8000 loss: 0.283098\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.302124\n",
      "Step 8500 accuracy: 93.0%\n",
      "Step 9000 loss: 0.371170\n",
      "Step 9000 accuracy: 90.6%\n",
      "Step 9500 loss: 0.382251\n",
      "Step 9500 accuracy: 87.5%\n",
      "Step 10000 loss: 0.595918\n",
      "Step 10000 accuracy: 85.2%\n",
      "Accuracy in test dataset: 94.69\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.434541\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.779306\n",
      "Step 500 accuracy: 76.6%\n",
      "Step 1000 loss: 0.778376\n",
      "Step 1000 accuracy: 78.1%\n",
      "Step 1500 loss: 0.701316\n",
      "Step 1500 accuracy: 79.7%\n",
      "Step 2000 loss: 0.736955\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.827246\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.673088\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.582668\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.501409\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.595819\n",
      "Step 4500 accuracy: 81.2%\n",
      "Step 5000 loss: 0.601179\n",
      "Step 5000 accuracy: 82.0%\n",
      "Step 5500 loss: 0.526959\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.669417\n",
      "Step 6000 accuracy: 85.9%\n",
      "Step 6500 loss: 0.449104\n",
      "Step 6500 accuracy: 87.5%\n",
      "Step 7000 loss: 0.593544\n",
      "Step 7000 accuracy: 82.8%\n",
      "Step 7500 loss: 0.534875\n",
      "Step 7500 accuracy: 85.2%\n",
      "Step 8000 loss: 0.331599\n",
      "Step 8000 accuracy: 89.1%\n",
      "Step 8500 loss: 0.433670\n",
      "Step 8500 accuracy: 88.3%\n",
      "Step 9000 loss: 0.502963\n",
      "Step 9000 accuracy: 88.3%\n",
      "Step 9500 loss: 0.489199\n",
      "Step 9500 accuracy: 83.6%\n",
      "Step 10000 loss: 0.703539\n",
      "Step 10000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.75\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.650239\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.904640\n",
      "Step 500 accuracy: 76.6%\n",
      "Step 1000 loss: 0.948603\n",
      "Step 1000 accuracy: 72.7%\n",
      "Step 1500 loss: 0.759882\n",
      "Step 1500 accuracy: 78.9%\n",
      "Step 2000 loss: 0.774070\n",
      "Step 2000 accuracy: 78.1%\n",
      "Step 2500 loss: 0.774143\n",
      "Step 2500 accuracy: 78.1%\n",
      "Step 3000 loss: 0.672773\n",
      "Step 3000 accuracy: 78.1%\n",
      "Step 3500 loss: 0.702287\n",
      "Step 3500 accuracy: 81.2%\n",
      "Step 4000 loss: 0.575206\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.777108\n",
      "Step 4500 accuracy: 75.0%\n",
      "Step 5000 loss: 0.622953\n",
      "Step 5000 accuracy: 82.0%\n",
      "Step 5500 loss: 0.674724\n",
      "Step 5500 accuracy: 81.2%\n",
      "Step 6000 loss: 0.715250\n",
      "Step 6000 accuracy: 79.7%\n",
      "Step 6500 loss: 0.586398\n",
      "Step 6500 accuracy: 83.6%\n",
      "Step 7000 loss: 0.613640\n",
      "Step 7000 accuracy: 79.7%\n",
      "Step 7500 loss: 0.665785\n",
      "Step 7500 accuracy: 75.0%\n",
      "Step 8000 loss: 0.457542\n",
      "Step 8000 accuracy: 79.7%\n",
      "Step 8500 loss: 0.428920\n",
      "Step 8500 accuracy: 86.7%\n",
      "Step 9000 loss: 0.623975\n",
      "Step 9000 accuracy: 82.0%\n",
      "Step 9500 loss: 0.582422\n",
      "Step 9500 accuracy: 80.5%\n",
      "Step 10000 loss: 0.679403\n",
      "Step 10000 accuracy: 81.2%\n",
      "Accuracy in test dataset: 92.65\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.398081\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.629586\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.630501\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.525915\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.551924\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.542366\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.453691\n",
      "Step 3000 accuracy: 86.7%\n",
      "Step 3500 loss: 0.398074\n",
      "Step 3500 accuracy: 89.8%\n",
      "Step 4000 loss: 0.290861\n",
      "Step 4000 accuracy: 90.6%\n",
      "Step 4500 loss: 0.440030\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.401983\n",
      "Step 5000 accuracy: 88.3%\n",
      "Step 5500 loss: 0.334856\n",
      "Step 5500 accuracy: 90.6%\n",
      "Step 6000 loss: 0.355420\n",
      "Step 6000 accuracy: 89.1%\n",
      "Step 6500 loss: 0.276777\n",
      "Step 6500 accuracy: 92.2%\n",
      "Step 7000 loss: 0.360767\n",
      "Step 7000 accuracy: 90.6%\n",
      "Step 7500 loss: 0.306882\n",
      "Step 7500 accuracy: 89.8%\n",
      "Step 8000 loss: 0.181693\n",
      "Step 8000 accuracy: 94.5%\n",
      "Step 8500 loss: 0.254735\n",
      "Step 8500 accuracy: 92.2%\n",
      "Step 9000 loss: 0.246497\n",
      "Step 9000 accuracy: 94.5%\n",
      "Step 9500 loss: 0.242539\n",
      "Step 9500 accuracy: 92.2%\n",
      "Step 10000 loss: 0.494935\n",
      "Step 10000 accuracy: 86.7%\n",
      "Accuracy in test dataset: 95.16\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.367167\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.649795\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.618291\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.515797\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.582987\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.532709\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.462299\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.377089\n",
      "Step 3500 accuracy: 90.6%\n",
      "Step 4000 loss: 0.305123\n",
      "Step 4000 accuracy: 92.2%\n",
      "Step 4500 loss: 0.467713\n",
      "Step 4500 accuracy: 87.5%\n",
      "Step 5000 loss: 0.434918\n",
      "Step 5000 accuracy: 87.5%\n",
      "Step 5500 loss: 0.367403\n",
      "Step 5500 accuracy: 90.6%\n",
      "Step 6000 loss: 0.381024\n",
      "Step 6000 accuracy: 89.1%\n",
      "Step 6500 loss: 0.311420\n",
      "Step 6500 accuracy: 91.4%\n",
      "Step 7000 loss: 0.385134\n",
      "Step 7000 accuracy: 89.1%\n",
      "Step 7500 loss: 0.382208\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.216626\n",
      "Step 8000 accuracy: 93.8%\n",
      "Step 8500 loss: 0.291682\n",
      "Step 8500 accuracy: 91.4%\n",
      "Step 9000 loss: 0.302556\n",
      "Step 9000 accuracy: 93.0%\n",
      "Step 9500 loss: 0.324000\n",
      "Step 9500 accuracy: 88.3%\n",
      "Step 10000 loss: 0.526790\n",
      "Step 10000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 95.25\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.321346\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.675526\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.686688\n",
      "Step 1000 accuracy: 78.9%\n",
      "Step 1500 loss: 0.612735\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.598261\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.619873\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.474680\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.496922\n",
      "Step 3500 accuracy: 89.8%\n",
      "Step 4000 loss: 0.401382\n",
      "Step 4000 accuracy: 87.5%\n",
      "Step 4500 loss: 0.468274\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.535021\n",
      "Step 5000 accuracy: 82.0%\n",
      "Step 5500 loss: 0.438308\n",
      "Step 5500 accuracy: 88.3%\n",
      "Step 6000 loss: 0.479887\n",
      "Step 6000 accuracy: 88.3%\n",
      "Step 6500 loss: 0.324870\n",
      "Step 6500 accuracy: 90.6%\n",
      "Step 7000 loss: 0.468132\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.504124\n",
      "Step 7500 accuracy: 87.5%\n",
      "Step 8000 loss: 0.224383\n",
      "Step 8000 accuracy: 93.0%\n",
      "Step 8500 loss: 0.314214\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.360427\n",
      "Step 9000 accuracy: 90.6%\n",
      "Step 9500 loss: 0.371555\n",
      "Step 9500 accuracy: 89.1%\n",
      "Step 10000 loss: 0.507824\n",
      "Step 10000 accuracy: 85.9%\n",
      "Accuracy in test dataset: 94.96\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.487179\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.718666\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.679098\n",
      "Step 1000 accuracy: 77.3%\n",
      "Step 1500 loss: 0.669081\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.655066\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.758848\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.678071\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.528088\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.423620\n",
      "Step 4000 accuracy: 86.7%\n",
      "Step 4500 loss: 0.582753\n",
      "Step 4500 accuracy: 81.2%\n",
      "Step 5000 loss: 0.541230\n",
      "Step 5000 accuracy: 84.4%\n",
      "Step 5500 loss: 0.559326\n",
      "Step 5500 accuracy: 84.4%\n",
      "Step 6000 loss: 0.543573\n",
      "Step 6000 accuracy: 86.7%\n",
      "Step 6500 loss: 0.462024\n",
      "Step 6500 accuracy: 85.9%\n",
      "Step 7000 loss: 0.562432\n",
      "Step 7000 accuracy: 85.2%\n",
      "Step 7500 loss: 0.574362\n",
      "Step 7500 accuracy: 82.0%\n",
      "Step 8000 loss: 0.351890\n",
      "Step 8000 accuracy: 90.6%\n",
      "Step 8500 loss: 0.441927\n",
      "Step 8500 accuracy: 84.4%\n",
      "Step 9000 loss: 0.409502\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.521146\n",
      "Step 9500 accuracy: 85.2%\n",
      "Step 10000 loss: 0.669133\n",
      "Step 10000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 94.31\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 128 | over_fit : False |\n",
      "Step 0 loss: 2.521565\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.893724\n",
      "Step 500 accuracy: 75.0%\n",
      "Step 1000 loss: 0.823920\n",
      "Step 1000 accuracy: 74.2%\n",
      "Step 1500 loss: 0.671848\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.866335\n",
      "Step 2000 accuracy: 76.6%\n",
      "Step 2500 loss: 0.792843\n",
      "Step 2500 accuracy: 80.5%\n",
      "Step 3000 loss: 0.785744\n",
      "Step 3000 accuracy: 78.9%\n",
      "Step 3500 loss: 0.698667\n",
      "Step 3500 accuracy: 79.7%\n",
      "Step 4000 loss: 0.548723\n",
      "Step 4000 accuracy: 86.7%\n",
      "Step 4500 loss: 0.656666\n",
      "Step 4500 accuracy: 79.7%\n",
      "Step 5000 loss: 0.870989\n",
      "Step 5000 accuracy: 78.1%\n",
      "Step 5500 loss: 0.645562\n",
      "Step 5500 accuracy: 83.6%\n",
      "Step 6000 loss: 0.608768\n",
      "Step 6000 accuracy: 84.4%\n",
      "Step 6500 loss: 0.570540\n",
      "Step 6500 accuracy: 85.2%\n",
      "Step 7000 loss: 0.620154\n",
      "Step 7000 accuracy: 79.7%\n",
      "Step 7500 loss: 0.720281\n",
      "Step 7500 accuracy: 76.6%\n",
      "Step 8000 loss: 0.388551\n",
      "Step 8000 accuracy: 88.3%\n",
      "Step 8500 loss: 0.498398\n",
      "Step 8500 accuracy: 85.9%\n",
      "Step 9000 loss: 0.540265\n",
      "Step 9000 accuracy: 84.4%\n",
      "Step 9500 loss: 0.572817\n",
      "Step 9500 accuracy: 81.2%\n",
      "Step 10000 loss: 0.607918\n",
      "Step 10000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 93.13\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.296302\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.595585\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.560625\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.562191\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.477170\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.525298\n",
      "Step 2500 accuracy: 86.7%\n",
      "Accuracy in test dataset: 92.92\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.342427\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.631767\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.602588\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.566476\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.511578\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.563872\n",
      "Step 2500 accuracy: 85.5%\n",
      "Accuracy in test dataset: 92.66\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.440536\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.719695\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.654639\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.702725\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.544067\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.644338\n",
      "Step 2500 accuracy: 82.4%\n",
      "Accuracy in test dataset: 92.0\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.464461\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.760691\n",
      "Step 500 accuracy: 77.7%\n",
      "Step 1000 loss: 0.748976\n",
      "Step 1000 accuracy: 80.1%\n",
      "Step 1500 loss: 0.773117\n",
      "Step 1500 accuracy: 80.1%\n",
      "Step 2000 loss: 0.628312\n",
      "Step 2000 accuracy: 81.6%\n",
      "Step 2500 loss: 0.719989\n",
      "Step 2500 accuracy: 77.3%\n",
      "Accuracy in test dataset: 91.26\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.503203\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.934326\n",
      "Step 500 accuracy: 71.5%\n",
      "Step 1000 loss: 0.898741\n",
      "Step 1000 accuracy: 74.6%\n",
      "Step 1500 loss: 0.904275\n",
      "Step 1500 accuracy: 77.7%\n",
      "Step 2000 loss: 0.733129\n",
      "Step 2000 accuracy: 77.3%\n",
      "Step 2500 loss: 0.971179\n",
      "Step 2500 accuracy: 72.3%\n",
      "Accuracy in test dataset: 90.54\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.301300\n",
      "Step 0 accuracy: 13.7%\n",
      "Step 500 loss: 0.587512\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.563138\n",
      "Step 1000 accuracy: 85.5%\n",
      "Step 1500 loss: 0.562171\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.446651\n",
      "Step 2000 accuracy: 86.3%\n",
      "Step 2500 loss: 0.511886\n",
      "Step 2500 accuracy: 86.3%\n",
      "Accuracy in test dataset: 93.2\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.334779\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.606990\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.565865\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.562586\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.487721\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.533427\n",
      "Step 2500 accuracy: 85.5%\n",
      "Accuracy in test dataset: 93.15\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.444541\n",
      "Step 0 accuracy: 5.1%\n",
      "Step 500 loss: 0.679606\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.663858\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.658057\n",
      "Step 1500 accuracy: 81.6%\n",
      "Step 2000 loss: 0.528430\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.637476\n",
      "Step 2500 accuracy: 80.9%\n",
      "Accuracy in test dataset: 92.46\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.394565\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.718530\n",
      "Step 500 accuracy: 77.3%\n",
      "Step 1000 loss: 0.715181\n",
      "Step 1000 accuracy: 81.6%\n",
      "Step 1500 loss: 0.735356\n",
      "Step 1500 accuracy: 81.6%\n",
      "Step 2000 loss: 0.580652\n",
      "Step 2000 accuracy: 82.4%\n",
      "Step 2500 loss: 0.715248\n",
      "Step 2500 accuracy: 81.2%\n",
      "Accuracy in test dataset: 91.57\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.604970\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.793756\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.842331\n",
      "Step 1000 accuracy: 79.3%\n",
      "Step 1500 loss: 0.753892\n",
      "Step 1500 accuracy: 80.5%\n",
      "Step 2000 loss: 0.725342\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.778075\n",
      "Step 2500 accuracy: 75.4%\n",
      "Accuracy in test dataset: 91.03\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.321716\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.604571\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.555759\n",
      "Step 1000 accuracy: 85.5%\n",
      "Step 1500 loss: 0.558967\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.459543\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.499123\n",
      "Step 2500 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.49\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.320074\n",
      "Step 0 accuracy: 14.5%\n",
      "Step 500 loss: 0.605084\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.569295\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.566091\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.469119\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.539026\n",
      "Step 2500 accuracy: 86.3%\n",
      "Accuracy in test dataset: 93.15\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.313429\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.639370\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.636761\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.612704\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.523052\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.615825\n",
      "Step 2500 accuracy: 83.2%\n",
      "Accuracy in test dataset: 92.75\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.480467\n",
      "Step 0 accuracy: 6.6%\n",
      "Step 500 loss: 0.698673\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.691310\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.660939\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.577464\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.678152\n",
      "Step 2500 accuracy: 80.5%\n",
      "Accuracy in test dataset: 92.17\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.543708\n",
      "Step 0 accuracy: 9.8%\n",
      "Step 500 loss: 0.758353\n",
      "Step 500 accuracy: 76.2%\n",
      "Step 1000 loss: 0.758485\n",
      "Step 1000 accuracy: 79.3%\n",
      "Step 1500 loss: 0.775161\n",
      "Step 1500 accuracy: 81.2%\n",
      "Step 2000 loss: 0.655918\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.762668\n",
      "Step 2500 accuracy: 78.5%\n",
      "Accuracy in test dataset: 91.46\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.345669\n",
      "Step 0 accuracy: 6.6%\n",
      "Step 500 loss: 0.586748\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.541773\n",
      "Step 1000 accuracy: 85.5%\n",
      "Step 1500 loss: 0.529733\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.437493\n",
      "Step 2000 accuracy: 86.7%\n",
      "Step 2500 loss: 0.482389\n",
      "Step 2500 accuracy: 86.7%\n",
      "Accuracy in test dataset: 93.54\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.360334\n",
      "Step 0 accuracy: 7.4%\n",
      "Step 500 loss: 0.598865\n",
      "Step 500 accuracy: 83.6%\n",
      "Step 1000 loss: 0.544165\n",
      "Step 1000 accuracy: 85.9%\n",
      "Step 1500 loss: 0.555102\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.458365\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.506334\n",
      "Step 2500 accuracy: 87.1%\n",
      "Accuracy in test dataset: 93.51\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.401288\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.616205\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.566951\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.580620\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.486908\n",
      "Step 2000 accuracy: 84.8%\n",
      "Step 2500 loss: 0.511397\n",
      "Step 2500 accuracy: 85.5%\n",
      "Accuracy in test dataset: 93.4\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.492882\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.669846\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.664124\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.637678\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.540003\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.656431\n",
      "Step 2500 accuracy: 83.2%\n",
      "Accuracy in test dataset: 92.58\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 2560 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.550547\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.776653\n",
      "Step 500 accuracy: 75.8%\n",
      "Step 1000 loss: 0.775490\n",
      "Step 1000 accuracy: 80.9%\n",
      "Step 1500 loss: 0.698081\n",
      "Step 1500 accuracy: 81.6%\n",
      "Step 2000 loss: 0.589998\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.700306\n",
      "Step 2500 accuracy: 81.2%\n",
      "Accuracy in test dataset: 91.89\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.347418\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.602070\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.572817\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.577106\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.465469\n",
      "Step 2000 accuracy: 86.3%\n",
      "Step 2500 loss: 0.534801\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.421845\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.342556\n",
      "Step 3500 accuracy: 89.5%\n",
      "Step 4000 loss: 0.541386\n",
      "Step 4000 accuracy: 82.4%\n",
      "Step 4500 loss: 0.355871\n",
      "Step 4500 accuracy: 90.2%\n",
      "Step 5000 loss: 0.408934\n",
      "Step 5000 accuracy: 87.9%\n",
      "Accuracy in test dataset: 94.18\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.321760\n",
      "Step 0 accuracy: 12.9%\n",
      "Step 500 loss: 0.619066\n",
      "Step 500 accuracy: 82.4%\n",
      "Step 1000 loss: 0.605520\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.579559\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.497262\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.584977\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.456026\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.354335\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.558828\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.390423\n",
      "Step 4500 accuracy: 89.1%\n",
      "Step 5000 loss: 0.451893\n",
      "Step 5000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 93.91\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.318954\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.725144\n",
      "Step 500 accuracy: 78.1%\n",
      "Step 1000 loss: 0.634464\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.619573\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.534055\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.653584\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.543800\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.443233\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.655715\n",
      "Step 4000 accuracy: 81.2%\n",
      "Step 4500 loss: 0.499908\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.559154\n",
      "Step 5000 accuracy: 84.0%\n",
      "Accuracy in test dataset: 93.03\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.419692\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.714665\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.739404\n",
      "Step 1000 accuracy: 80.1%\n",
      "Step 1500 loss: 0.829013\n",
      "Step 1500 accuracy: 78.5%\n",
      "Step 2000 loss: 0.615377\n",
      "Step 2000 accuracy: 80.5%\n",
      "Step 2500 loss: 0.710867\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.610357\n",
      "Step 3000 accuracy: 83.2%\n",
      "Step 3500 loss: 0.523559\n",
      "Step 3500 accuracy: 84.0%\n",
      "Step 4000 loss: 0.742604\n",
      "Step 4000 accuracy: 76.2%\n",
      "Step 4500 loss: 0.590085\n",
      "Step 4500 accuracy: 84.0%\n",
      "Step 5000 loss: 0.663008\n",
      "Step 5000 accuracy: 83.2%\n",
      "Accuracy in test dataset: 91.74\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.582187\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.988669\n",
      "Step 500 accuracy: 71.5%\n",
      "Step 1000 loss: 0.883801\n",
      "Step 1000 accuracy: 76.2%\n",
      "Step 1500 loss: 0.888071\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.744559\n",
      "Step 2000 accuracy: 77.7%\n",
      "Step 2500 loss: 1.024598\n",
      "Step 2500 accuracy: 72.7%\n",
      "Step 3000 loss: 0.813257\n",
      "Step 3000 accuracy: 78.5%\n",
      "Step 3500 loss: 0.639004\n",
      "Step 3500 accuracy: 82.0%\n",
      "Step 4000 loss: 0.887643\n",
      "Step 4000 accuracy: 72.3%\n",
      "Step 4500 loss: 0.813681\n",
      "Step 4500 accuracy: 74.6%\n",
      "Step 5000 loss: 0.905555\n",
      "Step 5000 accuracy: 75.0%\n",
      "Accuracy in test dataset: 90.73\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.309239\n",
      "Step 0 accuracy: 10.9%\n",
      "Step 500 loss: 0.602067\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.570851\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.564987\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.460819\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.517162\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.419710\n",
      "Step 3000 accuracy: 87.9%\n",
      "Step 3500 loss: 0.331286\n",
      "Step 3500 accuracy: 89.8%\n",
      "Step 4000 loss: 0.534485\n",
      "Step 4000 accuracy: 81.6%\n",
      "Step 4500 loss: 0.347248\n",
      "Step 4500 accuracy: 90.6%\n",
      "Step 5000 loss: 0.396347\n",
      "Step 5000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 94.25\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.315846\n",
      "Step 0 accuracy: 10.5%\n",
      "Step 500 loss: 0.590405\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.585099\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.576340\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.483408\n",
      "Step 2000 accuracy: 84.4%\n",
      "Step 2500 loss: 0.532277\n",
      "Step 2500 accuracy: 86.7%\n",
      "Step 3000 loss: 0.436778\n",
      "Step 3000 accuracy: 87.9%\n",
      "Step 3500 loss: 0.346262\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.574342\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.372865\n",
      "Step 4500 accuracy: 89.8%\n",
      "Step 5000 loss: 0.442186\n",
      "Step 5000 accuracy: 87.5%\n",
      "Accuracy in test dataset: 94.38\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.317887\n",
      "Step 0 accuracy: 14.5%\n",
      "Step 500 loss: 0.642642\n",
      "Step 500 accuracy: 79.3%\n",
      "Step 1000 loss: 0.633009\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.618973\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.577829\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.622997\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.491218\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.426512\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.597053\n",
      "Step 4000 accuracy: 81.6%\n",
      "Step 4500 loss: 0.469977\n",
      "Step 4500 accuracy: 86.3%\n",
      "Step 5000 loss: 0.540822\n",
      "Step 5000 accuracy: 83.6%\n",
      "Accuracy in test dataset: 93.54\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.373784\n",
      "Step 0 accuracy: 11.7%\n",
      "Step 500 loss: 0.708608\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.714833\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.675937\n",
      "Step 1500 accuracy: 82.0%\n",
      "Step 2000 loss: 0.589280\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.732946\n",
      "Step 2500 accuracy: 78.5%\n",
      "Step 3000 loss: 0.572021\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.450436\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.743872\n",
      "Step 4000 accuracy: 78.1%\n",
      "Step 4500 loss: 0.492938\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.599258\n",
      "Step 5000 accuracy: 84.4%\n",
      "Accuracy in test dataset: 92.57\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.667819\n",
      "Step 0 accuracy: 6.2%\n",
      "Step 500 loss: 0.802818\n",
      "Step 500 accuracy: 76.6%\n",
      "Step 1000 loss: 0.788661\n",
      "Step 1000 accuracy: 80.5%\n",
      "Step 1500 loss: 0.751169\n",
      "Step 1500 accuracy: 80.9%\n",
      "Step 2000 loss: 0.700746\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.856206\n",
      "Step 2500 accuracy: 76.6%\n",
      "Step 3000 loss: 0.652963\n",
      "Step 3000 accuracy: 81.6%\n",
      "Step 3500 loss: 0.540451\n",
      "Step 3500 accuracy: 84.8%\n",
      "Step 4000 loss: 0.819897\n",
      "Step 4000 accuracy: 78.9%\n",
      "Step 4500 loss: 0.608014\n",
      "Step 4500 accuracy: 82.4%\n",
      "Step 5000 loss: 0.684171\n",
      "Step 5000 accuracy: 79.7%\n",
      "Accuracy in test dataset: 91.67\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.298513\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.590330\n",
      "Step 500 accuracy: 83.6%\n",
      "Step 1000 loss: 0.560332\n",
      "Step 1000 accuracy: 85.9%\n",
      "Step 1500 loss: 0.556808\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.457543\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.497684\n",
      "Step 2500 accuracy: 87.5%\n",
      "Step 3000 loss: 0.414981\n",
      "Step 3000 accuracy: 87.9%\n",
      "Step 3500 loss: 0.337542\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.511502\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.347310\n",
      "Step 4500 accuracy: 89.8%\n",
      "Step 5000 loss: 0.388584\n",
      "Step 5000 accuracy: 87.9%\n",
      "Accuracy in test dataset: 94.52\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.299180\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.624367\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.573878\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.553238\n",
      "Step 1500 accuracy: 84.8%\n",
      "Step 2000 loss: 0.473405\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.537197\n",
      "Step 2500 accuracy: 86.3%\n",
      "Step 3000 loss: 0.441145\n",
      "Step 3000 accuracy: 88.7%\n",
      "Step 3500 loss: 0.352249\n",
      "Step 3500 accuracy: 87.9%\n",
      "Step 4000 loss: 0.535050\n",
      "Step 4000 accuracy: 83.2%\n",
      "Step 4500 loss: 0.378915\n",
      "Step 4500 accuracy: 89.8%\n",
      "Step 5000 loss: 0.414076\n",
      "Step 5000 accuracy: 87.9%\n",
      "Accuracy in test dataset: 94.31\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.365924\n",
      "Step 0 accuracy: 11.3%\n",
      "Step 500 loss: 0.641084\n",
      "Step 500 accuracy: 79.3%\n",
      "Step 1000 loss: 0.617801\n",
      "Step 1000 accuracy: 83.6%\n",
      "Step 1500 loss: 0.599428\n",
      "Step 1500 accuracy: 83.2%\n",
      "Step 2000 loss: 0.513955\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.618123\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.490764\n",
      "Step 3000 accuracy: 85.9%\n",
      "Step 3500 loss: 0.395380\n",
      "Step 3500 accuracy: 86.7%\n",
      "Step 4000 loss: 0.606298\n",
      "Step 4000 accuracy: 80.9%\n",
      "Step 4500 loss: 0.425531\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.488709\n",
      "Step 5000 accuracy: 86.3%\n",
      "Accuracy in test dataset: 93.93\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.409971\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.704759\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.684866\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.708125\n",
      "Step 1500 accuracy: 80.1%\n",
      "Step 2000 loss: 0.548170\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.642025\n",
      "Step 2500 accuracy: 81.2%\n",
      "Step 3000 loss: 0.546200\n",
      "Step 3000 accuracy: 85.2%\n",
      "Step 3500 loss: 0.444817\n",
      "Step 3500 accuracy: 85.9%\n",
      "Step 4000 loss: 0.666135\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.480911\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.574141\n",
      "Step 5000 accuracy: 84.0%\n",
      "Accuracy in test dataset: 93.05\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.659431\n",
      "Step 0 accuracy: 8.6%\n",
      "Step 500 loss: 0.761181\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.728716\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.728934\n",
      "Step 1500 accuracy: 78.5%\n",
      "Step 2000 loss: 0.661249\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.817462\n",
      "Step 2500 accuracy: 79.7%\n",
      "Step 3000 loss: 0.666836\n",
      "Step 3000 accuracy: 82.4%\n",
      "Step 3500 loss: 0.504620\n",
      "Step 3500 accuracy: 85.2%\n",
      "Step 4000 loss: 0.828697\n",
      "Step 4000 accuracy: 74.2%\n",
      "Step 4500 loss: 0.590664\n",
      "Step 4500 accuracy: 82.4%\n",
      "Step 5000 loss: 0.623462\n",
      "Step 5000 accuracy: 83.2%\n",
      "Accuracy in test dataset: 92.16\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.341628\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.580117\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.540979\n",
      "Step 1000 accuracy: 86.3%\n",
      "Step 1500 loss: 0.532016\n",
      "Step 1500 accuracy: 85.5%\n",
      "Step 2000 loss: 0.457472\n",
      "Step 2000 accuracy: 86.7%\n",
      "Step 2500 loss: 0.504449\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.393394\n",
      "Step 3000 accuracy: 89.1%\n",
      "Step 3500 loss: 0.308345\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.509027\n",
      "Step 4000 accuracy: 82.8%\n",
      "Step 4500 loss: 0.339898\n",
      "Step 4500 accuracy: 91.0%\n",
      "Step 5000 loss: 0.371384\n",
      "Step 5000 accuracy: 89.5%\n",
      "Accuracy in test dataset: 94.76\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.307416\n",
      "Step 0 accuracy: 13.3%\n",
      "Step 500 loss: 0.589759\n",
      "Step 500 accuracy: 82.8%\n",
      "Step 1000 loss: 0.561932\n",
      "Step 1000 accuracy: 85.2%\n",
      "Step 1500 loss: 0.560320\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.469466\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.509781\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.411258\n",
      "Step 3000 accuracy: 88.3%\n",
      "Step 3500 loss: 0.338236\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.522140\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.360832\n",
      "Step 4500 accuracy: 89.5%\n",
      "Step 5000 loss: 0.404977\n",
      "Step 5000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 94.58\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.364281\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.652724\n",
      "Step 500 accuracy: 81.2%\n",
      "Step 1000 loss: 0.626420\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.591595\n",
      "Step 1500 accuracy: 84.0%\n",
      "Step 2000 loss: 0.484501\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.546825\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.452921\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.377176\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.590823\n",
      "Step 4000 accuracy: 80.1%\n",
      "Step 4500 loss: 0.408850\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.467900\n",
      "Step 5000 accuracy: 85.5%\n",
      "Accuracy in test dataset: 94.23\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.420056\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.667312\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.709750\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.614207\n",
      "Step 1500 accuracy: 81.6%\n",
      "Step 2000 loss: 0.568072\n",
      "Step 2000 accuracy: 84.0%\n",
      "Step 2500 loss: 0.611887\n",
      "Step 2500 accuracy: 83.6%\n",
      "Step 3000 loss: 0.517251\n",
      "Step 3000 accuracy: 86.3%\n",
      "Step 3500 loss: 0.390672\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.621083\n",
      "Step 4000 accuracy: 79.3%\n",
      "Step 4500 loss: 0.481043\n",
      "Step 4500 accuracy: 84.0%\n",
      "Step 5000 loss: 0.566246\n",
      "Step 5000 accuracy: 83.2%\n",
      "Accuracy in test dataset: 93.48\n",
      "==============================================\n",
      "hidden_layers_cf: [2048] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 5120 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.710508\n",
      "Step 0 accuracy: 4.3%\n",
      "Step 500 loss: 0.693143\n",
      "Step 500 accuracy: 79.3%\n",
      "Step 1000 loss: 0.719578\n",
      "Step 1000 accuracy: 82.0%\n",
      "Step 1500 loss: 0.681584\n",
      "Step 1500 accuracy: 80.9%\n",
      "Step 2000 loss: 0.576604\n",
      "Step 2000 accuracy: 83.2%\n",
      "Step 2500 loss: 0.720894\n",
      "Step 2500 accuracy: 78.5%\n",
      "Step 3000 loss: 0.613812\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.460557\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.786205\n",
      "Step 4000 accuracy: 75.8%\n",
      "Step 4500 loss: 0.571701\n",
      "Step 4500 accuracy: 82.8%\n",
      "Step 5000 loss: 0.627444\n",
      "Step 5000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.68\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.311480\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.595711\n",
      "Step 500 accuracy: 80.5%\n",
      "Step 1000 loss: 0.559067\n",
      "Step 1000 accuracy: 85.5%\n",
      "Step 1500 loss: 0.566691\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.480173\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.516575\n",
      "Step 2500 accuracy: 84.4%\n",
      "Step 3000 loss: 0.425760\n",
      "Step 3000 accuracy: 88.7%\n",
      "Step 3500 loss: 0.331855\n",
      "Step 3500 accuracy: 89.5%\n",
      "Step 4000 loss: 0.558047\n",
      "Step 4000 accuracy: 81.6%\n",
      "Step 4500 loss: 0.355360\n",
      "Step 4500 accuracy: 91.4%\n",
      "Step 5000 loss: 0.408896\n",
      "Step 5000 accuracy: 87.1%\n",
      "Step 5500 loss: 0.465326\n",
      "Step 5500 accuracy: 87.1%\n",
      "Step 6000 loss: 0.378338\n",
      "Step 6000 accuracy: 89.8%\n",
      "Step 6500 loss: 0.401286\n",
      "Step 6500 accuracy: 89.5%\n",
      "Step 7000 loss: 0.260314\n",
      "Step 7000 accuracy: 93.8%\n",
      "Step 7500 loss: 0.336235\n",
      "Step 7500 accuracy: 91.0%\n",
      "Step 8000 loss: 0.291703\n",
      "Step 8000 accuracy: 91.8%\n",
      "Step 8500 loss: 0.356177\n",
      "Step 8500 accuracy: 90.2%\n",
      "Step 9000 loss: 0.385494\n",
      "Step 9000 accuracy: 87.9%\n",
      "Step 9500 loss: 0.322380\n",
      "Step 9500 accuracy: 91.0%\n",
      "Step 10000 loss: 0.296270\n",
      "Step 10000 accuracy: 93.4%\n",
      "Accuracy in test dataset: 94.81\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.360832\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.642801\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.586197\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.580308\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.514689\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.577010\n",
      "Step 2500 accuracy: 82.8%\n",
      "Step 3000 loss: 0.461069\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.356263\n",
      "Step 3500 accuracy: 89.5%\n",
      "Step 4000 loss: 0.616915\n",
      "Step 4000 accuracy: 80.5%\n",
      "Step 4500 loss: 0.388488\n",
      "Step 4500 accuracy: 89.8%\n",
      "Step 5000 loss: 0.482268\n",
      "Step 5000 accuracy: 86.3%\n",
      "Step 5500 loss: 0.502490\n",
      "Step 5500 accuracy: 85.2%\n",
      "Step 6000 loss: 0.457609\n",
      "Step 6000 accuracy: 86.3%\n",
      "Step 6500 loss: 0.471869\n",
      "Step 6500 accuracy: 87.1%\n",
      "Step 7000 loss: 0.322089\n",
      "Step 7000 accuracy: 90.6%\n",
      "Step 7500 loss: 0.376930\n",
      "Step 7500 accuracy: 88.3%\n",
      "Step 8000 loss: 0.333022\n",
      "Step 8000 accuracy: 91.0%\n",
      "Step 8500 loss: 0.388056\n",
      "Step 8500 accuracy: 87.9%\n",
      "Step 9000 loss: 0.461718\n",
      "Step 9000 accuracy: 86.7%\n",
      "Step 9500 loss: 0.376889\n",
      "Step 9500 accuracy: 88.7%\n",
      "Step 10000 loss: 0.329783\n",
      "Step 10000 accuracy: 90.2%\n",
      "Accuracy in test dataset: 94.84\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.363010\n",
      "Step 0 accuracy: 10.2%\n",
      "Step 500 loss: 0.695998\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.653104\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.698795\n",
      "Step 1500 accuracy: 82.8%\n",
      "Step 2000 loss: 0.587517\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.675017\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.517613\n",
      "Step 3000 accuracy: 83.6%\n",
      "Step 3500 loss: 0.421974\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.656689\n",
      "Step 4000 accuracy: 77.7%\n",
      "Step 4500 loss: 0.516167\n",
      "Step 4500 accuracy: 85.2%\n",
      "Step 5000 loss: 0.572616\n",
      "Step 5000 accuracy: 84.0%\n",
      "Step 5500 loss: 0.600640\n",
      "Step 5500 accuracy: 80.5%\n",
      "Step 6000 loss: 0.566112\n",
      "Step 6000 accuracy: 85.2%\n",
      "Step 6500 loss: 0.549354\n",
      "Step 6500 accuracy: 84.8%\n",
      "Step 7000 loss: 0.338150\n",
      "Step 7000 accuracy: 89.5%\n",
      "Step 7500 loss: 0.457280\n",
      "Step 7500 accuracy: 86.7%\n",
      "Step 8000 loss: 0.435277\n",
      "Step 8000 accuracy: 87.1%\n",
      "Step 8500 loss: 0.526130\n",
      "Step 8500 accuracy: 83.6%\n",
      "Step 9000 loss: 0.574597\n",
      "Step 9000 accuracy: 83.2%\n",
      "Step 9500 loss: 0.464202\n",
      "Step 9500 accuracy: 85.9%\n",
      "Step 10000 loss: 0.388824\n",
      "Step 10000 accuracy: 90.6%\n",
      "Accuracy in test dataset: 93.6\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.453102\n",
      "Step 0 accuracy: 12.1%\n",
      "Step 500 loss: 0.725212\n",
      "Step 500 accuracy: 79.3%\n",
      "Step 1000 loss: 0.736512\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.752204\n",
      "Step 1500 accuracy: 78.1%\n",
      "Step 2000 loss: 0.635861\n",
      "Step 2000 accuracy: 81.2%\n",
      "Step 2500 loss: 0.777746\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.615668\n",
      "Step 3000 accuracy: 83.2%\n",
      "Step 3500 loss: 0.505163\n",
      "Step 3500 accuracy: 83.6%\n",
      "Step 4000 loss: 0.834607\n",
      "Step 4000 accuracy: 73.8%\n",
      "Step 4500 loss: 0.540255\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.750786\n",
      "Step 5000 accuracy: 81.2%\n",
      "Step 5500 loss: 0.692414\n",
      "Step 5500 accuracy: 78.9%\n",
      "Step 6000 loss: 0.654110\n",
      "Step 6000 accuracy: 80.1%\n",
      "Step 6500 loss: 0.660358\n",
      "Step 6500 accuracy: 79.7%\n",
      "Step 7000 loss: 0.611242\n",
      "Step 7000 accuracy: 83.2%\n",
      "Step 7500 loss: 0.603727\n",
      "Step 7500 accuracy: 84.0%\n",
      "Step 8000 loss: 0.584775\n",
      "Step 8000 accuracy: 82.8%\n",
      "Step 8500 loss: 0.607836\n",
      "Step 8500 accuracy: 80.9%\n",
      "Step 9000 loss: 0.678999\n",
      "Step 9000 accuracy: 82.4%\n",
      "Step 9500 loss: 0.545725\n",
      "Step 9500 accuracy: 87.1%\n",
      "Step 10000 loss: 0.525502\n",
      "Step 10000 accuracy: 84.0%\n",
      "Accuracy in test dataset: 92.41\n",
      "==============================================\n",
      "hidden_layers_cf: [256] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.653485\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.969150\n",
      "Step 500 accuracy: 70.3%\n",
      "Step 1000 loss: 0.797314\n",
      "Step 1000 accuracy: 77.7%\n",
      "Step 1500 loss: 0.919761\n",
      "Step 1500 accuracy: 74.6%\n",
      "Step 2000 loss: 0.767300\n",
      "Step 2000 accuracy: 77.7%\n",
      "Step 2500 loss: 0.954226\n",
      "Step 2500 accuracy: 77.3%\n",
      "Step 3000 loss: 0.774210\n",
      "Step 3000 accuracy: 78.1%\n",
      "Step 3500 loss: 0.689604\n",
      "Step 3500 accuracy: 77.0%\n",
      "Step 4000 loss: 0.818379\n",
      "Step 4000 accuracy: 73.8%\n",
      "Step 4500 loss: 0.699487\n",
      "Step 4500 accuracy: 78.5%\n",
      "Step 5000 loss: 0.805236\n",
      "Step 5000 accuracy: 76.6%\n",
      "Step 5500 loss: 0.859909\n",
      "Step 5500 accuracy: 72.3%\n",
      "Step 6000 loss: 0.753033\n",
      "Step 6000 accuracy: 77.3%\n",
      "Step 6500 loss: 0.820944\n",
      "Step 6500 accuracy: 73.0%\n",
      "Step 7000 loss: 0.644217\n",
      "Step 7000 accuracy: 83.6%\n",
      "Step 7500 loss: 0.788697\n",
      "Step 7500 accuracy: 78.9%\n",
      "Step 8000 loss: 0.708563\n",
      "Step 8000 accuracy: 80.5%\n",
      "Step 8500 loss: 0.728278\n",
      "Step 8500 accuracy: 80.9%\n",
      "Step 9000 loss: 0.803811\n",
      "Step 9000 accuracy: 79.3%\n",
      "Step 9500 loss: 0.726636\n",
      "Step 9500 accuracy: 80.1%\n",
      "Step 10000 loss: 0.668420\n",
      "Step 10000 accuracy: 80.9%\n",
      "Accuracy in test dataset: 91.05\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.369592\n",
      "Step 0 accuracy: 5.9%\n",
      "Step 500 loss: 0.596556\n",
      "Step 500 accuracy: 81.6%\n",
      "Step 1000 loss: 0.567714\n",
      "Step 1000 accuracy: 85.5%\n",
      "Step 1500 loss: 0.558809\n",
      "Step 1500 accuracy: 85.2%\n",
      "Step 2000 loss: 0.464945\n",
      "Step 2000 accuracy: 85.2%\n",
      "Step 2500 loss: 0.512289\n",
      "Step 2500 accuracy: 84.8%\n",
      "Step 3000 loss: 0.409826\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.332748\n",
      "Step 3500 accuracy: 89.5%\n",
      "Step 4000 loss: 0.540971\n",
      "Step 4000 accuracy: 81.6%\n",
      "Step 4500 loss: 0.335933\n",
      "Step 4500 accuracy: 91.0%\n",
      "Step 5000 loss: 0.390131\n",
      "Step 5000 accuracy: 88.7%\n",
      "Step 5500 loss: 0.443783\n",
      "Step 5500 accuracy: 87.5%\n",
      "Step 6000 loss: 0.376387\n",
      "Step 6000 accuracy: 88.7%\n",
      "Step 6500 loss: 0.379352\n",
      "Step 6500 accuracy: 88.3%\n",
      "Step 7000 loss: 0.237284\n",
      "Step 7000 accuracy: 93.8%\n",
      "Step 7500 loss: 0.308844\n",
      "Step 7500 accuracy: 92.2%\n",
      "Step 8000 loss: 0.272444\n",
      "Step 8000 accuracy: 91.8%\n",
      "Step 8500 loss: 0.334780\n",
      "Step 8500 accuracy: 91.0%\n",
      "Step 9000 loss: 0.355533\n",
      "Step 9000 accuracy: 89.8%\n",
      "Step 9500 loss: 0.286386\n",
      "Step 9500 accuracy: 91.4%\n",
      "Step 10000 loss: 0.259085\n",
      "Step 10000 accuracy: 91.8%\n",
      "Accuracy in test dataset: 95.23\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.352092\n",
      "Step 0 accuracy: 9.0%\n",
      "Step 500 loss: 0.614035\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.585549\n",
      "Step 1000 accuracy: 84.8%\n",
      "Step 1500 loss: 0.577717\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.485734\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.543224\n",
      "Step 2500 accuracy: 85.2%\n",
      "Step 3000 loss: 0.461063\n",
      "Step 3000 accuracy: 87.1%\n",
      "Step 3500 loss: 0.383980\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.575858\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.362932\n",
      "Step 4500 accuracy: 89.5%\n",
      "Step 5000 loss: 0.442788\n",
      "Step 5000 accuracy: 87.5%\n",
      "Step 5500 loss: 0.490877\n",
      "Step 5500 accuracy: 86.3%\n",
      "Step 6000 loss: 0.406732\n",
      "Step 6000 accuracy: 87.9%\n",
      "Step 6500 loss: 0.433677\n",
      "Step 6500 accuracy: 87.1%\n",
      "Step 7000 loss: 0.269145\n",
      "Step 7000 accuracy: 93.0%\n",
      "Step 7500 loss: 0.372414\n",
      "Step 7500 accuracy: 91.0%\n",
      "Step 8000 loss: 0.300833\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.378140\n",
      "Step 8500 accuracy: 86.3%\n",
      "Step 9000 loss: 0.387571\n",
      "Step 9000 accuracy: 87.5%\n",
      "Step 9500 loss: 0.320244\n",
      "Step 9500 accuracy: 91.0%\n",
      "Step 10000 loss: 0.314494\n",
      "Step 10000 accuracy: 91.4%\n",
      "Accuracy in test dataset: 95.22\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.371503\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.702490\n",
      "Step 500 accuracy: 78.5%\n",
      "Step 1000 loss: 0.611242\n",
      "Step 1000 accuracy: 83.2%\n",
      "Step 1500 loss: 0.627214\n",
      "Step 1500 accuracy: 83.6%\n",
      "Step 2000 loss: 0.524926\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.637017\n",
      "Step 2500 accuracy: 82.0%\n",
      "Step 3000 loss: 0.518468\n",
      "Step 3000 accuracy: 84.8%\n",
      "Step 3500 loss: 0.393109\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.671209\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.471829\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.539219\n",
      "Step 5000 accuracy: 83.2%\n",
      "Step 5500 loss: 0.546245\n",
      "Step 5500 accuracy: 82.4%\n",
      "Step 6000 loss: 0.490495\n",
      "Step 6000 accuracy: 87.1%\n",
      "Step 6500 loss: 0.528566\n",
      "Step 6500 accuracy: 84.8%\n",
      "Step 7000 loss: 0.370310\n",
      "Step 7000 accuracy: 90.2%\n",
      "Step 7500 loss: 0.415955\n",
      "Step 7500 accuracy: 89.8%\n",
      "Step 8000 loss: 0.388626\n",
      "Step 8000 accuracy: 87.9%\n",
      "Step 8500 loss: 0.492212\n",
      "Step 8500 accuracy: 85.5%\n",
      "Step 9000 loss: 0.466659\n",
      "Step 9000 accuracy: 85.5%\n",
      "Step 9500 loss: 0.446819\n",
      "Step 9500 accuracy: 87.9%\n",
      "Step 10000 loss: 0.357014\n",
      "Step 10000 accuracy: 89.1%\n",
      "Accuracy in test dataset: 94.46\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.423194\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.716486\n",
      "Step 500 accuracy: 79.3%\n",
      "Step 1000 loss: 0.669655\n",
      "Step 1000 accuracy: 82.8%\n",
      "Step 1500 loss: 0.725573\n",
      "Step 1500 accuracy: 78.1%\n",
      "Step 2000 loss: 0.588550\n",
      "Step 2000 accuracy: 82.8%\n",
      "Step 2500 loss: 0.718225\n",
      "Step 2500 accuracy: 80.9%\n",
      "Step 3000 loss: 0.590825\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.434319\n",
      "Step 3500 accuracy: 86.3%\n",
      "Step 4000 loss: 0.709901\n",
      "Step 4000 accuracy: 79.7%\n",
      "Step 4500 loss: 0.526381\n",
      "Step 4500 accuracy: 86.7%\n",
      "Step 5000 loss: 0.602733\n",
      "Step 5000 accuracy: 83.2%\n",
      "Step 5500 loss: 0.639670\n",
      "Step 5500 accuracy: 80.1%\n",
      "Step 6000 loss: 0.580093\n",
      "Step 6000 accuracy: 84.4%\n",
      "Step 6500 loss: 0.621510\n",
      "Step 6500 accuracy: 83.2%\n",
      "Step 7000 loss: 0.456145\n",
      "Step 7000 accuracy: 87.9%\n",
      "Step 7500 loss: 0.479082\n",
      "Step 7500 accuracy: 85.2%\n",
      "Step 8000 loss: 0.438242\n",
      "Step 8000 accuracy: 89.5%\n",
      "Step 8500 loss: 0.565493\n",
      "Step 8500 accuracy: 82.8%\n",
      "Step 9000 loss: 0.598739\n",
      "Step 9000 accuracy: 83.2%\n",
      "Step 9500 loss: 0.489284\n",
      "Step 9500 accuracy: 84.8%\n",
      "Step 10000 loss: 0.415594\n",
      "Step 10000 accuracy: 88.3%\n",
      "Accuracy in test dataset: 93.21\n",
      "==============================================\n",
      "hidden_layers_cf: [512] | keep_prob_cf: [0.1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.582275\n",
      "Step 0 accuracy: 9.4%\n",
      "Step 500 loss: 0.853010\n",
      "Step 500 accuracy: 70.7%\n",
      "Step 1000 loss: 0.812825\n",
      "Step 1000 accuracy: 81.2%\n",
      "Step 1500 loss: 0.824604\n",
      "Step 1500 accuracy: 76.6%\n",
      "Step 2000 loss: 0.708953\n",
      "Step 2000 accuracy: 79.7%\n",
      "Step 2500 loss: 0.859893\n",
      "Step 2500 accuracy: 77.7%\n",
      "Step 3000 loss: 0.651403\n",
      "Step 3000 accuracy: 82.8%\n",
      "Step 3500 loss: 0.519350\n",
      "Step 3500 accuracy: 84.4%\n",
      "Step 4000 loss: 0.785541\n",
      "Step 4000 accuracy: 76.2%\n",
      "Step 4500 loss: 0.628717\n",
      "Step 4500 accuracy: 83.6%\n",
      "Step 5000 loss: 0.771018\n",
      "Step 5000 accuracy: 79.7%\n",
      "Step 5500 loss: 0.710204\n",
      "Step 5500 accuracy: 78.9%\n",
      "Step 6000 loss: 0.633012\n",
      "Step 6000 accuracy: 82.4%\n",
      "Step 6500 loss: 0.752159\n",
      "Step 6500 accuracy: 79.7%\n",
      "Step 7000 loss: 0.544861\n",
      "Step 7000 accuracy: 85.9%\n",
      "Step 7500 loss: 0.637311\n",
      "Step 7500 accuracy: 81.2%\n",
      "Step 8000 loss: 0.559832\n",
      "Step 8000 accuracy: 85.5%\n",
      "Step 8500 loss: 0.602043\n",
      "Step 8500 accuracy: 82.0%\n",
      "Step 9000 loss: 0.739203\n",
      "Step 9000 accuracy: 78.1%\n",
      "Step 9500 loss: 0.616318\n",
      "Step 9500 accuracy: 82.0%\n",
      "Step 10000 loss: 0.587634\n",
      "Step 10000 accuracy: 82.8%\n",
      "Accuracy in test dataset: 92.14\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [1] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.324088\n",
      "Step 0 accuracy: 12.5%\n",
      "Step 500 loss: 0.593353\n",
      "Step 500 accuracy: 80.9%\n",
      "Step 1000 loss: 0.548169\n",
      "Step 1000 accuracy: 85.5%\n",
      "Step 1500 loss: 0.551205\n",
      "Step 1500 accuracy: 85.9%\n",
      "Step 2000 loss: 0.459307\n",
      "Step 2000 accuracy: 85.5%\n",
      "Step 2500 loss: 0.512995\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.402096\n",
      "Step 3000 accuracy: 88.3%\n",
      "Step 3500 loss: 0.318807\n",
      "Step 3500 accuracy: 89.1%\n",
      "Step 4000 loss: 0.515175\n",
      "Step 4000 accuracy: 83.6%\n",
      "Step 4500 loss: 0.334156\n",
      "Step 4500 accuracy: 91.0%\n",
      "Step 5000 loss: 0.392936\n",
      "Step 5000 accuracy: 87.9%\n",
      "Step 5500 loss: 0.423935\n",
      "Step 5500 accuracy: 87.9%\n",
      "Step 6000 loss: 0.348815\n",
      "Step 6000 accuracy: 90.2%\n",
      "Step 6500 loss: 0.375162\n",
      "Step 6500 accuracy: 89.8%\n",
      "Step 7000 loss: 0.233703\n",
      "Step 7000 accuracy: 93.8%\n",
      "Step 7500 loss: 0.289421\n",
      "Step 7500 accuracy: 93.8%\n",
      "Step 8000 loss: 0.274402\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.309370\n",
      "Step 8500 accuracy: 89.8%\n",
      "Step 9000 loss: 0.316368\n",
      "Step 9000 accuracy: 91.0%\n",
      "Step 9500 loss: 0.263757\n",
      "Step 9500 accuracy: 91.4%\n",
      "Step 10000 loss: 0.268108\n",
      "Step 10000 accuracy: 93.0%\n",
      "Accuracy in test dataset: 95.56\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.8] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.343216\n",
      "Step 0 accuracy: 7.0%\n",
      "Step 500 loss: 0.596327\n",
      "Step 500 accuracy: 82.0%\n",
      "Step 1000 loss: 0.573734\n",
      "Step 1000 accuracy: 84.0%\n",
      "Step 1500 loss: 0.560025\n",
      "Step 1500 accuracy: 86.3%\n",
      "Step 2000 loss: 0.472903\n",
      "Step 2000 accuracy: 85.9%\n",
      "Step 2500 loss: 0.508687\n",
      "Step 2500 accuracy: 85.5%\n",
      "Step 3000 loss: 0.449927\n",
      "Step 3000 accuracy: 87.9%\n",
      "Step 3500 loss: 0.339974\n",
      "Step 3500 accuracy: 88.7%\n",
      "Step 4000 loss: 0.526741\n",
      "Step 4000 accuracy: 82.0%\n",
      "Step 4500 loss: 0.350639\n",
      "Step 4500 accuracy: 91.4%\n",
      "Step 5000 loss: 0.418599\n",
      "Step 5000 accuracy: 87.9%\n",
      "Step 5500 loss: 0.462476\n",
      "Step 5500 accuracy: 86.3%\n",
      "Step 6000 loss: 0.411308\n",
      "Step 6000 accuracy: 87.5%\n",
      "Step 6500 loss: 0.396648\n",
      "Step 6500 accuracy: 88.7%\n",
      "Step 7000 loss: 0.250409\n",
      "Step 7000 accuracy: 93.4%\n",
      "Step 7500 loss: 0.345863\n",
      "Step 7500 accuracy: 91.0%\n",
      "Step 8000 loss: 0.290051\n",
      "Step 8000 accuracy: 91.4%\n",
      "Step 8500 loss: 0.366367\n",
      "Step 8500 accuracy: 88.3%\n",
      "Step 9000 loss: 0.388478\n",
      "Step 9000 accuracy: 87.9%\n",
      "Step 9500 loss: 0.304616\n",
      "Step 9500 accuracy: 89.8%\n",
      "Step 10000 loss: 0.314357\n",
      "Step 10000 accuracy: 90.6%\n",
      "Accuracy in test dataset: 95.33\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.4] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.370494\n",
      "Step 0 accuracy: 7.8%\n",
      "Step 500 loss: 0.654310\n",
      "Step 500 accuracy: 78.9%\n",
      "Step 1000 loss: 0.605861\n",
      "Step 1000 accuracy: 84.4%\n",
      "Step 1500 loss: 0.594260\n",
      "Step 1500 accuracy: 84.4%\n",
      "Step 2000 loss: 0.491235\n",
      "Step 2000 accuracy: 83.6%\n",
      "Step 2500 loss: 0.574799\n",
      "Step 2500 accuracy: 84.0%\n",
      "Step 3000 loss: 0.470779\n",
      "Step 3000 accuracy: 87.5%\n",
      "Step 3500 loss: 0.397763\n",
      "Step 3500 accuracy: 88.3%\n",
      "Step 4000 loss: 0.570046\n",
      "Step 4000 accuracy: 81.6%\n",
      "Step 4500 loss: 0.371237\n",
      "Step 4500 accuracy: 88.3%\n",
      "Step 5000 loss: 0.519920\n",
      "Step 5000 accuracy: 85.5%\n",
      "Step 5500 loss: 0.516420\n",
      "Step 5500 accuracy: 85.9%\n",
      "Step 6000 loss: 0.509626\n",
      "Step 6000 accuracy: 85.5%\n",
      "Step 6500 loss: 0.478220\n",
      "Step 6500 accuracy: 85.5%\n",
      "Step 7000 loss: 0.337974\n",
      "Step 7000 accuracy: 90.6%\n",
      "Step 7500 loss: 0.398595\n",
      "Step 7500 accuracy: 87.9%\n",
      "Step 8000 loss: 0.364289\n",
      "Step 8000 accuracy: 89.5%\n",
      "Step 8500 loss: 0.401622\n",
      "Step 8500 accuracy: 87.5%\n",
      "Step 9000 loss: 0.482206\n",
      "Step 9000 accuracy: 84.4%\n",
      "Step 9500 loss: 0.403303\n",
      "Step 9500 accuracy: 88.7%\n",
      "Step 10000 loss: 0.350641\n",
      "Step 10000 accuracy: 90.2%\n",
      "Accuracy in test dataset: 94.96\n",
      "==============================================\n",
      "hidden_layers_cf: [1024] | keep_prob_cf: [0.2] | learning_rate: 0.1 | num_step: 10240 | batch_size: 256 | over_fit : False |\n",
      "Step 0 loss: 2.483479\n",
      "Step 0 accuracy: 8.2%\n",
      "Step 500 loss: 0.655122\n",
      "Step 500 accuracy: 80.1%\n",
      "Step 1000 loss: 0.690747\n",
      "Step 1000 accuracy: 82.4%\n",
      "Step 1500 loss: 0.618749\n",
      "Step 1500 accuracy: 81.6%\n",
      "Step 2000 loss: 0.606810\n",
      "Step 2000 accuracy: 82.0%\n",
      "Step 2500 loss: 0.691094\n",
      "Step 2500 accuracy: 80.1%\n"
     ]
    }
   ],
   "source": [
    "# log_dir to visualize the graph\n",
    "log_dir = 'tf_log_dropout'\n",
    "\n",
    "\n",
    "# Create a neural network with dropout in each hidden layer\n",
    "# The original paper use dropout rate of 0.2 for input and 0.5 for hidden layers\n",
    "# Input:\n",
    "## train_data : tf.placeholder with shape (None, IMAGE_PIXELS)\n",
    "## hidden_layers : list of hidden layer configuration\n",
    "## keep_prob : list of placeholder type float for dropout\n",
    "# Output:\n",
    "## logits : tf.Tensor hold the final value\n",
    "def dropout_inference(train_data, hidden_layers, keep_prob) :\n",
    "    num_hidden_layers = len(hidden_layers)\n",
    "    name = 'dropout_layer_'\n",
    "    # Create weights, biases as variables and add the computation for hidden layers\n",
    "    input_len = IMAGE_PIXELS\n",
    "    data = train_data\n",
    "    for i, num_nodes in enumerate(hidden_layers) :\n",
    "        with tf.variable_scope(name + str(i)) :\n",
    "            weights = tf.get_variable('weights',\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=\n",
    "                                      tf.truncated_normal([input_len, num_nodes],\n",
    "                                                         stddev=scale_dev(IMAGE_PIXELS)))\n",
    "            biases = tf.get_variable('biases', dtype=tf.float32, \n",
    "                                     initializer=tf.zeros([num_nodes]))\n",
    "        # Update input data with dropout layer (layered matmul and addition)\n",
    "        data = tf.nn.relu(tf.nn.dropout((tf.matmul(data, weights) + biases), keep_prob[i], \n",
    "                             name='dropout'), name = 'relu')\n",
    "        # Summary ops to collect distribution\n",
    "        w_hist = tf.histogram_summary('weights', weights)\n",
    "        b_hist = tf.histogram_summary('biases', biases)\n",
    "        h_hist = tf.histogram_summary('dropout_output', data)\n",
    "        # Next weights matrix shape[0]\n",
    "        input_len = num_nodes\n",
    "    # Create output layer\n",
    "    with tf.variable_scope('output_layer') :\n",
    "        weights = tf.get_variable('weights',\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=\n",
    "                                  tf.truncated_normal([hidden_layers[-1], num_labels],\n",
    "                                                     stddev=scale_dev(hidden_layers[-1])))\n",
    "        biases = tf.get_variable('biases', dtype=tf.float32,\n",
    "                                initializer=tf.zeros([num_labels]))\n",
    "    logits = tf.add(tf.matmul(data, weights) ,biases, name='logits')\n",
    "    return logits\n",
    "\n",
    "# Create loss function\n",
    "# Input:\n",
    "## logits : tf.Tensor store logits value\n",
    "## train_labels : tf.placeholder for labels\n",
    "# Output:\n",
    "## loss : tf.Tensor hold the result of the loss \n",
    "def dropout_loss(logits, train_labels) :\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits, train_labels, \n",
    "                                                      name='xentropy')\n",
    "    loss = tf.reduce_mean(xentropy, name='xentropy_mean')\n",
    "    loss_summary = tf.scalar_summary('cross entropy loss', loss)\n",
    "    return loss\n",
    "\n",
    "# Create training ops\n",
    "# Input:\n",
    "## loss : tf.Tensor store the loss function\n",
    "## learning_rate : learning rate of SGD\n",
    "# Output:\n",
    "## train_op : tf.Tensor to run the training operation\n",
    "def dropout_train(loss, learning_rate) :\n",
    "    with tf.variable_scope('training') :\n",
    "        global_step = tf.Variable(0) # Count number of step taken\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "    \n",
    "# Accuracy measurement\n",
    "# Input: \n",
    "## logits - tf.Tensor hold the computation\n",
    "## labels - true label of the data\n",
    "# Output:\n",
    "## accuracy - tf.Tensor hold the accuracy\n",
    "def dropout_acc(logits, labels) :\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\n",
    "    return accuracy\n",
    "\n",
    "def dropout_run(hidden_layers_cf, keep_prob_cf, learning_rate, num_step, batch_size, over_fit) :\n",
    "    print ('==============================================')\n",
    "    print ('hidden_layers_cf: %s | keep_prob_cf: %s | learning_rate: %s | num_step: %s | batch_size: %s | over_fit : %s |'\n",
    "          % (str(hidden_layers_cf), str(keep_prob_cf), str(learning_rate), str(num_step), str(batch_size), str(over_fit)))\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default() :\n",
    "        # Place holder to feed data\n",
    "        tf_data = tf.placeholder(dtype=tf.float32, shape=[None, IMAGE_PIXELS])\n",
    "        tf_labels = tf.placeholder(dtype=tf.float32, shape=[None, num_labels])\n",
    "        # Hidden layer configurations\n",
    "        hidden_config = hidden_layers_cf\n",
    "        keep_prob = tf.placeholder(dtype=tf.float32, shape=[len(hidden_config)])\n",
    "        # Build the graph\n",
    "        logits = dropout_inference(tf_data, hidden_config, keep_prob)\n",
    "        loss = dropout_loss(logits, tf_labels)\n",
    "        train_op = dropout_train(loss, learning_rate)\n",
    "        acc = dropout_acc(logits, tf_labels)\n",
    "        with tf.Session() as sess :\n",
    "            # Summary\n",
    "            merged = tf.merge_all_summaries()\n",
    "            writer = tf.train.SummaryWriter(log_dir, sess.graph_def)\n",
    "            # Initialize all variables\n",
    "            init = tf.initialize_all_variables()\n",
    "            sess.run(init)\n",
    "            # Feed data for test and validation\n",
    "            test_dict = {tf_data : test_dataset, tf_labels : test_labels, \n",
    "                         keep_prob : [1.0]}\n",
    "            valid_dict = {tf_data : valid_dataset, tf_labels : valid_labels,\n",
    "                         keep_prob : [1.0]}\n",
    "            # Test for overfit\n",
    "            ok = 1\n",
    "            if over_fit :\n",
    "                ok = 0 # Keep offset_data at 0\n",
    "            # Training process\n",
    "            for i in range(num_step) :\n",
    "                # Record performance on validation dataset\n",
    "                if i % 1000 == 0 :\n",
    "                    result = sess.run([merged, acc], feed_dict=valid_dict)\n",
    "                    summary_str = result[0]\n",
    "                    writer.add_summary(summary_str, i)\n",
    "\n",
    "                offset = ok * (i * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                feed_data = train_dataset[offset:(offset + batch_size)]\n",
    "                feed_labels = train_labels[offset:(offset+batch_size)]\n",
    "                train_dict = {tf_data : feed_data, tf_labels : feed_labels, \n",
    "                              keep_prob : keep_prob_cf}\n",
    "                _, step_loss, step_acc  = sess.run([train_op, loss, acc], feed_dict=train_dict)\n",
    "                if i % 2000 == 0 :\n",
    "                    randomize(train_dataset, train_labels)\n",
    "                if i % 500 == 0 :\n",
    "                    print (\"Step %d loss: %f\" % (i, step_loss))\n",
    "                    print (\"Step %d accuracy: %.1f%%\" % (i, step_acc))\n",
    "\n",
    "            ac = sess.run(acc, feed_dict=test_dict)\n",
    "            print (\"Accuracy in test dataset: %s\" % ac)\n",
    "            \n",
    "for learning_rates in [0.05, 0.1, 0.25, 0.5] :\n",
    "    for batch_sizes in [128, 256, 512, 1024, 2000] :\n",
    "        for num_step in [2560, 5120, 10240] :\n",
    "            for hidden_layers_cf in [[256], [512], [1024], [2048]] :\n",
    "                for keep_prob_cf in [[1], [0.8], [0.4], [0.2], [0.1]] :\n",
    "                    for over_fit in [False] :\n",
    "                        dropout_run(hidden_layers_cf, keep_prob_cf, learning_rates, num_step, batch_sizes, over_fit)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cell author : Hoang NT\n",
    "# Cell : Neural network with dropout, mulilayers and learning rate decay\n",
    "\n",
    "def custom_dropout_inference\n",
    "        \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
